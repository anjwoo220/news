import feedparser
import googlesearch
import google.generativeai as genai
from datetime import datetime, timedelta
import time
import json
import os
import requests
import re
from bs4 import BeautifulSoup


import streamlit as st

# Helper: Render Custom Mobile-Optimized Header
def render_custom_header(text, level=1):
    """
    Renders a custom HTML header for SEO and Mobile UI optimization.
    - H1: 22px (Mobile Friendly)
    - H2: 18px
    - Adjusts margins to save space.
    """
    font_size = "22px" if level == 1 else "18px"
    margin = "10px 0 5px 0"
    color = "#333333" # Default dark grey, can be adjusted for dark mode via CSS variables if needed
    
    # Use CSS variable for text color to support Dark Mode automatically if desired,
    # or stick to fixed color. Let's use var(--text-color) for better adaptation.
    # But user requested #333333 specifically. Let's stick to user request but add dark mode support via Streamlit's theming if possible.
    # User said: "Color: #333333 (ë‹¤í¬ëª¨ë“œ ëŒ€ì‘ í•„ìš”ì‹œ var(--text-color) ì‚¬ìš©)"
    # Let's use var(--text-color) to be safe for dark mode which is active.
    
    st.markdown(
        f"""
        <{f'h{level}'} style='text-align: left; font-size: {font_size}; font-weight: 700; margin: {margin}; color: var(--text-color); line-height: 1.2;'>
            {text}
        </{f'h{level}'}>
        """,
        unsafe_allow_html=True
    )

# Helper: Check if text contains Thai characters
def is_thai(text):
    import re
    if not text: return False
    return bool(re.search(r'[\u0E00-\u0E7F]', text))

# Helper: Convert Thai Buddhist year to Gregorian year
def convert_thai_year(text: str) -> str:
    import re
    def repl(match):
        year = int(match.group())
        if year > 2500:  # typical Buddhist year
            return str(year - 543)
        return match.group()
    return re.sub(r'\b\d{4}\b', repl, text)

# Helper: Translate text to Korean using Gemini
def translate_text(text: str, dest: str = "ko") -> str:
    """
    Translate Thai text to Korean using Gemini 2.0 Flash.
    Handles API key loading and ensures robust response.
    """
    # 1. Quick Check: Is it already Korean or just numbers?
    if not text or len(text.strip()) == 0:
        return ""
    
    # 2. Convert Thai Buddhist year first
    text = convert_thai_year(text)
    
    # 3. Use Gemini
    try:
        # Lazy load API key if needed (or assume configured globally in app)
        # But utils might be imported separately, so re-check/configure.
        import google.generativeai as genai
        import toml
        
        # Try to get key efficiently
        api_key = os.environ.get("GEMINI_API_KEY")
        if not api_key:
            try:
                secrets = toml.load(".streamlit/secrets.toml")
                api_key = secrets.get("GEMINI_API_KEY")
            except: pass
            
        if api_key:
            genai.configure(api_key=api_key)
            
        model = genai.GenerativeModel('gemini-2.0-flash')
        
        prompt = f"""
        Translate the following Thai text to Korean.
        - Maintain the original tone (News/Formal).
        - Output ONLY the translated text. Do not add explanations.
        - If the text is already Korean, return it as is.
        
        Text:
        {text}
        """
        
        response = model.generate_content(prompt)
        translated = response.text.strip()
        return translated
        
    except Exception as e:
        print(f"Translation Error for '{text[:20]}...': {e}")
        return text


# Helper: Check if article is within last N days
def is_recent(entry, days=3):
    if not hasattr(entry, 'published_parsed'):
        return True # Default to include if no date
    
    # published_parsed is a struct_time, convert to datetime
    pub_date = datetime.fromtimestamp(time.mktime(entry.published_parsed))
    limit_date = datetime.now() - timedelta(days=days)
    return pub_date >= limit_date

# Helper: Check relevance to Thailand
def is_relevant_to_thailand(entry):
    """
    Determines if an article is relevant to Thailand based on keywords and script.
    Checks: Title, Summary (if available)
    """
    import re
    
    # 1. content to check
    text = (entry.title + " " + entry.get('summary', '')).lower()
    
    # 2. Check for Thai Characters (Script)
    if re.search(r'[\u0E00-\u0E7F]', text):
        return True
        
    # 3. Check for English Keywords
    keywords = [
        "thailand", "thai", "bangkok", "phuket", "pattaya", "chiang", 
        "samui", "krabi", "isan", "baht", "pheu thai", 
        "prime minister", "paetongtarn", "thaksin", "king", "royal",
        "cabinet", "govt", "police", "otp", "airport"
    ]
    
    for kw in keywords:
        if kw in text:
            return True
            
    return False

# 1. RSS Parsing (Balanced)
def fetch_balanced_rss(feeds_config, processed_urls=None):
    """
    Fetches RSS feeds and returns a balanced mix of items across categories.
    feeds_config: List of dicts [{'category': '...', 'url': '...'}, ...]
    processed_urls: Set of strings (optional) to skip already seen news.
    """
    import requests
    
    if processed_urls is None:
        processed_urls = set()
    
    # Using a typical browser User-Agent
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
        'Accept': 'application/rss+xml, application/xml, text/xml, */*'
    }
    
    category_buckets = {}
    MAX_PER_CATEGORY = 80  # Increased from 20 to 80 to allow checking more feeds (e.g. Pattaya News)
    
    for feed in feeds_config:
        category = feed.get('category', 'General')
        url = feed.get('url')
        
        if category not in category_buckets:
            category_buckets[category] = []
            
        # Check quota early
        if len(category_buckets[category]) >= MAX_PER_CATEGORY:
            print(f"Skipping feed {url} (Quota full for {category})")
            continue
            
        try:
            print(f"Fetching [{category}] {url}...")
            response = requests.get(url, headers=headers, timeout=10)
            
            if response.status_code != 200:
                print(f"Failed to fetch {url}: Status {response.status_code}")
                continue
                
            feed_data = feedparser.parse(response.content)
            
            if feed_data.bozo:
                print(f"XML Parse Warning for {url}: {feed_data.bozo_exception}")
            
            print(f"Successfully parsed {url}: Found {len(feed_data.entries)} entries.")
            
            for entry in feed_data.entries:
                # Re-check quota inside loop
                if len(category_buckets[category]) >= MAX_PER_CATEGORY:
                    break
                
                # Filter: Relevance Check (Skip non-Thai news)
                if not is_relevant_to_thailand(entry):
                    # print(f"Skipping irrelevant: {entry.title}") 
                    continue

                # Filter: Skip already processed
                if entry.link in processed_urls:
                    # print(f"Skipping already processed: {entry.title}")
                    continue

                if is_recent(entry):
                    item = {
                        "title": entry.title,
                        "link": entry.link,
                        "published": entry.get("published", str(datetime.now())),
                        "summary": entry.get("summary", ""),
                        "source": feed_data.feed.get("title", url),
                        "suggested_category": category, # Hint for AI or logic
                        "_raw_entry": entry
                    }
                    category_buckets[category].append(item)
                    
        except Exception as e:
            print(f"Error fetching {url}: {e}")

    # Interleave (Round-Robin) to create balanced list
    balanced_items = []
    max_items_per_cat = max(len(items) for items in category_buckets.values()) if category_buckets else 0
    
    categories = list(category_buckets.keys())
    
    for i in range(max_items_per_cat):
        for cat in categories:
            if i < len(category_buckets[cat]):
                balanced_items.append(category_buckets[cat][i])
                
    return balanced_items

# 2. Gemini Analysis
import re

def clean_html(raw_html):
    cleanr = re.compile('<.*?>')
    cleantext = re.sub(cleanr, '', raw_html)
    return cleantext


# --------------------------------------------------------------------------------
# Google News RSS Fetcher (Backup Source)
# --------------------------------------------------------------------------------
def fetch_google_news_rss(query="Thailand Tourism", period="24h"):
    """
    Fetches Google News RSS for a specific query.
    Returns: List of dicts matching news item structure.
    """
    import feedparser
    import urllib.parse
    import time
    import requests
    
    encoded_query = urllib.parse.quote(query)
    # hl=en-TH, gl=TH ensures Thailand focus
    # when:24h = Last 24 hours
    # scoring=n = Sort by Date (Newest first)
    rss_url = f"https://news.google.com/rss/search?q={encoded_query}+when:{period}&hl=en-TH&gl=TH&ceid=TH:en&scoring=n"
    
    print(f"Fetching Google News: {query}...")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'
    }
    
    try:
        # [FIX] Use requests with User-Agent to avoid 403/Blocking
        response = requests.get(rss_url, headers=headers, timeout=10)
        
        if response.status_code == 200:
            feed = feedparser.parse(response.content)
            items = []
            for entry in feed.entries:
                # Standardize to our News Item format
                item = {
                    'title': entry.title,
                    'link': entry.link,
                    'published': entry.get('published', ''),
                    'summary': entry.get('description', ''),
                    'source': entry.get('source', {}).get('title', 'Google News'),
                    '_raw_entry': entry # Keep for image extraction
                }
                items.append(item)
            print(f" -> Found {len(items)} items from Google News.")
            return items
        else:
            print(f"Google News Fetch Failed: Status {response.status_code}")
            return []
            
    except Exception as e:
        print(f"Google News Fetch Error: {e}")
        return []

# Helper: Fetch Full Content from URL
def fetch_full_content(url):
    """
    Scrapes the main text content from a news URL.
    Returns: String (text) or None
    """
    import requests
    from bs4 import BeautifulSoup
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'
        }
        # Timeout slightly longer for scraping
        response = requests.get(url, headers=headers, timeout=5)
# Reverted Google Cache Fallback
        
        if response.status_code != 200:
            return None
            
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Remove unwanted elements
        for script in soup(["script", "style", "nav", "footer", "header", "aside", "noscript"]):
            script.decompose()
            
        # Extract text from p tags (most reliable for news)
        paragraphs = soup.find_all('p')
        text = ' '.join([p.get_text() for p in paragraphs])
        
        # Clean up whitespace
        text = ' '.join(text.split())
        
        # Remove Google Cache Header Artifacts (if any)
        if "Google's cache of" in text:
             text = text.replace("This is Google's cache of", "")
        
        if len(text) < 100: # Too short, likely failed
            return None
            
        return text[:3000] # Limit to 3000 chars
        
    except Exception as e:
        # print(f"Error scraping {url}: {e}")
        return None

def analyze_news_with_gemini(news_items, api_key, existing_titles=None, current_time=None):
    if not news_items:
        return {}, "No news items to analyze."
        
    genai.configure(api_key=api_key)
    
    # Analyze ALL provided items
    limited_news_items = news_items[:10] 
    
    aggregated_topics = []
    total_items = len(limited_news_items)
    print(f"Starting sequential analysis for {total_items} items...")

    # Format existing titles for context
    existing_context = "\n".join([f"- {t}" for t in (existing_titles or [])[:15]])

    for idx, item in enumerate(limited_news_items):
        print(f"[{idx+1}/{total_items}] Processing: {item['title']}...")
        
        full_content = fetch_full_content(item['link'])
        if not full_content:
            full_content = clean_html(item['summary'])[:800]

        # New Context-Aware Prompt
        prompt = f"""
# Role
ë‹¹ì‹ ì€ íƒœêµ­ ë°©ì½•ì„ ì—¬í–‰í•˜ëŠ” í•œêµ­ì¸ ì—¬í–‰ìë¥¼ ìœ„í•œ 'ì‹¤ì‹œê°„ ë‰´ìŠ¤ íë ˆì´í„°'ì…ë‹ˆë‹¤.
í˜„ì¬ ì‹œê°ì€ {current_time or 'ì•Œ ìˆ˜ ì—†ìŒ'} ì´ë©°, ì•„ì¹¨/ì €ë… ë¸Œë¦¬í•‘ì„ ìœ„í•´ ë‰´ìŠ¤ë¥¼ ì„ ë³„ ì¤‘ì…ë‹ˆë‹¤.

# Task
ì…ë ¥ëœ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë¶„ì„í•˜ì—¬ ì—¬í–‰ìì—ê²Œ í•„ìš”í•œ ì •ë³´ë¥¼ ì„ ë³„í•˜ê³  ìš”ì•½í•˜ì„¸ìš”.
ì´ë•Œ, **'ê¸°ê³„ì ì¸ ì¤‘ë³µ'ê³¼ 'ì˜ë¯¸ ìˆëŠ” ì—…ë°ì´íŠ¸'ë¥¼ êµ¬ë¶„**í•˜ëŠ” ê²ƒì´ ê°€ì¥ ì¤‘ìš”í•©ë‹ˆë‹¤.

# Input Data
1. **Candidate News:** 
   - Title: {item['title']}
   - Source: {item['source']}
   - Content Snippet: {full_content[:1500]}
2. **Existing News (ìµœê·¼ 24ì‹œê°„ ë‚´ ì´ë¯¸ ê²Œì‹œëœ ê¸°ì‚¬ë“¤):**
{existing_context}

# ğŸ” Filtering & Scoring Logic (3-Step)

## Step 1: 'ì—…ë°ì´íŠ¸' ì—¬ë¶€ íŒë‹¨ (Context Check)
ê¸°ì¡´ ë‰´ìŠ¤(Existing News)ì™€ ì£¼ì œê°€ ë¹„ìŠ·í•˜ë”ë¼ë„, ì•„ë˜ ê²½ìš°ì—ëŠ” **'ìƒˆë¡œìš´ ë‰´ìŠ¤'**ë¡œ ì·¨ê¸‰í•˜ì„¸ìš”.
- **ì‹œê°„ ê²½ê³¼:** ì‚¬ê±´ì˜ ì§„í–‰ ìƒí™©ì´ ë³€í•œ ê²½ìš° (ì˜ˆ: ì‹œìœ„ ë°œìƒ -> ì‹œìœ„ í•´ì‚°, ì‚¬ê³  ë°œìƒ -> ì‚¬ìƒì ì§‘ê³„ ì™„ë£Œ)
- **ì¼ì¼ ë¸Œë¦¬í•‘:** ë‚ ì”¨, ë¯¸ì„¸ë¨¼ì§€(PM2.5), í™˜ìœ¨ ë“± ë§¤ì¼ ë³€í•˜ëŠ” ìˆ˜ì¹˜ëŠ” ì–´ì œì™€ ì œëª©ì´ ë¹„ìŠ·í•´ë„ **ì˜¤ëŠ˜ ë‚ ì§œ ë°ì´í„°ë¼ë©´ í•„ìˆ˜ ê²Œì‹œ(Score +3)**.
- **ì•„ì¹¨/ì €ë…:** 'Morning Briefing' ë˜ëŠ” 'Daily Update' ì„±ê²©ì˜ ê¸°ì‚¬ëŠ” ìš°ì„ ìˆœìœ„ë¥¼ ë†’ì„.

## Step 2: Scoring (1~10ì )
- **7~10ì  (í•„ìˆ˜):** ì—¬í–‰ê° ì•ˆì „ ìœ„í˜‘(ì‹œìœ„, í™ìˆ˜, ë²”ì£„), ë¹„ì/ì…êµ­ ê·œì • ë³€ê²½, ëŒ€í˜• ì¶•ì œ, ê³µí•­ í˜¼ì¡.
- **4~6ì  (ë³´í†µ):** ìƒˆë¡œìš´ í•«í”Œ, ì¼ë°˜ì ì¸ ë‚ ì”¨, ì†Œì†Œí•œ ê·œì œ, í¥ë¯¸ë¡œìš´ ë¡œì»¬ ë‰´ìŠ¤.
- **1~3ì  (ë¬´ì‹œ):** ë‹¨ìˆœ ì •ì¹˜ ì‹¸ì›€, ì—°ì˜ˆì¸ ê°€ì‹­, ì—¬í–‰ê³¼ ë¬´ê´€í•œ ë‰´ìŠ¤.

# Constraints
- ì´ë¯¸ ê²Œì‹œëœ ë‰´ìŠ¤ì™€ **ë‚´ìš©ì´ 100% ë™ì¼í•˜ë©´ ì œì™¸**í•˜ì„¸ìš”.
- í•˜ì§€ë§Œ **'ìƒí™©ì´ ì—…ë°ì´íŠ¸' ë˜ì—ˆë‹¤ë©´ ë°˜ë“œì‹œ í¬í•¨**í•˜ì„¸ìš”.
- ì•„ì¹¨ì—ëŠ” 'ì˜¤ëŠ˜ì˜ ì˜ˆë³´/ì˜ˆì •' ìœ„ì£¼, ì €ë…ì—ëŠ” 'ì˜¤ëŠ˜ ë°œìƒí•œ ì‚¬ê±´/ê²°ê³¼' ìœ„ì£¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ë‘ì„¸ìš”.

# Output Format (JSON Only)
{{
  "topics": [
    {{
      "title": "ê¸°ì‚¬ ì œëª©",
      "summary": "í•µì‹¬ 3ì¤„ ìš”ì•½ (- ë¡œ ì‹œì‘)",
      "full_translated": "ê¸°ì‚¬ ì „ë¬¸ (Markdown)",
      "category": "ì¹´í…Œê³ ë¦¬",
      "tourist_impact_score": 0,
      "impact_reason": "ì ìˆ˜ ë¶€ì—¬ ë° ì—…ë°ì´íŠ¸ íŒë‹¨ ê·¼ê±°",
      "event_info": {{
          "date": "YYYY-MM-DD",
          "location": "...", 
          "price": "...",
          "location_google_map_query": "..."
      }},
      "references": [
        {{"title": "{item['title']}", "url": "{item['link']}", "source": "{item['source']}"}}
      ]
    }}
  ]
}}
"""
        
        # Retry Logic with Safety Limits
        max_retries = 3
        retry_count = 0
        success = False
        
        while retry_count < max_retries and not success:
            try:
                model = genai.GenerativeModel('gemini-2.0-flash', generation_config={"response_mime_type": "application/json"})
                response = model.generate_content(prompt)
                # Force HTTPS for all URLs in the generated content (Markdown links, Image URLs, References)
                safe_text = response.text.replace("http://", "https://")
                result = json.loads(safe_text)
                
                if 'topics' in result and result['topics']:
                    # --- Python Verification (Strict Mode Enforcement) ---
                    for topic in result['topics']:
                        if topic.get('category') == 'ì¶•ì œ/ì´ë²¤íŠ¸':
                            evt = topic.get('event_info')
                            # Check strict conditions
                            if not evt or not evt.get('location') or not evt.get('date') or not evt.get('price'):
                                print(f"   -> [Strict Mode] Downgrading '{topic['title']}' from Event to Travel News (Missing Info)")
                                topic['category'] = 'ì—¬í–‰/ê´€ê´‘'
                                topic['event_info'] = None # Clear it
                            elif evt.get('location') == 'Unknown' or evt.get('location') == 'null':
                                 print(f"   -> [Strict Mode] Downgrading '{topic['title']}' (Location Unknown)")
                                 topic['category'] = 'ì—¬í–‰/ê´€ê´€'
                                 topic['event_info'] = None

                    aggregated_topics.extend(result['topics'])
                    print(f"   -> Success. Topics so far: {len(aggregated_topics)}")
                    success = True
                else:
                    raise ValueError("Empty topics in response")
                
            except Exception as e:
                retry_count += 1
                wait_time = 2 ** retry_count # Exponential backoff: 2s, 4s, 8s
                print(f"   -> API Error for item {idx+1} (Attempt {retry_count}/{max_retries}): {e}")
                
                if "429" in str(e):
                    print("   -> Rate Limit Hit. Waiting longer...")
                    time.sleep(60) # Special wait for Rate Limit
                elif retry_count < max_retries:
                    print(f"   -> Retrying in {wait_time}s...")
                    time.sleep(wait_time)
                else:
                    print("   -> Max retries reached. Skipping this item.")
            
        # Delay logic (except for the last one)
        if idx < total_items - 1:
            print("   -> Waiting 20 seconds to respect API rate limits...")
            time.sleep(20)

    return {"topics": aggregated_topics}, None


def load_local_json(file_path):
    if os.path.exists(file_path):
        with open(file_path, 'r', encoding='utf-8') as f:
            try:
                return json.load(f)
            except:
                return {}
    return {}


# 3. Exchange Rate (THB -> KRW)
def get_thb_krw_rate():
    """
    Fetches the current THB to KRW exchange rate.
    Uses 'data/exchange_rate.json' for persistence.
    """
    RATE_FILE = 'data/exchange_rate.json'
    url = "https://api.frankfurter.app/latest?from=THB&to=KRW"
    
    # helper to save
    def save_rate(rate):
        try:
            with open(RATE_FILE, 'w', encoding='utf-8') as f:
                json.dump({"rate": rate, "updated_at": str(datetime.now())}, f)
        except: pass

    # helper to load
    def load_cached_rate():
        if os.path.exists(RATE_FILE):
            try:
                with open(RATE_FILE, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return data.get("rate")
            except: pass
        return None

    try:
        import requests
        response = requests.get(url, timeout=5)
        if response.status_code == 200:
            data = response.json()
            rate = data.get('rates', {}).get('KRW')
            if rate:
                save_rate(rate)
                return rate
    except Exception as e:
        print(f"Exchange Rate Error: {e}")
    
    # Fallback to cached rate if live fetch fails
    cached = load_cached_rate()
    if cached:
        return cached
        
    # If absolutely no data (first run ever & fail), return None or handled by UI
    return 0.0

# 4. Air Quality (WAQI)
def get_air_quality(token):
    """
    Fetches real-time Air Quality (PM 2.5) for Bangkok.
    Returns:
        dict: {'aqi': int, 'status': str} or None if failed.
    """
    url = f"https://api.waqi.info/feed/bangkok/?token={token}"
    try:
        import requests
        response = requests.get(url, timeout=5)
        if response.status_code == 200:
            data = response.json()
            if data.get('status') == 'ok':
                aqi = data['data']['aqi']
                return {'aqi': aqi}
    except Exception as e:
        print(f"Air Quality Error: {e}")
    
    return None



def fetch_thai_events():
    """
    Fetches and parses event information from ThaiTicketMajor, BK Magazine, and TAT News using Gemini.
    Returns:
        list: A list of event dictionaries (title, date, location, region, image_url, link, type).
    """
    print("Fetching Thai Events (National)...")
    
    targets = [
        {
            "name": "ThaiTicketMajor",
            "url": "https://www.thaiticketmajor.com/concert/",
            "selector": "body"
        },
        {
            "name": "BK Magazine",
            "url": "https://bk.asia-city.com/things-to-do-bangkok",
            "selector": "div.view-content"
        },
        {
            "name": "TAT News",
            "url": "https://www.tatnews.org/category/events-festivals/",
            "selector": "body"
        }
    ]

    combined_html_context = ""

    for target in targets:
        try:
            print(f" - Requesting {target['name']}...")
            headers = {'User-Agent': 'Mozilla/5.0'}
            response = requests.get(target['url'], headers=headers, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Extract relevant part
                content = soup.select_one(target['selector'])
                if not content:
                     content = soup.body
                
                # Kill scripts
                for s in content(["script", "style", "nav", "footer", "header"]):
                    s.extract()
                
                html_snippet = str(content)[:20000] # Increased limit for TAT
                
                combined_html_context += f"\n\n--- Source: {target['name']} ({target['url']}) ---\n{html_snippet}"
                
        except Exception as e:
            print(f"Error fetching {target['name']}: {e}")

    if not combined_html_context:
        return []

    # Gemini Processing
    try:
        # Load API Key (Handle Env vs Secrets)
        api_key = os.environ.get("GEMINI_API_KEY")
        if not api_key:
             try:
                import toml
                secrets = toml.load(".streamlit/secrets.toml")
                api_key = secrets.get("GEMINI_API_KEY")
             except:
                pass
        
        if not api_key:
            return []

        import google.generativeai as genai
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.0-flash', generation_config={"response_mime_type": "application/json"})
        
        today_str = datetime.now().strftime('%Y-%m-%d')
        
        prompt = f"""
        You are a helpful event curator for Korean tourists visiting Thailand.
        Analyze the following HTML snippets from event websites (ThaiTicketMajor, BK Magazine, TAT News).
        Extract a list of distinct events/festivals across Thailand.
        
        Current Date: {today_str}
        
        CRITICAL: Identify the **REGION** (City/Province) based on the location info.
        - If "Chiang Mai" -> "ì¹˜ì•™ë§ˆì´"
        - If "Phuket" -> "í‘¸ì¼“"
        - If "Pattaya" -> "íŒŒíƒ€ì•¼"
        - If "Bangkok" -> "ë°©ì½•"
        - If "Koh Samui" -> "ì½”ì‚¬ë¬´ì´"
        - If unknown or miscellaneous, default to "ê¸°íƒ€" (Others) or "ë°©ì½•" if mostly likely Bangkok.
        
        Return the result ONLY as a JSON list of objects.
        
        JSON Format:
        [
            {{
                "title": "Event Name (Summarize in Korean, e.g. 'ì†¡í¬ë€ ì¶•ì œ')",
                "date": "YYYY-MM-DD or Date Range String (e.g. '2024-04-13 ~')",
                "location": "Venue Name (in Korean or English)",
                "region": "ë°©ì½•/ì¹˜ì•™ë§ˆì´/í‘¸ì¼“/íŒŒíƒ€ì•¼/ê¸°íƒ€",
                "image_url": "Full URL of the event poster/image",
                "link": "Full URL to booking page or article",
                "booking_date": "YYYY-MM-DD HH:MM (Ticket Open Time) or 'Now Open' or 'TBD'",
                "price": "Exact Price (e.g. '3,000 THB') or range",
                "type": "ì¶•ì œ" or "ì½˜ì„œíŠ¸" or "ì „ì‹œ" or "ê¸°íƒ€"
            }}
        ]

        Rules:
        1. Select 8-12 diverse items (Mix of Concerts, Festivals, Exhibitions).
        2. CRITICAL: EXCLUDE events that ended BEFORE {today_str}. Only show current or future events.
        3. CRITICAL: If you see a date from a past year (e.g. 2024 if today is 2026, or 2017, 2018...), IGNORE IT. Do not output old events.
        4. Prefer events happening soon (next 45 days).
        3. Ensure image_url is absolute.
        4. Output strictly JSON.
        
        HTML Context:
        {combined_html_context}
        """
        
        print(" - Sending to Gemini...")
        response = model.generate_content(prompt)
        text = response.text.strip()
        
        # Clean markdown
        if text.startswith("```json"):
            text = text[7:]
        if text.endswith("```"):
            text = text[:-3]
            
        data = json.loads(text)
        print(f" - Parsed {len(data)} events with Region info.")
        return data

    except Exception as e:
        print(f"Gemini processing error: {e}")
        return []

def extract_event_from_url(url, api_key):
    """
    Scrapes a URL and uses Gemini to extract event details.
    Returns a dict with processed event info.
    """
    try:
        # 1. Scrape Content
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
             'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7'
        }
        resp = requests.get(url, headers=headers, timeout=10)
        resp.raise_for_status()
        
        soup = BeautifulSoup(resp.content, 'html.parser')
        
        # Remove scripts/styles
        for script in soup(["script", "style", "nav", "footer", "header"]):
            script.decompose()
            
        text_content = soup.get_text(separator=' ', strip=True)[:15000] # Limit context
        
        # Try to find OG Image
        og_image = ""
        meta_img = soup.find("meta", property="og:image")
        if meta_img:
            og_image = meta_img.get("content", "")
            
        title_guess = soup.title.string if soup.title else ""

        # 2. Gemini Analysis
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.0-flash-exp')
        
        prompt = f"""
        Analyze the following webpage text and extract event information.
        
        URL: {url}
        Page Title: {title_guess}
        Text Content:
        {text_content}
        
        Goal: Extract details for a "Big Match" event (Festival/Concert).
        
        Output JSON Format:
        {{
            "title": "Event Name (Korean, e.g. 'ë¡¤ë§ë¼ìš°ë“œ íƒœêµ­ 2024')",
            "date": "YYYY-MM-DD or Range (e.g. '2024-11-22 ~ 11-24')",
            "location": "Venue Name (Korean/English)",
            "region": "One of: ['ë°©ì½•', 'íŒŒíƒ€ì•¼', 'ì¹˜ì•™ë§ˆì´', 'í‘¸ì¼“', 'ê¸°íƒ€']",
            "type": "One of: ['ì¶•ì œ', 'ì½˜ì„œíŠ¸', 'ì „ì‹œ', 'í´ëŸ½/íŒŒí‹°', 'ê¸°íƒ€']",
            "booking_date": "Ticket Open Date (YYYY-MM-DD HH:MM) or 'Now Open'",
            "price": "Exact Price (e.g. '3,000 THB') or Range",
            "status": "One of: ['í‹°ì¼“ì˜¤í”ˆ', 'ê°œìµœí™•ì •', 'ë§¤ì§„', 'ì •ë³´ì—†ìŒ']",
            "image_url": "Use existing OG Image if valid, or find one in text. If none, return empty string.",
            "description": "1 line summary in Korean"
        }}
        
        If image_url is missing in text, use this one: {og_image}
        
        Translate all text to natural Korean.
        If information is missing, use "ì •ë³´ì—†ìŒ" or "" (empty string).
        """
        
        response = model.generate_content(prompt)
        text_response = response.text.strip()
        
        # Parse JSON
        if "```json" in text_response:
            text_response = text_response.replace("```json", "").replace("```", "")
        if text_response.startswith("```"): # Catch raw block
            text_response = text_response.replace("```", "")
        
        data = json.loads(text_response)
        data['link'] = url # Ensure link is set
        
        # Safety fallback for image
        if not data.get('image_url') and og_image:
            data['image_url'] = og_image
            
        return data, None
        
    except Exception as e:
        return None, str(e)

def fetch_big_events_by_keywords(keywords, api_key):
    """
    Crawls Google News RSS (Thailand Locale) for keywords and critically verifies details with Gemini.
    """
    import feedparser
    import urllib.parse
    
    found_events = []
    
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel('gemini-2.0-flash-exp')

    for kw in keywords:
        print(f"Checking keyword: {kw}")
        encoded_kw = urllib.parse.quote(kw)
        # Use Thailand Locale (en-TH)
        rss_url = f"https://news.google.com/rss/search?q={encoded_kw}&hl=en-TH&gl=TH&ceid=TH:en"
        
        feed = feedparser.parse(rss_url)
        
        # Check top 2 entries (efficient)
        entries_to_check = feed.entries[:2]
        if not entries_to_check:
            continue
            
        # Aggregate text for analysis
        combined_text = f"Target Event: {kw}\n"
        for i, entry in enumerate(entries_to_check):
            combined_text += f"[{i+1}] Title: {entry.title}\nLink: {entry.link}\nSummary: {entry.get('summary','')}\nPubDate: {entry.get('published','')}\n\n"
            
        prompt = f"""
        Analyze these news search results for the event "{kw}" in Thailand.
        
        News Content:
        {combined_text}
        
        Goal: Determine if there is CONFIRMED information about the NEXT event date and venue.
        
        CRITICAL VALIDATION RULES:
        1. **CONFIRMED ONLY**: Do NOT extract if it's just a "rumor", "expected to be", "in talks", or from a past year.
        2. **Future Only**: Date must be in the future (2025-2027).
        3. **Specifics**: You must find BOTH a specific date (or confirmed month) AND a venue/city.
        
        If the event is NOT confirmed or is just a rumor:
        Return JSON: {{ "found": false, "reason": "Just a rumor or no data" }}

        If CONFIRMED:
        Return JSON:
        {{
            "found": true,
            "title": "Event Name (Korean)",
            "date": "YYYY-MM-DD or Range",
            "location": "Venue Name",
            "booking_date": "Ticket Open Date (YYYY-MM-DD HH:MM) or 'TBD'",
            "price": "Exact Price (e.g. '3,000 THB') or Range",
            "status": "ê°œìµœí™•ì •", 
            "link": "Best Link URL from the news",
            "description": "1 line confirmed summary in Korean"
        }}
        """
        
        try:
            response = model.generate_content(prompt)
            text = response.text.strip()
            if "```json" in text:
                text = text.replace("```json", "").replace("```", "")
            if text.startswith("```"):
                text = text.replace("```", "")
                
            data = json.loads(text)
            
            if data.get('found'):
                # Basic validation
                if '201' in data.get('date',''): 
                     pass
                else:
                    found_events.append(data)
            else:
                print(f" -> {kw}: Not confirmed ({data.get('reason')})")
                    
        except Exception as e:
            print(f"Error analyzing {kw}: {e}")
            
    return found_events

# --------------------------------------------------------------------------------
# Trend Hunter (Magazine) Logic - 4 Sources
# --------------------------------------------------------------------------------

def fetch_trend_hunter_items(api_key, existing_links=None):
    """
    Aggregates trend/travel content via Google News RSS for 4 sources:
    1. Wongnai (Restaurants)
    2. TheSmartLocal TH (Hotspots)
    3. Chillpainai (Local Travel)
    4. BK Magazine (BKK Life)
    
    Returns:
        list: shuffled list of dicts {title, desc, location, image_url, link, badge}
    """
    import random
    import requests
    import feedparser
    
    print("Fetching Trend Hunter items via Google News RSS...")
    
    items = []
    if existing_links is None:
        existing_links = set()
    else:
        existing_links = set(existing_links)
        
    seen_links = set() # Local deduplication
    
    # Target Domains (Loaded from sources.json)
    SOURCES_FILE = 'data/sources.json'
    targets = []
    
    # 1. Try Loading from File
    if os.path.exists(SOURCES_FILE):
        try:
            with open(SOURCES_FILE, 'r', encoding='utf-8') as f:
                s_data = json.load(f)
                if s_data.get('magazine_targets'):
                    # Filter enabled only
                    targets = [t for t in s_data['magazine_targets'] if t.get('enabled', True)]
        except Exception as e:
            print(f"Error loading sources.json: {e}")
            
    # 2. Fallback if empty (Hardcoded defaults)
    if not targets:
        print("Using default magazine targets (Fallback).")
        targets = [
            {"name": "Wongnai", "domain": "wongnai.com", "tag": "[ë§›ì§‘ë­í‚¹]"},
            {"name": "Chillpainai", "domain": "chillpainai.com", "tag": "[ë¡œì»¬ì—¬í–‰]"},
            {"name": "BK Magazine", "domain": "bk.asia-city.com", "tag": "[ë°©ì½•ë¼ì´í”„]"},
            {"name": "The Smart Local", "domain": "thesmartlocal.co.th", "tag": "[MZí•«í”Œ]"}
        ]
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    # Helper: Gemini Analyzer
    def analyze_rss_items(raw_inputs, source_tag):
        if not raw_inputs: return []
        try:
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel('gemini-2.0-flash', generation_config={"response_mime_type": "application/json"})
            
            prompt = f"""
            You are a expert Korean Travel Editor acting as a **"Hotplace Detector"**.
            Your goal is to filter and rewrite RSS items into high-quality **Korean** magazine content.
            
            Input Data ({source_tag}):
            {json.dumps(raw_inputs, ensure_ascii=False)}
            
            **CRITICAL FILTERING RULES (Hotplace Detector)**:
            Analyze each item. If an item falls into any of these categories, **return null** for that item instead of a JSON object:
            1. **Not a specific visitable place**: General news, flight promos, "Thai Trip" general guides, or listicles without a clear focus.
            2. **Vague/Ad**: Content that sounds like a generic advertisement or lacks specific details.
            3. **No Image**: If you cannot infer a strong visual context or the input lacks an image.
            
            **REWRITE INSTRUCTIONS (For valid items)**:
            1. **LANGUAGE**: Natural, witty, trendy **Korean**.
            2. **INFERENCE**: Infer details (Vibe, Menu, Tips) from context.
            3. **FIELDS**:
               - "catchy_headline": Click-bait style 1-liner in Korean.
               - "desc": 2-3 sentences summary (Focus on why it's hot).
               - "location": Infer Area (e.g. 'Thong Lor', 'Siam').
               - "badge": Use "{source_tag}"
            
            Return JSON List of objects (excluding nulls).
            Example:
            [
                }},
                null,
                ...
            ]
            """
            
            response = model.generate_content(prompt)
            data = json.loads(response.text.strip().replace("```json", "").replace("```", ""))
            
            processed = []
            for res in data:
                if not res: continue # Skip null items (filtered)
                
                idx = res.get('original_index')
                if idx is not None and idx < len(raw_inputs):
                    original = raw_inputs[idx]
                    res['image_url'] = original.get('raw_img') 
                    res['link'] = original.get('raw_link')
                    res['badge'] = source_tag
                    processed.append(res)
            return processed
        except Exception as e:
            print(f"Analysis Error ({source_tag}): {e}")
            return []

    # Main Loop
    for target in targets:
        try:
            # Google News RSS URL (Reduced restriction)
            rss_url = f"https://news.google.com/rss/search?q=site:{target['domain']}&hl=en-TH&gl=TH&ceid=TH:en"
            print(f"Reading RSS: {target['name']}...")
            
            resp = requests.get(rss_url, headers=headers, timeout=10)
            feed = feedparser.parse(resp.content)
            
            raw_items = []
            # Check up to 10 entries to find 2 valid ones
            for entry in feed.entries[:10]:
                if len(raw_items) >= 2: break
                
                # 1. Deduplication (Link & Title)
                if entry.link in existing_links or entry.link in seen_links:
                    print(f"Skipping duplicate: {entry.title}")
                    continue
                
                # 2. Chillpainai Filter
                if target['name'] == "Chillpainai" and "Thai Trip" in entry.title:
                    print(f"Skipping Chillpainai 'Thai Trip': {entry.title}")
                    continue

                seen_links.add(entry.link)
                
                # Attempt to find image
                img_src = ""
                if 'media_content' in entry:
                    img_src = entry.media_content[0]['url']
                elif 'description' in entry:
                     import re
                     match = re.search(r'src="([^"]+)"', entry.description)
                     if match: img_src = match.group(1)
                
                raw_items.append({
                    "raw_title": entry.title,
                    "raw_link": entry.link,
                    "raw_img": img_src,
                    "context": f"Latest article from {target['name']}"
                })
            
            if raw_items:
                analyzed = analyze_rss_items(raw_items, target['tag'])
                items.extend(analyzed)
                
        except Exception as e:
            print(f"Error fetching {target['name']}: {e}")

    # Shuffle for Magazine feel
    random.shuffle(items)
    return items

def push_changes_to_github(files_to_commit, commit_message):
    """
    Commits and pushes specified files to GitHub.
    Requires GITHUB_TOKEN in secrets.toml or environment.
    """
    import subprocess
    import toml
    
    # 1. Get Token
    token = os.environ.get("GITHUB_TOKEN")
    if not token:
        try:
            secrets = toml.load(".streamlit/secrets.toml")
            token = secrets.get("GITHUB_TOKEN")
        except: pass
    
    if not token:
        return False, "GITHUB_TOKEN not found in secrets."

    # 2. Configure Git (If needed)
    # Check if user is set
    try:
        subprocess.run("git config user.name", shell=True, check=True, capture_output=True)
    except:
        subprocess.run('git config user.email "auto-deploy@streamlit.app"', shell=True)
        subprocess.run('git config user.name "Streamlit Admin"', shell=True)

    try:
        # 3. Add Files
        for f in files_to_commit:
            subprocess.run(f"git add {f}", shell=True, check=True)
            
        # 4. Commit
        subprocess.run(f'git commit -m "{commit_message}"', shell=True, check=True)
        
        # 5. Push
        # Use token in URL for auth
        repo_url = subprocess.check_output("git remote get-url origin", shell=True, text=True).strip()
        
        if "https://" in repo_url:
            auth_url = repo_url.replace("https://", f"https://{token}@")
        else:
            auth_url = repo_url
            
        subprocess.run(f"git push {auth_url} HEAD:main", shell=True, check=True)
        
        return True, "Successfully pushed to GitHub!"
        
    except subprocess.CalledProcessError as e:
        return False, f"Git Error: {e}"
    except Exception as e:
        return False, f"Error: {e}"


# --------------------------------------------------------------------------------
# Visitor Counter (counterapi.dev)
# --------------------------------------------------------------------------------

# --------------------------------------------------------------------------------
# Visitor Counter (counterapi.dev)
# --------------------------------------------------------------------------------

def get_visitor_stats():
    """
    Fetches both Total and Daily visitor counts.
    Returns: (total_count, daily_count)
    """
    try:
        import requests
        from datetime import datetime
        
        namespace = "today-thailand-app"
        today_str = datetime.now().strftime("%Y-%m-%d")
        
        # Keys
        key_total = f"total"
        key_daily = f"date_{today_str}"
        
        # 1. Get Total
        total_val = 0
        try:
            url_total = f"https://api.counterapi.dev/v1/{namespace}/{key_total}"
            r1 = requests.get(url_total, timeout=2)
            if r1.status_code == 200:
                total_val = r1.json().get("count", 0)
        except: pass
        
        # 2. Get Daily
        daily_val = 0
        try:
            url_daily = f"https://api.counterapi.dev/v1/{namespace}/{key_daily}"
            r2 = requests.get(url_daily, timeout=2)
            if r2.status_code == 200:
                daily_val = r2.json().get("count", 0)
        except: pass
            
        return total_val, daily_val
        
    except:
        return 0, 0

def increment_visitor_stats():
    """
    Increments both Total and Daily counts (once per session).
    Returns: (new_total, new_daily)
    """
    try:
        import requests
        from datetime import datetime
        
        namespace = "today-thailand-app"
        today_str = datetime.now().strftime("%Y-%m-%d")
        
        # Keys
        key_total = f"total"
        key_daily = f"date_{today_str}"
        
        # 1. Hit Total (+1)
        total_val = 0
        try:
            url_total = f"https://api.counterapi.dev/v1/{namespace}/{key_total}/up"
            r1 = requests.get(url_total, timeout=2)
            if r1.status_code == 200:
                total_val = r1.json().get("count", 0)
        except: pass
        
        # 2. Hit Daily (+1)
        daily_val = 0
        try:
            url_daily = f"https://api.counterapi.dev/v1/{namespace}/{key_daily}/up"
            r2 = requests.get(url_daily, timeout=2)
            if r2.status_code == 200:
                daily_val = r2.json().get("count", 0)
        except: pass
        
        return total_val, daily_val
        
    except:
        return 0, 0

# --------------------------------------------------------------------------------
# Twitter Trend Analyzer (trends24.in + Gemini)
# --------------------------------------------------------------------------------
def fetch_twitter_trends(api_key):
    """
    Scrapes trends24.in/thailand/ for top 10 hashtags and analyzes them with Gemini.
    Returns: dict { "topic": "...", "reason": "...", "severity": "info" } or None
    """
    import requests
    from bs4 import BeautifulSoup
    import google.generativeai as genai
    import json
    
    url = "https://trends24.in/thailand/"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        print("Fetching Twitter Trends from trends24.in...")
        resp = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(resp.content, 'html.parser')
        
        # trends24 structure: .trend-card__list (first one is latest) -> li -> a
        trend_list = soup.select('.trend-card__list')
        
        if not trend_list:
            print("No trend list found.")
            return None
            
        # Get top 10 from the most recent hour (first list)
        top_trends = []
        for li in trend_list[0].find_all('li')[:10]:
            top_trends.append(li.get_text(strip=True))
            
        if not top_trends:
            return None
            
        print(f"Top 10 Trends: {top_trends}")
        
        # Analyze with Gemini
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.0-flash-exp')
        
        prompt = f"""
        Analyze these real-time Thailand Twitter trends:
        {json.dumps(top_trends, ensure_ascii=False)}
        
        Goal: Identify if there is ANY critical or useful information for a FOREIGN TRAVELER in Bangkok/Thailand right now.
        
        Criteria:
        1.  **Critical**: Protests, Riots, Severe Weather (Floods), Transport Chaos (BTS/MRT breakdowns), Major Accidents.
        2.  **Useful**: K-Pop Star arrival (Airport crowds), Major Festival occurring NOW.
        3.  **Ignore**: Political gossip, Celebrity dating rumors, Fan-wars, TV show hashtags.
        
        Output JSON (Return null if nothing important):
        {{
            "topic": "Keyword or Hashtag",
            "reason": "1 simple sentence in KOREAN explaining the situation for a tourist. (e.g. 'ì‹œìœ„ë¡œ ì¸í•´ ì‹œì•” ì§€ì—­ì´ í˜¼ì¡í•˜ë‹ˆ í”¼í•˜ì„¸ìš”')",
            "severity": "warning" (for danger/disruption) or "info" (for events/crowds)
        }}
        
        * If multiple issues, pick the MOST critical one.
        * If nothing relevant, return null.
        """
        
        response = model.generate_content(prompt)
        result_text = response.text.strip().replace("```json", "").replace("```", "")
        
        # specific handling for null
        if "null" in result_text.lower() and len(result_text) < 10:
             return None
             
        data = json.loads(result_text)
        
        # Add Collection Time (Bangkok Time)
        import pytz
        from datetime import datetime
        bkk = pytz.timezone('Asia/Bangkok')
        now_bkk = datetime.now(bkk)
        
        data['collected_at'] = now_bkk.strftime("%Y-%m-%d %H:%M:%S")
        
        return data


    except Exception as e:
        print(f"Twitter Trend Error: {e}")
        return None

# --------------------------------------------------------
# Hotel Fact Check Features
# --------------------------------------------------------
import streamlit as st # Added for user requested st.error/st.warning

def fetch_hotel_candidates(hotel_name, city, api_key):
    """
    Step 1: Search for potential hotels (Candidates).
    Returns: List of dicts [{'id':..., 'name':..., 'address':...}] or None
    """
    # 1. Query Expansion (Removed forced 'Hotel' suffix)
    # Why? 'Centara' + 'Hotel' -> strictly matches 'Centara Hotel' (budget branch),
    # obscuring 'Centara Grand Mirage' (Resort).
    # Google Places TextSearch handles "Brand in City" better without forced suffixes.
    
    hotel_name = hotel_name.strip()
    
    # 2. Construct Query
    # Detect Korean to optimize query structure
    import re
    is_korean = bool(re.search(r'[ê°€-í£]', hotel_name))
    
    if is_korean:
         # 2-1. Brand Mapping (Korean -> English) for higher accuracy
         # Google Maps works significantly better with English brand names.
         brand_map = {
             "ì„¼íƒ€ë¼": "Centara",
             "ì•„ë§ˆë¦¬": "Amari",
             "ííŠ¼": "Hilton",
             "í•˜ì–íŠ¸": "Hyatt",
             "ë©”ë¦¬ì–´íŠ¸": "Marriott",
             "ì‰ë¼í†¤": "Sheraton",
             "í™€ë¦¬ë°ì´ì¸": "Holiday Inn",
             "ì•„ë‚œíƒ€ë¼": "Anantara",
             "ì•„ë°”ë‹ˆ": "Avani",
             "ë‘ì§“íƒ€ë‹ˆ": "Dusit Thani",
             "ë…¸ë³´í…”": "Novotel",
             "ë¥´ë©”ë¥´ë””ì•™": "Le Meridien",
             "ì†Œí”¼í…”": "Sofitel",
             "í’€ë§Œ": "Pullman",
             "ì¸í„°ì»¨í‹°ë„¨íƒˆ": "InterContinental",
             "ë°˜ì–€íŠ¸ë¦¬": "Banyan Tree",
             "ìƒ¹ê·¸ë¦´ë¼": "Shangri-La",
             "ì¼í•€ìŠ¤í‚¤": "Kempinski",
             "ì¹´í ë¼": "Capella",
             "í¬ì‹œì¦ŒìŠ¤": "Four Seasons",
             "ì„¸ì¸íŠ¸ë ˆì§€ìŠ¤": "St. Regis",
             "ë”ìŠ¤íƒ ë‹¤ë“œ": "The Standard"
         }
         
         # Check if hotel_name starts with or contains a known brand
         english_brand = None
         for kr_brand, en_brand in brand_map.items():
            if kr_brand in hotel_name:
                # Replace Korean Brand with English Brand in the query
                # e.g. "ì„¼íƒ€ë¼" -> "Centara"
                # e.g. "ì„¼íƒ€ë¼ ê·¸ëœë“œ" -> "Centara ê·¸ëœë“œ" (Mixed is fine, but pure English is best)
                # Let's just switch to English mode if it's a pure brand query
                if hotel_name.strip() == kr_brand:
                    hotel_name = en_brand
                    is_korean = False # Switch to English Logic
                else:
                    # Mixed case: "ì„¼íƒ€ë¼ ë¦¬ì¡°íŠ¸" -> replace 'ì„¼íƒ€ë¼' with 'Centara'
                    hotel_name = hotel_name.replace(kr_brand, en_brand)
                    # Keep is_korean = True for now unless we are sure, 
                    # but actually "Centara ë¦¬ì¡°íŠ¸" is better searched as "Centara Resort" (English logic handles mixed okay?)
                    # Let's try to trust the English Logic if we have English Name now.
                    # Actually better to treat as English-ish if we injected English Brand.
                    pass 
                break
    
    if is_korean:
         # Korean Fallback: Revert to Broad Search (No 'Hotel Resort' force)
         # 'Hotel Resort' keyword excluded pure Hotels (e.g. Centara Nova).
         # Broad search 'Name City Thailand' is safest for unmapped brands.
         search_query = f"{hotel_name} {city} Thailand"
    else:
         # English (or Mapped English): Use 'in' logic
         search_query = f"{hotel_name} in {city}, Thailand"
    
    url = "https://places.googleapis.com/v1/places:searchText"
    headers = {
        "Content-Type": "application/json",
        "X-Goog-Api-Key": api_key,
        "X-Goog-FieldMask": "places.id,places.displayName,places.formattedAddress"
    }
    # Limit to 10 candidates
    payload = {
        "textQuery": search_query,
        "maxResultCount": 20
    }
    
    try:
        response = requests.post(url, json=payload, headers=headers)
        if response.status_code != 200:
            # st.error(f"ğŸš¨ API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
            return None
            
        data = response.json()
        
        if not data.get("places"):
            return [] 
            
        # Extract meaningful candidates (Increased to 10)
        candidates = []
        for p in data["places"][:10]:
            candidates.append({
                "id": p["id"],
                "name": p.get("displayName", {}).get("text", "Unknown"),
                "address": p.get("formattedAddress", "")
            })
        return candidates

    except Exception as e:
        st.error(f"ì‹œìŠ¤í…œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def fetch_hotel_details(place_id, api_key):
    """
    Step 2: Fetch full details for a specific Place ID.
    Returns: place_dict or None
    """
    url = f"https://places.googleapis.com/v1/places/{place_id}"
    headers = {
        "X-Goog-Api-Key": api_key,
        "X-Goog-FieldMask": "id,displayName,formattedAddress,rating,userRatingCount,reviews,photos"
    }
    
    try:
        resp = requests.get(url, headers=headers)
        if resp.status_code != 200:
            st.error(f"ìƒì„¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {resp.text}")
            return None
            
        place = resp.json()
        
        # Photo handling
        photo_url = None
        if place.get("photos"):
            photo_ref = place["photos"][0]["name"]
            photo_url = f"https://places.googleapis.com/v1/{photo_ref}/media?maxHeightPx=800&maxWidthPx=800&key={api_key}"

        return {
            "name": place.get("displayName", {}).get("text", "Unknown"),
            "address": place.get("formattedAddress", ""),
            "rating": place.get("rating", 0.0),
            "review_count": place.get("userRatingCount", 0),
            "reviews": place.get("reviews", []),
            "photo_url": photo_url
        }
    except Exception as e:
        st.error(f"ìƒì„¸ ì •ë³´ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return None

def analyze_hotel_reviews(hotel_name, rating, reviews, api_key):
    """
    Analyze hotel reviews using Gemini with a specific 'Cold Inspector' persona.
    """
    try:
        # 1. Prepare Review Text
        reviews_text = ""
        for r in reviews[:5]: # Use top 5 reviews
             text = r.get("text", {}).get("text", "")
             if text:
                 reviews_text += f"- {text}\n"

        # 2. Gemini Prompt
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.0-flash', generation_config={"response_mime_type": "application/json"})

        prompt = f"""
        ë„ˆëŠ” 'ëƒ‰ì² í•œ í˜¸í…” ê²€ì¦ê°€'ì•¼. ì‚¬ìš©ìê°€ ì´ í˜¸í…”ì„ **"ì‹¤ì œë¡œ ì˜ˆì•½í• ì§€ ë§ì§€"** ê²°ì •í•  ìˆ˜ ìˆë„ë¡, ê´‘ê³  ë©˜íŠ¸ëŠ” ë¹¼ê³  ì˜¤ì§ **íŒ©íŠ¸ì™€ ì‹¤ì œ í›„ê¸°**ì— ê¸°ë°˜í•´ì„œ ë¶„ì„í•´ì¤˜.

        **[ë¶„ì„ ëŒ€ìƒ]**
        * í˜¸í…”ëª…: {hotel_name} (í‰ì : {rating})
        * êµ¬ê¸€ ë§µ ìµœì‹  ë¦¬ë·° ë°ì´í„°: {reviews_text}
        * **ì¶”ê°€ ì§€ì‹:** ìœ„ ë¦¬ë·° ì™¸ì—ë„, ë„¤ê°€ ì´ë¯¸ í•™ìŠµí•´ì„œ ì•Œê³  ìˆëŠ” ì´ í˜¸í…”ì˜ íŠ¹ì§•(ìœ„ì¹˜, ë¸Œëœë“œ í‰íŒ, ìˆ˜ì˜ì¥, ì¡°ì‹ ìŠ¤íƒ€ì¼ ë“±)ì„ ì´ë™ì›í•´.

        **[ë‹¨ì (Cons) ì‘ì„± ì ˆëŒ€ ê·œì¹™ - ìœ„ë°˜ ì‹œ ì˜¤ë‹µ ì²˜ë¦¬]**
        1. ğŸ”‡ **'ì—†ìŒ' ì¤‘ê³„ ê¸ˆì§€:** "ì†ŒìŒ ê´€ë ¨ ì–¸ê¸‰ ì—†ìŒ", "ìˆ˜ì•• ì •ë³´ ë¶€ì¡±", "ì¡°ì‹ ë¶ˆë§Œ ì—†ìŒ" ê°™ì´ **ë°ì´í„°ê°€ ì—†ë‹¤ëŠ” ì‚¬ì‹¤ì„ ì ì§€ ë§ˆ.** ì‚¬ìš©ìëŠ” 'ì§„ì§œ ë¬¸ì œì 'ë§Œ ê¶ê¸ˆí•´í•´.
        2. ğŸ¯ **ì˜¤ì§ 'ì¡´ì¬í•˜ëŠ” ë¶ˆë§Œ'ë§Œ:** ì‹¤ì œ ë¦¬ë·°ë‚˜ ë°ì´í„°ì—ì„œ **"ì‹œë„ëŸ½ë‹¤", "ë”ëŸ½ë‹¤", "ë§›ì—†ë‹¤", "ë©€ë‹¤"** ì²˜ëŸ¼ ëª…í™•í•˜ê²Œ ì§€ì ëœ ë¶€ì •ì  í‚¤ì›Œë“œê°€ ìˆì„ ë•Œë§Œ ì ì–´.
        3. ğŸ›¡ï¸ **ë¹ˆ ì¹¸ ì²˜ë¦¬:** ë§Œì•½ ìœ„ ê¸°ì¤€ìœ¼ë¡œ ë¶„ì„í–ˆì„ ë•Œ **ëª…í™•í•œ ë‹¨ì ì´ í•˜ë‚˜ë„ ì—†ë‹¤ë©´**, ì–µì§€ë¡œ ë§Œë“¤ì–´ë‚´ì§€ ë§ê³  ë”± í•œ ì¤„ë§Œ ì ì–´:
           ğŸ‘‰ "íŠ¹ë³„í•œ ë‹¨ì ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (ì „ë°˜ì ìœ¼ë¡œ ìš°ìˆ˜í•œ í‰ê°€)"
        4. **ê¸ˆì§€ ì˜ˆì‹œ:** "ìœ„ì¹˜ ê´€ë ¨ ì •ë³´ ë¶€ì¡±" (X), "í•œêµ­ì¸ ì…ë§› í™•ì¸ í•„ìš”" (X)

        **[ë¹„ì¶”ì²œ(Not Recommended) ì‘ì„± ê°€ì´ë“œ - ê¸°ê³„ì  ë©˜íŠ¸ ê¸ˆì§€]**
        1. ğŸš« **ê¸ˆì§€ í‘œí˜„:** "ë‹¨ì ì— ì˜ˆë¯¼í•œ ì‚¬ëŒ", "ì™„ë²½í•¨ì„ ì¶”êµ¬í•˜ëŠ” ì‚¬ëŒ", "ë¶ˆí¸í•¨ì„ ì‹«ì–´í•˜ëŠ” ì‚¬ëŒ" ê°™ì€ ë»”í•œ ë§ì€ ì“°ì§€ ë§ˆ.
        2. âœ… **êµ¬ì²´ì  ì¡°ê±´ ëª…ì‹œ:** ë¹„ì¶”ì²œ ëŒ€ìƒì€ ë°˜ë“œì‹œ **ê°€ê²©, ì†ŒìŒ, ìœ„ì¹˜, ê°ì„±** ë“± êµ¬ì²´ì  ì´ìœ ì™€ ì—°ê²°ë¼ì•¼ í•´.
           - (ì†ŒìŒ) ğŸ‘‰ "ì ê·€ê°€ ë°ê±°ë‚˜ ì¡°ìš©í•œ íœ´ì‹ì„ ìµœìš°ì„ ìœ¼ë¡œ í•˜ëŠ” ì—¬í–‰ê°"
           - (ìœ„ì¹˜) ğŸ‘‰ "ì§€í•˜ì² ì—­ê¹Œì§€ ë„ë³´ ì´ë™ì„ ì„ í˜¸í•˜ëŠ” ëšœë²…ì´ ì—¬í–‰ê°"
           - (ì²­ê²°) ğŸ‘‰ "ìœ„ìƒ ìƒíƒœì— ë¯¼ê°í•˜ê±°ë‚˜ ì•„ì´ì™€ í•¨ê»˜í•˜ëŠ” ê°€ì¡± ì—¬í–‰ê°"
        3. **ë‹¨ì ì´ ì—†ì„ ë•Œ:** ì–µì§€ë¡œ ë‹¨ì ì„ ì°¾ì§€ ë§ê³  **'ê°€ê²©'**ì´ë‚˜ **'ì—¬í–‰ ëª©ì '**ì„ ì–¸ê¸‰í•´.
           - (ë¹„ì‹¼ í˜¸í…”) ğŸ‘‰ "ê°€ì„±ë¹„ë¥¼ ì¤‘ìš”í•˜ê²Œ ìƒê°í•˜ëŠ” ì•Œëœ° ì—¬í–‰ê°"
           - (íŒŒí‹° í˜¸í…”) ğŸ‘‰ "ì¡°ìš©í•œ íë§ì„ ì›í•˜ëŠ” íœ´ì–‘ ëª©ì  ì—¬í–‰ê°"

        **[ì¶œë ¥ í¬ë§· (JSON)]**
        ì‘ë‹µì€ ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ì„ ì§€ì¼œì¤˜.

        {{
            "name_eng": "Trip.com ë“± OTAì—ì„œ ì‚¬ìš©í•˜ëŠ” í˜¸í…”ì˜ 'ì •ì‹ ì˜ë¬¸ í’€ë„¤ì„' (ì˜ˆ: Centara Grand at CentralWorld)",
            "trip_keyword": "íŠ¸ë¦½ë‹·ì»´ ê²€ìƒ‰ìš© 'í•œêµ­ì–´' í•µì‹¬ í‚¤ì›Œë“œ (ë„ì‹œ/êµ­ê°€ëª… ì œê±°, ë¸Œëœë“œ+ì§€ì ëª…ë§Œ ë‚¨ê¹€. ì˜ˆ: ì•„ë§ˆë¦¬ ì›Œí„°ê²Œì´íŠ¸)",
            "price_level": "ğŸ’° or ğŸ’°ğŸ’° or ğŸ’°ğŸ’°ğŸ’° or ğŸ’°ğŸ’°ğŸ’°ğŸ’° (1~4ë‹¨ê³„, ì €ë ´/ë³´í†µ/ë¹„ìŒˆ/ì´ˆí˜¸í™”)",
            "price_range_text": "í•œêµ­ ì›í™” ê¸°ì¤€ ì˜ˆìƒ 1ë°• ìš”ê¸ˆ (ì˜ˆ: ì•½ 120,000ì› ~ 180,000ì›, ì‹œì¦Œ ë³€ë™ ê°€ëŠ¥)",
            "one_line_verdict": "í•œ ì¤„ ê²°ë¡  (ì˜ˆ: ìœ„ì¹˜ëŠ” ê¹¡íŒ¨ì§€ë§Œ ê·€ë§ˆê°œ í•„ìˆ˜ì¸ ê°€ì„±ë¹„ í˜¸í…”)",
            "recommendation_target": "ì¶”ì²œ: ê¸ì •ì ì¸ ì„œë¹„ìŠ¤ ê²½í—˜ì„ ì¤‘ì‹œí•˜ëŠ” ì—¬í–‰ê°, ë¹„ì¶”ì²œ: í˜¸í…”ì˜ ì„±ê²©(ê°€ê²©Â·ë¶„ìœ„ê¸°Â·ìœ„ì¹˜)ê³¼ ë°˜ëŒ€ë˜ëŠ” ì—¬í–‰ì",
            "location_analysis": "ìœ„ì¹˜ ë° ë™ì„  (ì—­ê³¼ì˜ ê±°ë¦¬, ì£¼ë³€ í¸ì˜ì /ë§ˆì‚¬ì§€ìƒµ, ì¹˜ì•ˆ, ë„ë³´ ë‚œì´ë„)",
            "room_condition": "ê°ì‹¤ ë””í…Œì¼ (ì²­ê²°ë„, ì¹¨êµ¬, ìŠµê¸°/ëƒ„ìƒˆ, ì†ŒìŒ, ë²Œë ˆ, ë·°)",
            "service_breakfast": "ì„œë¹„ìŠ¤ ë° ì¡°ì‹ (ì§ì› ì¹œì ˆë„, ì¡°ì‹ ë©”ë‰´ êµ¬ì„± ë° ë§›, í•œêµ­ì¸ ì…ë§› ì í•©ë„)",
            "pool_facilities": "ìˆ˜ì˜ì¥ ë° ë¶€ëŒ€ì‹œì„¤ (ìˆ˜ì˜ì¥ í¬ê¸°/ìˆ˜ì§ˆ/ê·¸ëŠ˜ ì—¬ë¶€, í—¬ìŠ¤ì¥ ë“±)",
            "pros": ["ì¥ì 1 (êµ¬ì²´ì  ê·¼ê±°)", "ì¥ì 2", "ì¥ì 3", "ì¥ì 4", "ì¥ì 5"],
            "cons": ["ë‹¨ì 1 (ì¹˜ëª…ì ì¸ ë¶€ë¶„)", "ë‹¨ì 2", "ë‹¨ì 3", "ë‹¨ì 4", "ë‹¨ì 5"],
            "summary_score": {{
                "cleanliness": 0,  // 5ì  ë§Œì  (ì •ìˆ˜)
                "location": 0,
                "comfort": 0,
                "value": 0
            }}
        }}
        """
        
        response = model.generate_content(prompt)
        return json.loads(response.text)

    except Exception as e:
        return {"error": str(e)}

# --------------------------------------------------------------------------------
# Infographic Generator (PIL based)
# --------------------------------------------------------------------------------
def ensure_font_loaded():
    """
    Downloads NanumGothic font if not present.
    Returns path to font file.
    """
    FONT_URL = "https://github.com/google/fonts/raw/main/ofl/nanumgothic/NanumGothic-Bold.ttf"
    FONT_PATH = "data/NanumGothic-Bold.ttf"
    
    if not os.path.exists(FONT_PATH):
        try:
            print("Downloading font for Infographic...")
            import requests
            r = requests.get(FONT_URL, timeout=10)
            with open(FONT_PATH, 'wb') as f:
                f.write(r.content)
            print("Font downloaded.")
        except Exception as e:
            print(f"Font download failed: {e}")
            return None # Fallback to default
            
    return FONT_PATH

def prettify_infographic_text(category, items, api_key):
    """
    Uses Gemini to shorten news into 'Emoji + One-liner' format.
    """
    if not items: return []
    
    # Cost optimization: If API Key missing, just use titles
    if not api_key:
        return [f"ğŸ“° {item['title']}" for item in items[:3]]

    import google.generativeai as genai
    import json
    
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel('gemini-2.0-flash-exp')
    
    # Simplified inputs
    inputs = "\n".join([f"- {item['title']}" for item in items[:3]])
    
    prompt = f"""
    Convert these 3 news headlines into a "Social Media Infographic" style (Korean).
    Category: {category}
    
    Input:
    {inputs}
    
    Goal: Return a JSON list of strings. Each string must start with a relevant Emoji and be very short (max 20 chars).
    Example: ["ğŸš¨ ì‹œì•” íŒŒë¼ê³¤ ì´ê²© ë°œìƒ", "â›ˆï¸ ë‚´ì¼ ë°©ì½• í™ìˆ˜ ì£¼ì˜", "ğŸ‰ ì†¡í¬ë€ ì¶•ì œ ì¼ì • ë°œí‘œ"]
    
    Output JSON: {{ "lines": ["...", "...", "..."] }}
    """
    
    try:
        resp = model.generate_content(prompt)
        text = resp.text.strip().replace("```json", "").replace("```", "")
        if text.startswith("```"): text = text.replace("```", "")
        data = json.loads(text)
        return data.get("lines", [])
    except:
        # Fallback
        return [f"ğŸ“° {item['title'][:18]}..." for item in items[:3]]

def generate_category_infographic(category, items, date_str, api_key):
    """
    Generates a social media image for a specific category.
    """
    from PIL import Image, ImageDraw, ImageFont
    import os
    
    # 1. Config Map (Color & Text)
    # Categories: "ì •ì¹˜/ì‚¬íšŒ", "ê²½ì œ", "ì—¬í–‰/ê´€ê´‘", "ì‚¬ê±´/ì‚¬ê³ ", "ì¶•ì œ/ì´ë²¤íŠ¸", "ê¸°íƒ€"
    theme_map = {
        "ì •ì¹˜/ì‚¬íšŒ": {"color": (59, 130, 246), "bg_file": "assets/bg_politics.png", "title": "POLITICS & SOCIAL"}, # Blue
        "ê²½ì œ": {"color": (34, 197, 94), "bg_file": "assets/bg_economy.png", "title": "ECONOMY"}, # Green
        "ì—¬í–‰/ê´€ê´‘": {"color": (249, 115, 22), "bg_file": "assets/bg_travel.png", "title": "TRAVEL NEWS"}, # Orange
        "ì‚¬ê±´/ì‚¬ê³ ": {"color": (239, 68, 68), "bg_file": "assets/bg_safety.png", "title": "SAFETY ALERT"}, # Red
        "ì¶•ì œ/ì´ë²¤íŠ¸": {"color": (236, 72, 153), "bg_file": "assets/bg_travel.png", "title": "THAI EVENTS"}, # Pink
        "ê¸°íƒ€": {"color": (107, 114, 128), "bg_file": "assets/template.png", "title": "DAILY NEWS"} # Gray
    }
    
    theme = theme_map.get(category, theme_map["ê¸°íƒ€"])
    
    # 2. Get AI Content
    lines = prettify_infographic_text(category, items, api_key)
    if not lines: return None

    # 3. Setup Canvas (1080x1080 Square for Instagram)
    W, H = 1080, 1080
    
    # Background
    if os.path.exists(theme['bg_file']):
        img = Image.open(theme['bg_file']).convert("RGB")
        img = img.resize((W, H))
    else:
        # Create solid color background with gradient-ish look (simple solid for now)
        img = Image.new('RGB', (W, H), theme['color'])
        # Add a subtle dark overlay for text contrast
        overlay = Image.new('RGBA', (W, H), (0,0,0, 50))
        img.paste(overlay, (0,0), mask=overlay)

    draw = ImageDraw.Draw(img)
    
    # Fonts
    font_path = ensure_font_loaded()
    if not font_path:
        # Emergency fallback (might fail on korean)
        font_cat = ImageFont.load_default()
        font_date = ImageFont.load_default()
        font_body = ImageFont.load_default()
        font_footer = ImageFont.load_default()
    else:
        font_cat = ImageFont.truetype(font_path, 60)
        font_date = ImageFont.truetype(font_path, 40)
        font_body = ImageFont.truetype(font_path, 55)
        font_footer = ImageFont.truetype(font_path, 30)
        
    # Draw logic
    # Header: Category Title (English) + Date
    draw.text((80, 80), theme['title'], font=font_cat, fill="white")
    draw.text((80, 160), date_str, font=font_date, fill=(255, 255, 255, 200)) # Alpha 200
    
    # Divider
    draw.line((80, 230, 1000, 230), fill="white", width=4)
    
    # Body Content (Centered vertically-ish)
    start_y = 350
    gap = 120
    
    for i, line in enumerate(lines):
        # Draw badge/bullet?
        # Just text
        draw.text((80, start_y + (i * gap)), line, font=font_body, fill="white")
        
    # Footer
    draw.text((80, 1000), "ğŸ‡¹ğŸ‡­ ì˜¤ëŠ˜ì˜ íƒœêµ­ (Thai Briefing)", font=font_footer, fill=(255, 255, 255, 150))
    
# --------------------------------------------------------------------------------
# Taxi Fare Calculator (Google Maps + Rush Hour Logic)
# --------------------------------------------------------------------------------
def get_route_estimates(origin, destination, api_key):
    """
    Get Distance & Duration using Google Routes API (Compute Routes v2).
    Replaces legacy Directions API.
    Returns: dist_km, dur_min, traffic_ratio, error_message
    """
    if not origin or not destination:
        return None, None, None, "ì¶œë°œì§€ì™€ ëª©ì ì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."
        
    endpoint = "https://routes.googleapis.com/directions/v2:computeRoutes"
    
    # Prepare Origin/Dest objects
    def build_wp(val):
        if val.startswith("place_id:"):
            return {"placeId": val.split(":")[1]}
        else:
            return {"address": val}
            
    payload = {
        "origin": build_wp(origin),
        "destination": build_wp(destination),
        "travelMode": "DRIVE",
        "routingPreference": "TRAFFIC_AWARE", # Important for traffic data
        "computeAlternativeRoutes": False,
        "languageCode": "ko-KR",
        "units": "METRIC"
    }
    
    headers = {
        "Content-Type": "application/json",
        "X-Goog-Api-Key": api_key,
        "X-Goog-FieldMask": "routes.distanceMeters,routes.duration,routes.staticDuration"
    }
    
    try:
        import requests
        resp = requests.post(endpoint, json=payload, headers=headers, timeout=10)
        data = resp.json()
        
        if resp.status_code == 200:
            if not data.get("routes"):
                return None, None, None, "ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                
            route = data["routes"][0]
            
            # Distance (meters)
            dist_km = route.get("distanceMeters", 0) / 1000
            
            # Helper to parse "123s" string format
            def parse_duration(dur_str):
                if not dur_str: return 0
                return int(dur_str.replace("s", ""))
                
            # Duration (Real-time with TRAFFIC_AWARE)
            real_dur_sec = parse_duration(route.get("duration", "0s"))
            # Static Duration (No traffic)
            base_dur_sec = parse_duration(route.get("staticDuration", "0s"))
            
            dur_min = real_dur_sec / 60
            
            # Traffic Ratio
            traffic_ratio = 1.0
            if base_dur_sec > 0:
                traffic_ratio = real_dur_sec / base_dur_sec
            
            return dist_km, dur_min, traffic_ratio, None
            
        else:
            # API Error
            err_details = data.get("error", {})
            msg = err_details.get("message", "Unknown Error")
            status = err_details.get("status", resp.status_code)
            return None, None, None, f"Routes API ì˜¤ë¥˜ ({status}): {msg}"
            
    except Exception as e:
        return None, None, None, f"ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}"

def calculate_expert_fare(dist_km, dur_min, origin_txt="", dest_txt=""):
    """
    Calculates fair prices for various transport modes in Bangkok.
    Now includes Rush Hour Logic & Hell Zone Detection.
    
    Args:
        origin_txt (str): Name/Address of origin (for Hell Zone checking)
        dest_txt (str): Name/Address of dest
    """
    from datetime import datetime, time
    import pytz
    
    # 1. Check Rush Hour (Bangkok Time)
    tz_bkk = pytz.timezone('Asia/Bangkok')
    now_bkk = datetime.now(tz_bkk)
    current_time = now_bkk.time()
    
    is_rush_hour = False
    morning_start = time(7, 0)
    morning_end = time(9, 30)
    evening_start = time(16, 30)
    evening_end = time(20, 0)
    
    if (morning_start <= current_time <= morning_end) or \
       (evening_start <= current_time <= evening_end):
        is_rush_hour = True
        
    # 2. Check Hell Zone (Traffic Hell)
    hell_zones = ["Asok", "Sukhumvit", "Siam", "Sathorn", "Silom", "Thong Lo", "Phrom Phong"]
    chk_str = (str(origin_txt) + " " + str(dest_txt)).lower()
    is_hell_zone = any(z.lower() in chk_str for z in hell_zones)

    # 3. Base Meter Calculation
    # Note: 'dur_min' already includes traffic delay if Routes API works correclty.
    # Adjusted: Reduced time weight (2.5 -> 2.25) to be more realistic with modern traffic apps
    base_meter = 35 + (dist_km * 7) + (dur_min * 2.25)
    base_meter = int(base_meter)
    
    # 4. Multipliers
    # Tuned down Rush Hour Multiplier (1.5 -> 1.25) based on user feedback
    rush_mult = 1.25 if is_rush_hour else 1.0
    tuktuk_rush_mult = 1.2 if is_rush_hour else 1.0
    
    # Hell Zone Surcharge (1.1x) if applicable
    hell_mult = 1.1 if is_hell_zone else 1.0
    
    # Final App Multiplier (Combined)
    total_app_mult = rush_mult * hell_mult

    # Calculate raw prices (Adjusted down based on user feedback)
    # Target: Meter x (1.2 ~ 1.6 including surge)
    bolt_basic_raw = int(base_meter * 0.85 * total_app_mult)
    bolt_std_raw = int(base_meter * 1.0 * total_app_mult)
    grab_raw = int(base_meter * 1.1 * total_app_mult)
    
    # Grab Range (+- 10%)
    grab_min = int(grab_raw * 0.9)
    grab_max = int(grab_raw * 1.1)

    # Bike Range (+- 10%)
    bike_raw = 25 + (dist_km * 8)
    bike_min = int(bike_raw * 0.9)
    bike_max = int(bike_raw * 1.1)

    fares = {
        "bolt": {
            "label": "âš¡ Bolt (í†µí•©)",
            "price": f"{bolt_basic_raw} ~ {bolt_std_raw}",
            "tag": "ì°¨ ì¡ê¸° í˜ë“¦" if not is_rush_hour else "ë§¤ìš° ë¹„ìŒˆ",
            "color": "green" # Merged color
        },
        "grab_taxi": {
            "label": "ğŸ’š Grab (Standard)",
            "price": f"{grab_min} ~ {grab_max}",
            "tag": "ì•ˆì „/ë¹ ë¦„" if not is_rush_hour else "ë§¤ìš° ë¹„ìŒˆ",
            "color": "blue"
        },
        "bike": {
            "label": "ğŸï¸ ì˜¤í† ë°”ì´ (Win)",
            "price": f"{bike_min} ~ {bike_max}",
            "tag": "ğŸš€ ê°€ì¥ ë¹ ë¦„",
            "color": "orange",
            "warning_text": "âš ï¸ ì‚¬ê³  ìœ„í—˜ ë†’ìŒ / í—¬ë©§ í•„ìˆ˜ / ë³´í—˜ í™•ì¸"
        },
        "tuktuk": {
            "label": "ğŸ›º ëšëš (TukTuk)",
            "tag": "í˜‘ìƒ í•„ìˆ˜",
            "color": "red",
            "warning": True
        }
    }
    
    # Calc TukTuk Range
    tt_min = int(base_meter * 1.5 * tuktuk_rush_mult) 
    tt_max = int(base_meter * 2.0 * tuktuk_rush_mult)
    fares['tuktuk']['price'] = f"{tt_min} ~ {tt_max}"
    
    # ---------------------------------------------------------
    # 5. Intercity / Long Distance Logic (Flat Rate)
    # ---------------------------------------------------------
    is_intercity = False
    intercity_tip = None
    
    # Check Keywords (Priority)
    dest_lower = str(dest_txt).lower()
    
    flat_rates = {
        "pattaya": {"range": (1100, 1400), "tip": "ğŸšŒ ì—ê¹Œë§ˆì´ í„°ë¯¸ë„ì—ì„œ ë²„ìŠ¤ íƒ€ë©´ ì•½ 131ë°”íŠ¸!"},
        "hua hin": {"range": (2000, 2400), "tip": "ğŸš† ê¸°ì°¨ë‚˜ ë¯¸ë‹ˆë°´ì„ ì´ìš©í•˜ë©´ 200~400ë°”íŠ¸!"},
        "ayutthaya": {"range": (900, 1200), "tip": "ğŸš† ê¸°ì°¨(20ë°”íŠ¸~)ë‚˜ ë¯¸ë‹ˆë°´ì„ ì¶”ì²œí•©ë‹ˆë‹¤!"},
        "suvarnabhumi": {"range": (400, 500), "tip": "ğŸš† ê³µí•­ì² ë„(ARL)ë¥¼ íƒ€ë©´ ì‹œë‚´ê¹Œì§€ 45ë°”íŠ¸ ë‚´ì™¸!"} # Airport special
    }
    
    matched_zone = None
    for key, data in flat_rates.items():
        if key in dest_lower:
            matched_zone = data
            is_intercity = True
            break
            
    # Generic Long Distance (> 60km)
    if not matched_zone and dist_km >= 60:
        is_intercity = True
        # Formula: 1200 + ((dist - 100) * 10)
        est_price = 1200 + ((dist_km - 100) * 10)
        est_min = int(est_price * 0.9)
        est_max = int(est_price * 1.1)
        
        matched_zone = {"range": (est_min, est_max), "tip": "ğŸšŒ ì¥ê±°ë¦¬ ì´ë™ì€ ë²„ìŠ¤/ê¸°ì°¨/ë¯¸ë‹ˆë°´ ì´ìš©ì„ ê³ ë ¤í•´ë³´ì„¸ìš”! (í›¨ì”¬ ì €ë ´í•¨)"}

    if is_intercity and matched_zone:
        r_min, r_max = matched_zone['range']
        price_str = f"{r_min} ~ {r_max}"
        intercity_tip = matched_zone['tip']
        
        # Override Fares
        fares['bolt']['price'] = price_str
        fares['grab_taxi']['price'] = price_str # Apps often follow market flat rates for long distance
        fares['tuktuk']['price'] = "ìš´í–‰ ë¶ˆê°€" # Tuktuk highly unlikely
        fares['bike']['price'] = "ì¶”ì²œ ì•ˆí•¨"
    
    return base_meter, fares, is_rush_hour, is_hell_zone, intercity_tip

def search_places(query, api_key):
    """
    Search using Google Places Autocomplete API for better partial matching.
    Returns: {name, address, place_id}
    """
    if not query: return []
    
    # Use Autocomplete API as requested into order to support 'Top 10' predictions and 'components' filtering
    endpoint = "https://maps.googleapis.com/maps/api/place/autocomplete/json"
    params = {
        "input": query,
        "key": api_key,
        "language": "ko",
        "components": "country:TH" # Strict Thailand restriction
    }
    
    try:
        import requests
        resp = requests.get(endpoint, params=params, timeout=5)
        data = resp.json()
        
        candidates = []
        if data.get('status') == 'OK':
            for p in data.get('predictions', [])[:10]:
                main_text = p.get('structured_formatting', {}).get('main_text', '')
                sec_text = p.get('structured_formatting', {}).get('secondary_text', '')
                full_text = p.get('description', '')
                
                candidates.append({
                    "name": main_text if main_text else full_text,
                    "address": sec_text,
                    "place_id": p.get('place_id')
                })
        return candidates
    except Exception as e:
        print(f"Autocomplete Error: {e}")
        return []

# --------------------------------------------------------------------------------
# Wongnai Restaurant Analyzer
# --------------------------------------------------------------------------------
def search_wongnai_restaurant(restaurant_name, api_key=None):
    """
    Search for a restaurant on Wongnai using Google search.
    Tries legacy search first, and always falls back to Gemini if it fails or returns nothing.
    """
    found_url = None
    
    # 1. Try legacy search (might be throttled or throw exceptions)
    queries = [
        f"site:wongnai.com {restaurant_name}",
        f"wongnai {restaurant_name}"
    ]
    
    try:
        for query in queries:
            results = googlesearch.search(query, num_results=3)
            for url in results:
                if "wongnai.com/restaurants/" in url or "wongnai.com/r/" in url:
                    found_url = url
                    break
            if found_url: break
    except Exception as e:
        print(f"Legacy search failed: {e}")
        pass # Ignore legacy errors and move to Gemini fallback
    
    if found_url:
        return found_url
    
    # 2. Strong Fallback: Gemini Search
    if api_key:
        try:
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel('gemini-1.5-flash')
            prompt = f"Find the Wongnai restaurant URL for: {restaurant_name}. Return ONLY the direct URL starting with https://www.wongnai.com/restaurants/ or https://www.wongnai.com/r/"
            response = model.generate_content(prompt)
            raw_text = response.text.strip()
            
            # Extract URL more robustly
            match = re.search(r'(https?://(?:www\.)?wongnai\.com/(?:restaurants|r)/[^\s]+)', raw_text)
            if match:
                return match.group(1).rstrip('.')
        except Exception as e:
            print(f"Gemini fallback search error: {e}")
            
    return None

def scrape_wongnai_restaurant(url):
    """
    Scrape restaurant data from a Wongnai URL.
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    try:
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code != 200:
            return {"error": f"í˜„ì‹œì  ì›¡ë‚˜ì´ ì ‘ì†ì´ ì›í™œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ (Code: {response.status_code})"}

        soup = BeautifulSoup(response.text, 'html.parser')
        
        # 1. Name (Wongnai uses dynamic classes sometimes, but h1 is fairly stable)
        name_tag = soup.find('h1')
        name = name_tag.get_text(strip=True) if name_tag else "Unknown Restaurant"
        
        # 2. Score
        # Typically in a span or div with specific class patterns
        score_tag = soup.find(string=re.compile(r'^\d\.\d$')) # Looks for "4.5" etc.
        score = score_tag.strip() if score_tag else "ë°ì´í„° ì—†ìŒ"
        
        # 3. Price
        price_tag = soup.find(string=re.compile(r'^[à¸¿]+$')) # Looks for "à¸¿à¸¿", "à¸¿à¸¿à¸¿"
        price = price_tag.strip() if price_tag else "ë°ì´í„° ì—†ìŒ"
        
        # 4. Photo
        # Find first large image
        photo_url = None
        img_tags = soup.find_all('img')
        for img in img_tags:
            src = img.get('src', '')
            if 'wongnai.com' in src and '/static2/' not in src: # Avoid icons/loaders
                photo_url = src
                break
        
        # 5. Reviews
        reviews = []
        # Wongnai reviews are often in complex structures
        # We try to grab text blocks that look like reviews
        review_texts = soup.find_all(['p', 'span', 'div'], string=re.compile(r'.{20,}'))
        count = 0
        for rt in review_texts:
            text = rt.get_text(strip=True)
            if len(text) > 40 and count < 10:
                reviews.append(text)
                count += 1
            
        return {
            "name": name,
            "score": score,
            "price": price,
            "photo_url": photo_url,
            "reviews": reviews,
            "url": url
        }
    except Exception as e:
        return {"error": f"ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {str(e)}"}

def analyze_wongnai_data(restaurant_data, api_key):
    """
    Analyze Wongnai data using Gemini AI.
    """
    if "error" in restaurant_data:
        return restaurant_data

    genai.configure(api_key=api_key)
    model = genai.GenerativeModel('gemini-2.0-flash')

    reviews_text = "\n".join([f"- {r[:200]}..." for r in restaurant_data['reviews']])
    
    prompt = f"""
    íƒœêµ­ í˜„ì§€ì¸ ë§›ì§‘ ì‚¬ì´íŠ¸ 'Wongnai'ì˜ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´ ì‹ë‹¹ì„ í•œêµ­ì¸ ì—¬í–‰ê° ê´€ì ì—ì„œ ë¶„ì„í•´ì¤˜.

    [ì‹ë‹¹ ì •ë³´]
    - ì´ë¦„: {restaurant_data['name']}
    - ì›¡ë‚˜ì´ ë³„ì : {restaurant_data['score']}
    - íƒœêµ­ í˜„ì§€ ê°€ê²©ëŒ€: {restaurant_data['price']}
    
    [í˜„ì§€ ë¦¬ë·° ë°ì´í„° ìš”ì•½]
    {reviews_text}

    [ë¶„ì„ ê²°ê³¼ í•„ìˆ˜ í¬í•¨ ì‚¬í•­ (í•œêµ­ì–´ë¡œ ì‘ì„±)]:
    1. â­ í˜„ì§€ì¸ ë³„ì  ë¶„ìœ„ê¸° (ì ìˆ˜ê°€ ë†’ì€ì§€, ë¡œì»¬ ì‚¬ëŒë“¤ì—ê²Œ ì¸ê¸° ìˆëŠ” ê³³ì¸ì§€)
    2. ğŸ½ï¸ ì¶”ì²œ ë©”ë‰´ (ë¦¬ë·°ì—ì„œ ê°€ì¥ ë§ì´ ì¹­ì°¬ë°›ëŠ” ìŒì‹ ë˜ëŠ” ëŒ€í‘œ ë©”ë‰´)
    3. ğŸ‡°ğŸ‡· í•œêµ­ì¸ ì…ë§› ì í•©ë„ (ë§µê¸°, í–¥ì‹ ë£Œ ê°•ë„, í•œêµ­ì¸ì´ ì¢‹ì•„í•  ë§Œí•œ í¬ì¸íŠ¸)
    4. ğŸ’° ì²´ê° ë¬¼ê°€ (íƒœêµ­ ë¡œì»¬ ë¬¼ê°€ ëŒ€ë¹„ ì–´ëŠ ì •ë„ ìˆ˜ì¤€ì¸ì§€)
    5. ğŸš« ì£¼ì˜ì‚¬í•­ (ì›¨ì´íŒ… ì—¬ë¶€, ìœ„ì¹˜ì  íŠ¹ì§•, ì„œë¹„ìŠ¤ ê´€ë ¨ ì§€ì  ë“±)

    ì¹œì ˆí•˜ê³  ì‹ ë¢°ê° ìˆëŠ” ë§íˆ¬ë¡œ ìš”ì•½í•´ì„œ ë‹µë³€í•´ì¤˜. ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì„ ì‚¬ìš©í•˜ì—¬ ê°€ë…ì„± ìˆê²Œ ì‘ì„±í•  ê²ƒ.
    """
    
    try:
        response = model.generate_content(prompt)
        return {
            "summary": response.text,
            "info": restaurant_data
        }
    except Exception as e:
        return {"error": f"Gemini ë¶„ì„ ì‹¤íŒ¨: {e}"}
