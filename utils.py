import feedparser
import googlesearch
import google.generativeai as genai
from datetime import datetime, timedelta
import time
import json
import os
# import certifi
# os.environ["SSL_CERT_FILE"] = certifi.where()
import certifi
import os
os.environ["SSL_CERT_FILE"] = certifi.where()
import requests
import re
from bs4 import BeautifulSoup
import gspread
from oauth2client.service_account import ServiceAccountCredentials
import pandas as pd
import numpy as np
import csv
from streamlit_gsheets import GSheetsConnection
import streamlit as st
import pathlib

# --- GA4 (Google Analytics 4) Injection ---
@st.cache_resource
def inject_ga(ga_id):
    """
    Injects Google Analytics 4 tracking code into the Streamlit 'index.html' file.
    Runs once per server session using @st.cache_resource.
    """
    try:
        # 1. Locate index.html path
        # Streamlit library usually resides in site-packages/streamlit
        import streamlit
        st_path = pathlib.Path(streamlit.__path__[0])
        index_path = st_path / "static" / "index.html"

        if not index_path.exists():
            return

        # 2. Read index.html content
        with open(index_path, "r", encoding="utf-8") as f:
            html_content = f.read()

        # 3. Check for existing GA4 script
        if f"googletagmanager.com/gtag/js?id={ga_id}" in html_content:
            return

        # 4. Prepare GA4 script
        ga_script = f"""
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id={ga_id}"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){{dataLayer.push(arguments);}}
        gtag('js', new Date());
        gtag('config', '{ga_id}');
    </script>
"""
        # 5. Inject script before </head> tag
        new_html_content = html_content.replace("</head>", f"{ga_script}</head>")

        # 6. Write back to index.html
        with open(index_path, "w", encoding="utf-8") as f:
            f.write(new_html_content)

    except Exception:
        # Fail silently as per requirements
        pass

# --- ë‹¤êµ­ì–´ ì§€ì› (Multi-language Support) ---
UI_TEXT = {
    "main_title": {"ko": "ì˜¤ëŠ˜ì˜ íƒœêµ­ ğŸ‡¹ğŸ‡­", "en": "Thai Today ğŸ‡¹ğŸ‡­"},
    "main_subtitle": {"ko": "ë°©ì½• ë§›ì§‘, ë‰´ìŠ¤, ì—¬í–‰ í•„ìˆ˜ ì•±", "en": "Your Essential Guide to Bangkok"},
    "nav_news": {"ko": "ğŸ“° ë‰´ìŠ¤", "en": "ğŸ“° News"},
    "nav_hotel": {"ko": "ğŸ¨ í˜¸í…”", "en": "ğŸ¨ Hotel"},
    "nav_food": {"ko": "ğŸ½ï¸ ë§›ì§‘", "en": "ğŸ½ï¸ Taste"},
    "nav_guide": {"ko": "ğŸ“˜ ê°€ì´ë“œ", "en": "ğŸ“˜ Tour"},
    "nav_tour": {"ko": "ğŸ’ íˆ¬ì–´", "en": "ğŸ’ Tour"},
    "nav_taxi": {"ko": "ğŸš• íƒì‹œ", "en": "ğŸš• Taxi"},
    "nav_event": {"ko": "ğŸª ì´ë²¤íŠ¸", "en": "ğŸª Events"},
    "nav_board": {"ko": "ğŸ—£ï¸ ê²Œì‹œíŒ", "en": "ğŸ—£ï¸ Board"},
    "sidebar_menu": {"ko": "ğŸ“Œ ë©”ë‰´ ì„ íƒ", "en": "ğŸ“Œ Menu Selection"},
    "sidebar_info": {"ko": "ğŸ’¡ ì •ë³´ & ì§€ì›", "en": "ğŸ’¡ Info & Support"},
    "sidebar_lang": {"ko": "ğŸŒ ì–¸ì–´ ì„¤ì • (Language)", "en": "ğŸŒ Language Settings"},
    "about_title": {"ko": "â„¹ï¸ ì„œë¹„ìŠ¤ ì •ë³´ (About)", "en": "â„¹ï¸ About Service"},
    "about_desc": {
        "ko": "ì‹¤ì‹œê°„ íƒœêµ­ ì—¬í–‰ ì •ë³´, ë‰´ìŠ¤, í•«í”Œì„ í•œëˆˆì—! íƒœêµ­ ì •ë³´ê°€ í•„ìš”í•œ ëª¨ë“  ë¶„ë“¤ì„ ìœ„í•œ AI ê¸°ë°˜ ë¸Œë¦¬í•‘ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.",
        "en": "Real-time Thailand travel info, news, and hot spots at a glance! An AI-powered briefing service for everyone who needs info about Thailand."
    },
    "search_news": {"ko": "ğŸ” ë‚ ì§œ ê²€ìƒ‰ ë° ì˜µì…˜", "en": "ğŸ” Date Search & Options"},
    "search_keyword": {"ko": "ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰", "en": "ğŸ” Keyword Search"},
    "search_date": {"ko": "ğŸ“… ë‚ ì§œ ì„ íƒ", "en": "ğŸ“… Select Date"},
    "reset_search": {"ko": "ğŸ”„ ê²€ìƒ‰ì–´ ì´ˆê¸°í™”", "en": "ğŸ”„ Reset Search"},
    "news_header": {"ko": "ğŸ“… {} ë¸Œë¦¬í•‘", "en": "ğŸ“… {} Briefing"},
    "air_quality": {"ko": "ğŸŒ¬ï¸ ë°©ì½• ëŒ€ê¸°ì§ˆ", "en": "ğŸŒ¬ï¸ Bangkok Air Quality"},
    "exchange_rate": {"ko": "ğŸ’µ í™˜ìœ¨ (KRW/THB)", "en": "ğŸ’µ Exchange Rate"},
    "stat_today": {"ko": "ì˜¤ëŠ˜", "en": "Today"},
    "stat_total": {"ko": "ì „ì²´", "en": "Total"},
    "hotel_fact": {"ko": "ğŸ¨ í˜¸í…” íŒ©íŠ¸ì²´í¬", "en": "ğŸ¨ Hotel Fact Check"},
    "food_fact": {"ko": "ğŸœ ë§›ì§‘ íŒ©íŠ¸ì²´í¬", "en": "ğŸœ Taste Fact Check"},
    "food_desc": {"ko": "ì¸ìŠ¤íƒ€ ë§›ì§‘ì˜ ì§„ì‹¤! êµ¬ê¸€ ë§µ ë°ì´í„°ë¡œ ì§„ì§œ ë§›ì§‘ì¸ì§€ íŒë³„í•©ë‹ˆë‹¤.", "en": "The truth about trending spots! Verify real restaurants using Google Maps data."},
    "search_rest": {"ko": "ğŸ” ë§›ì§‘ ê²€ìƒ‰", "en": "ğŸ” Search Restaurant"},
    "rest_placeholder": {"ko": "ì˜ˆ: íŒì‚¬ë§ˆì´, Thip Samai, Zabb One", "en": "e.g., Thip Samai, Zabb One"},
    "hotel_search": {"ko": "ğŸ¨ í˜¸í…” ê²€ìƒ‰", "en": "ğŸ¨ Search Hotel"},
    "hotel_placeholder": {"ko": "ì˜ˆ: ë°©ì½• ë§¤ë¦¬ì–´íŠ¸, í˜ë‹ŒìŠë¼ ë°©ì½•", "en": "e.g., Marriott Bangkok, Peninsula"},
    "analysis_btn": {"ko": "ğŸ“Š íŒ©íŠ¸ì²´í¬ ë¶„ì„ ì‹œì‘", "en": "ğŸ“Š Start Fact Check Analysis"},
    "searching": {"ko": "ğŸ” ê²€ìƒ‰ ì¤‘...", "en": "ğŸ” Searching..."},
    "analyzing": {"ko": "ğŸ” ë°ì´í„° ë¶„ì„ ì¤‘...", "en": "ğŸ” Analyzing data..."},
    "no_results": {"ko": "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.", "en": "No results found."},
    "basic_info": {"ko": "â„¹ï¸ ê¸°ë³¸ ì •ë³´", "en": "â„¹ï¸ Basic Info"},
    "fact_report": {"ko": "âœ… íŒ©íŠ¸ì²´í¬ ë¦¬í¬íŠ¸", "en": "âœ… Fact Check Report"},
    "pros_cons": {"ko": "âš–ï¸ ì¥ë‹¨ì  ìš”ì•½", "en": "âš–ï¸ Pros & Cons"},
    "verdict": {"ko": "ğŸ“¢ ìš”ì•½ ë° íŒì •", "en": "ğŸ“¢ Verdict & Summary"},
    "best_review": {"ko": "ğŸ’¬ ë² ìŠ¤íŠ¸ ë¦¬ë·°", "en": "ğŸ’¬ Best Review"},
    "share_btn": {"ko": "ğŸ”— ìš”ì•½ ê²°ê³¼ ê³µìœ í•˜ê¸°", "en": "ğŸ”— Share Summary"},
    "rating_caption": {"ko": "5.0ì  ë§Œì  Â· ë¦¬ë·° {num_reviews:,}ê°œ", "en": "Out of 5.0 Â· {num_reviews:,} reviews"},
    "recommend_menu": {"ko": "ğŸ”¥ ë¦¬ë·°ì–´ë“¤ì˜ ì¶”ì²œ ë©”ë‰´", "en": "ğŸ”¥ Recommended by Reviewers"},
    "photo_caption": {"ko": "ğŸ“ ì‚¬ì§„ ì¶œì²˜: Google Maps ì‚¬ìš©ì ë¦¬ë·°", "en": "ğŸ“ Source: Google Maps user reviews"},
    "price_range": {"ko": "ğŸ’° ê°€ê²©ëŒ€", "en": "ğŸ’° Price Range"},
    "cuisine_type": {"ko": "ğŸ½ï¸ ìš”ë¦¬ ì¢…ë¥˜", "en": "ğŸ½ï¸ Cuisine"},
    "opening_status": {"ko": "ğŸ• ì˜ì—…ìƒíƒœ", "en": "ğŸ• Status"},
    "photos": {"ko": "ğŸ“¸ ì‚¬ì§„", "en": "ğŸ“¸ Photos"},
    "hotel_city": {"ko": "ì§€ì—­ (City)", "en": "City"},
    "hotel_find": {"ko": "ğŸ” í˜¸í…” ì°¾ê¸°", "en": "ğŸ” Find Hotel"},
    "hotel_select": {"ko": "ê²€ìƒ‰ëœ í˜¸í…” ì„ íƒ", "en": "Select a hotel"},
    "hotel_back": {"ko": "â¬…ï¸ ê²€ìƒ‰ ê²°ê³¼ë¡œ ëŒì•„ê°€ê¸°", "en": "â¬…ï¸ Back to results"},
    "pros_title": {"ko": "âœ… ì¥ì ", "en": "âœ… Pros"},
    "cons_title": {"ko": "âŒ ë‹¨ì  & ì£¼ì˜ì‚¬í•­", "en": "âŒ Cons & Cautions"},
    "location_title": {"ko": "ğŸ“ ìœ„ì¹˜ ë° ë™ì„ ", "en": "ğŸ“ Location & Traffic"},
    "room_title": {"ko": "ğŸ›ï¸ ë£¸ ì»¨ë””ì…˜", "en": "ğŸ›ï¸ Room Condition"},
    "service_title": {"ko": "ğŸ½ï¸ ì„œë¹„ìŠ¤ & ì¡°ì‹", "en": "ğŸ½ï¸ Service & Breakfast"},
    "facility_title": {"ko": "ğŸŠâ€â™‚ï¸ ìˆ˜ì˜ì¥ & ë¶€ëŒ€ì‹œì„¤", "en": "ğŸŠâ€â™‚ï¸ Pool & Facilities"},
    "score_title": {"ko": "ğŸ“Š íŒ©íŠ¸ì²´í¬ ì ìˆ˜", "en": "ğŸ“Š Fact Check Score"},
    "cleanliness": {"ko": "ì²­ê²°ë„", "en": "Cleanliness"},
    "location": {"ko": "ìœ„ì¹˜", "en": "Location"},
    "comfort": {"ko": "í¸ì•ˆí•¨", "en": "Comfort"},
    "value": {"ko": "ê°€ì„±ë¹„", "en": "Value"},
    "share_friend": {"ko": "ğŸ“¢ ì¹œêµ¬ì—ê²Œ ê³µìœ í•˜ê¸° (ë³µì‚¬)", "en": "ğŸ“¢ Share with friends (Copy)"},
    "share_caption": {"ko": "ğŸ‘† ìœ„ í…ìŠ¤íŠ¸ ìš°ì¸¡ ìƒë‹¨ ë³µì‚¬ ë²„íŠ¼ì„ ëˆŒëŸ¬ ì¹´í†¡ì— ë¶™ì—¬ë„£ìœ¼ì„¸ìš”!", "en": "ğŸ‘† Click the copy button in the top right to share."},
    "hotel_desc": {"ko": "ê´‘ê³  ì—†ëŠ” 'ì°' í›„ê¸° ë¶„ì„! êµ¬ê¸€ ë§µ ë¦¬ë·°ë¥¼ ëƒ‰ì² í•˜ê²Œ ê²€ì¦í•´ë“œë¦½ë‹ˆë‹¤.", "en": "Ad-free review analysis! Verifying Google Maps reviews with AI objectivity."},
    "issue_label": {"ko": "**[ì‹¤ì‹œê°„ ë°©ì½• ì´ìŠˆ]**", "en": "**[Real-time BKK Issue]**"},
    "as_of": {"ko": "{} ê¸°ì¤€", "en": "as of {}"},
    "guide_title": {"ko": "ğŸ“˜ íƒœêµ­ ì—¬í–‰ ê°€ì´ë“œ", "en": "ğŸ“˜ Travel Guide"},
    "guide_desc": {"ko": "í˜„ì§€ì¸ì²˜ëŸ¼ ì—¬í–‰í•˜ê¸°! ì‹¤ì† ìˆëŠ” íƒœêµ­ ì—¬í–‰ ê¿€íŒì„ ëª¨ì•˜ìŠµë‹ˆë‹¤.", "en": "Travel like a local! Essential tips for your Thailand trip."},
    "back_to_list": {"ko": "â¬…ï¸ ëª©ë¡ìœ¼ë¡œ ëŒì•„ê°€ê¸°", "en": "â¬…ï¸ Back to list"},
    "share_help": {"ko": "ğŸ“ ì´ ê¸€ì´ ë„ì›€ì´ ë˜ì…¨ë‹¤ë©´ ê³µìœ í•´ì£¼ì„¸ìš”!", "en": "ğŸ“ Share this if it was helpful!"},
    "no_guide": {"ko": "ğŸ“ ì•„ì§ ë“±ë¡ëœ ì—¬í–‰ ê°€ì´ë“œê°€ ì—†ìŠµë‹ˆë‹¤. ê³§ ìœ ìš©í•œ ê¸€ì´ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤!", "en": "ğŸ“ No guides available yet. Stay tuned!"},
    "read_more": {"ko": "ğŸ“– ìì„¸íˆ ë³´ê¸°", "en": "ğŸ“– Read More"},
    "taxi_title": {"ko": "ğŸš• íƒì‹œ/ëšëš ìš”ê¸ˆ íŒë…ê¸°", "en": "ğŸš• Taxi/TukTuk Fare Reader"},
    "taxi_desc": {"ko": "ë°©ì½• ì‹œë‚´ êµí†µë¹„, ë°”ê°€ì§€ì¸ì§€ ì•„ë‹Œì§€ 1ì´ˆ ë§Œì— íŒë…í•´ë“œë¦½ë‹ˆë‹¤.", "en": "Check if your Bangkok taxi fare is fair in 1 second."},
    "route_set": {"ko": "ğŸ“ ê²½ë¡œ ì„¤ì • (ì¥ì†Œ ê²€ìƒ‰)", "en": "ğŸ“ Route Settings (Search)"},
    "from": {"ko": "ì¶œë°œì§€ (From)", "en": "From"},
    "to": {"ko": "ë„ì°©ì§€ (To)", "en": "To"},
    "search": {"ko": "ğŸ” ê²€ìƒ‰", "en": "ğŸ” Search"},
    "calc_fare": {"ko": "ğŸ’¸ ê²½ë¡œ ë° ìš”ê¸ˆ ê³„ì‚°", "en": "ğŸ’¸ Calculate Fare"},
    "distance": {"ko": "ğŸ“ ì˜ˆìƒ ê±°ë¦¬", "en": "ğŸ“ Estimated Distance"},
    "duration": {"ko": "â±ï¸ ì†Œìš” ì‹œê°„", "en": "â±ï¸ Estimated Time"},
    "fare_table": {"ko": "ğŸ’° êµí†µìˆ˜ë‹¨ë³„ ì ì • ìš”ê¸ˆí‘œ", "en": "ğŸ’° Fair Fare by Transport"},
    "tour_title": {"ko": "ğŸ’ AI íˆ¬ì–´ ì½”ë””ë„¤ì´í„°", "en": "ğŸ’ AI Travel Planner"},
    "tour_desc": {"ko": "ë‹¹ì‹ ì˜ ì·¨í–¥ì— ë”± ë§ëŠ” íƒœêµ­ ì—¬í–‰ì„ ì„¤ê³„í•´ë“œë¦½ë‹ˆë‹¤. ì›í•˜ëŠ” ì¡°ê±´ì„ ì„ íƒí•˜ì„¸ìš”!", "en": "Design a Thailand trip that fits your style. Select your preferences!"},
    "tour_who": {"ko": "ëˆ„êµ¬ì™€ í•¨ê»˜ ê°€ì‹œë‚˜ìš”?", "en": "Who are you traveling with?"},
    "tour_style": {"ko": "ì–´ë–¤ ìŠ¤íƒ€ì¼ì˜ ì—¬í–‰ì„ ì„ í˜¸í•˜ì‹œë‚˜ìš”?", "en": "What is your travel style?"},
    "tour_budget": {"ko": "ì˜ˆì‚°ì€ ì–´ëŠ ì •ë„ ìƒê°í•˜ì‹œë‚˜ìš”?", "en": "What is your budget?"},
    "tour_find_btn": {"ko": "ğŸš€ ë‚˜ì—ê²Œ ë§ëŠ” íˆ¬ì–´ ì°¾ê¸°", "en": "ğŸš€ Find Tours for Me"},
    "tour_result_title": {"ko": "âœ¨ AI ì¶”ì²œ íˆ¬ì–´ ê²°ê³¼", "en": "âœ¨ AI Recommended Tours"},
    "tour_reason": {"ko": "ì¶”ì²œ ì´ìœ ", "en": "Why we recommend this"},
    "tour_pros": {"ko": "ì¥ì ", "en": "Pros"},
    "tour_tip": {"ko": "ê¿€íŒ", "en": "Tip"},
    "tour_region_selector": {"ko": "ë– ë‚˜ì‹œëŠ” ì—¬í–‰ì§€ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”! ğŸ‡¹ğŸ‡­", "en": "Select your destination! ğŸ‡¹ğŸ‡­"},
    # Planner Options Mapping
    "who_alone": {"ko": "í˜¼ì", "en": "Alone"},
    "who_couple": {"ko": "ì—°ì¸/ë¶€ë¶€", "en": "Couple"},
    "who_friend": {"ko": "ì¹œêµ¬", "en": "Friends"},
    "who_child": {"ko": "ê°€ì¡±(ì•„ì´ë™ë°˜)", "en": "Family (with children)"},
    "who_parent": {"ko": "ê°€ì¡±(ë¶€ëª¨ë‹˜)", "en": "Family (with parents)"},
    "style_healing": {"ko": "íë§/ë§ˆì‚¬ì§€", "en": "Healing/Massage"},
    "style_photo": {"ko": "ì¸ìƒìƒ·/ì‚¬ì§„", "en": "Photo-centric"},
    "style_history": {"ko": "ì—­ì‚¬/ë¬¸í™”", "en": "History/Culture"},
    "style_activity": {"ko": "ì•¡í‹°ë¹„í‹°/ìŠ¤ë¦´", "en": "Activity/Thrills"},
    "style_food": {"ko": "ë§›ì§‘/ì‹ë„ë½", "en": "Food/Gourmet"},
    "style_night": {"ko": "ì•¼ê²½/ë¡œë§¨í‹±", "en": "Night View/Romantic"},
    "style_unique": {"ko": "ì´ìƒ‰ì²´í—˜", "en": "Unique Experience"},
    "planner_title": {"ko": "ğŸ“ {} ììœ ì—¬í–‰ í”Œë˜ë„ˆ", "en": "ğŸ“ {} DIY Trip Planner"},
    "planner_guide": {"ko": "ìœ„ ëª©ë¡ì—ì„œ ë§ˆìŒì— ë“œëŠ” íˆ¬ì–´ë¥¼ 'ë‹´ê¸°' ë²„íŠ¼ìœ¼ë¡œ ì¶”ê°€í•´ë³´ì„¸ìš”! AIê°€ ì¼ì •ì„ ì§œë“œë¦½ë‹ˆë‹¤. ğŸ¤–", "en": "Add tours you like from the list above using the 'Add' button! AI will create an itinerary for you. ğŸ¤–"},
    "planner_cart": {"ko": "ğŸ›’ ë‚´ ì—¬í–‰ ì½”ìŠ¤", "en": "ğŸ›’ My Trip Route"},
    "budget_low": {"ko": "ê°€ì„±ë¹„(ì €ë ´)", "en": "Economy (Budget)"},
    "budget_mid": {"ko": "ì ë‹¹í•¨", "en": "Moderate"},
    "budget_high": {"ko": "ëŸ­ì…”ë¦¬/í”„ë¦¬ë¯¸ì—„", "en": "Luxury (Premium)"},
    "tour_fail": {"ko": "AI ì¶”ì²œì„ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì•„ë˜ ì „ì²´ ëª©ë¡ì—ì„œ ì§ì ‘ ì„ íƒí•´ì£¼ì„¸ìš”!", "en": "Failed to get AI recommendations. Please select from the list below!"},
    "added_to_cart": {"ko": "âœ… ë‹´ê¸° ì™„ë£Œ", "en": "âœ… Added"},
    "add_to_cart": {"ko": "â• ì¼ì •ì— ë‹´ê¸°", "en": "â• Add to Trip"},
    "all_tours_title": {"ko": "{} íˆ¬ì–´ ì „ì²´ ëª©ë¡ ({}ê°œ)", "en": "All {} Tours ({} items)"},
    "board_title": {"ko": "ğŸ—£ï¸ ì—¬í–‰ì ìˆ˜ë‹¤ë°©", "en": "ğŸ—£ï¸ Traveler's Board"},
    "board_desc": {"ko": "ì—¬í–‰ íŒ, ì§ˆë¬¸, ê±´ì˜ì‚¬í•­ ë“± ììœ ë¡­ê²Œ ì´ì•¼ê¸°ë¥¼ ë‚˜ëˆ ë³´ì„¸ìš”!", "en": "Share tips, ask questions, or suggest features!"},
    "write_btn": {"ko": "ë“±ë¡í•˜ê¸° ğŸ“", "en": "Post ğŸ“"},
    "nickname": {"ko": "ë‹‰ë„¤ì„", "en": "Nickname"},
    "password": {"ko": "ë¹„ë°€ë²ˆí˜¸ (ì‚­ì œìš© ìˆ«ì 4ìë¦¬)", "en": "Password (4 digits for deletion)"},
    "content": {"ko": "ë‚´ìš©", "en": "Content"},
    "write_expander": {"ko": "âœï¸ ê¸€ì“°ê¸° (ì—¬ê¸°ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”)", "en": "âœï¸ Write a post (Click here)"},
    "prev": {"ko": "â¬…ï¸ ì´ì „", "en": "â¬…ï¸ Previous"},
    "next": {"ko": "ë‹¤ìŒ â¡ï¸", "en": "Next â¡ï¸"},
    "other": {"ko": "ê¸°íƒ€ (ì§ì ‘ ì…ë ¥)", "en": "Other (Manual)"},
    "no_events": {"ko": "ğŸ“ ì•„ì§ ë“±ë¡ëœ ì´ë²¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.", "en": "ğŸ“ No events scheduled yet."},
    "event_date": {"ko": "ğŸ“… ì§„í–‰ ê¸°ê°„", "en": "ğŸ“… Duration"},
    "event_place": {"ko": "ğŸ“ ì¥ì†Œ", "en": "ğŸ“ Location"},
    "menu_info": {"ko": "ğŸ½ï¸ ë©”ë‰´ ì •ë³´", "en": "ğŸ½ï¸ Menu Information"},
    "menu_search_btn": {"ko": "ğŸ½ï¸ ë©”ë‰´íŒ ì´ë¯¸ì§€ ê²€ìƒ‰ (Google)", "en": "ğŸ½ï¸ Search Menu Images (Google)"},
    "menu_search_caption": {"ko": "âœ¨ êµ¬ê¸€ ì´ë¯¸ì§€ ê²€ìƒ‰ì„ í†µí•´ ë©”ë‰´íŒ ì‚¬ì§„ë“¤ì„ ëª¨ì•„ë´…ë‹ˆë‹¤.", "en": "âœ¨ Discover menu photos via Google Image search."},
    "clear_results": {"ko": "ğŸ—‘ï¸ ê²°ê³¼ ì§€ìš°ê¸°", "en": "ğŸ—‘ï¸ Clear Results"},
    "recent_history": {"ko": "ğŸ•’ ìµœê·¼ ë³¸ ë§›ì§‘ íˆìŠ¤í† ë¦¬", "en": "ğŸ•’ Recent Restaurant History"},
    "delete_history": {"ko": "ê¸°ë¡ ì‚­ì œ", "en": "Clear History"},
    "delete_post": {"ko": "ì‚­ì œí•˜ê¸°", "en": "Delete"},
    "confirm_pw": {"ko": "ë¹„ë°€ë²ˆí˜¸ í™•ì¸", "en": "Confirm Password"},
    "view_detail_again": {"ko": "ğŸ” ìƒì„¸ ë¶„ì„ ë‹¤ì‹œë³´ê¸°", "en": "ğŸ” View Details Again"},
    "news_cat": {"ko": "ì¹´í…Œê³ ë¦¬", "en": "Category"},
    "all": {"ko": "ì „ì²´", "en": "All"},
    "share_page": {"ko": "ğŸ“‹ ì¹´í†¡ ê³µìœ ìš© í…ìŠ¤íŠ¸ ìƒì„± (í˜„ì¬ í˜ì´ì§€)", "en": "ğŸ“‹ Generate Share Text (Current Page)"},
    "no_news_results": {"ko": "ì¡°ê±´ì— ë§ëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.", "en": "No news matches the criteria."},
    "no_news_update": {"ko": "ğŸ˜´ ì•„ì§ ì—…ë°ì´íŠ¸ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. (ì ì‹œ í›„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”)", "en": "ğŸ˜´ No news updates yet. Please check back later."},
    "view_full_article": {"ko": "ğŸ“„ ê¸°ì‚¬ ì „ë¬¸ ë³´ê¸°", "en": "ğŸ“„ View Full Article"},
    "summary_only": {"ko": "âš ï¸ ì´ ê¸°ì‚¬ëŠ” ìš”ì•½ë³¸ë§Œ ì œê³µë©ë‹ˆë‹¤.", "en": "âš ï¸ This article only provides a summary."},
    "related_share": {"ko": "ğŸ”— ê´€ë ¨ ê¸°ì‚¬ & ê³µìœ ", "en": "ğŸ”— Related Articles & Share"},
    "cat_politics": {"ko": "ğŸ›ï¸ ì •ì¹˜/ì‚¬íšŒ", "en": "ğŸ›ï¸ Politics/Society"},
    "cat_economy": {"ko": "ğŸ’¼ ê²½ì œ", "en": "ğŸ’¼ Economy"},
    "cat_travel": {"ko": "âœˆï¸ ì—¬í–‰/ê´€ê´‘", "en": "âœˆï¸ Travel/Tourism"},
    "cat_culture": {"ko": "ğŸ­ ë¬¸í™”/ì—”í„°", "en": "ğŸ­ Culture/Ent"},
    # Status Dashboard Labels
    "weather_label": {"ko": "ë°©ì½• ë‚ ì”¨", "en": "Bangkok Weather"},
    "air_quality_label": {"ko": "ë¯¸ì„¸ë¨¼ì§€", "en": "Air Quality"},
    "exchange_buy_label": {"ko": "í™˜ìœ¨ (ì‚´ ë•Œ)", "en": "Rate (Buy)"},
    "exchange_sell_label": {"ko": "í™˜ìœ¨ (íŒ” ë•Œ)", "en": "Rate (Sell)"},
    "currency_unit": {"ko": "ì›", "en": " KRW"},
    # AQI Status
    "aqi_good": {"ko": "ì¢‹ìŒ", "en": "Good"},
    "aqi_moderate": {"ko": "ë³´í†µ", "en": "Moderate"},
    "aqi_unhealthy": {"ko": "ë‚˜ì¨", "en": "Unhealthy"},
    "aqi_very_unhealthy": {"ko": "ë§¤ìš°ë‚˜ì¨", "en": "Very Unhealthy"},
    "aqi_loading": {"ko": "ë¡œë”©ì¤‘", "en": "Loading"},
    "aqi_error": {"ko": "ì˜¤ë¥˜", "en": "Error"},
    # Tour Tab
    "tour_title": {"ko": "ğŸ’ AI ì—¬í–‰ ì½”ë””ë„¤ì´í„°", "en": "ğŸ’ AI Travel Coordinator"},
    "tour_desc": {"ko": "ì—¬í–‰ ìŠ¤íƒ€ì¼ì„ ì•Œë ¤ì£¼ì‹œë©´, ì‹¤íŒ¨ ì—†ëŠ” í˜„ì§€ íˆ¬ì–´ë¥¼ ì¶”ì²œí•´ ë“œë ¤ìš”!", "en": "Tell us your travel style, and we'll recommend the best local tours!"},
    "tour_who": {"ko": "ëˆ„êµ¬ì™€ í•¨ê»˜ í•˜ì‹œë‚˜ìš”?", "en": "Who are you traveling with?"},
    "tour_style": {"ko": "ì„ í˜¸í•˜ëŠ” ìŠ¤íƒ€ì¼ì€?", "en": "What's your preferred style?"},
    "tour_budget": {"ko": "ì„ í˜¸í•˜ëŠ” ê°€ê²©ëŒ€ëŠ”?", "en": "Preferred price range?"},
    "tour_find_btn": {"ko": "âœ¨ ë‚´ ì·¨í–¥ì— ë”± ë§ëŠ” íˆ¬ì–´ ì°¾ê¸°", "en": "âœ¨ Find My Perfect Tour"},
    "tour_spinner": {"ko": "AIê°€ ìˆ˜ì²œ ê°œì˜ í›„ê¸°ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤... ğŸ¤–", "en": "AI is analyzing thousands of reviews... ğŸ¤–"},
    "tour_result_title": {"ko": "ğŸ¯ ë‹¹ì‹ ì„ ìœ„í•œ AI ì¶”ì²œ íˆ¬ì–´", "en": "ğŸ¯ AI-Recommended Tours for You"},
    "tour_book_btn": {"ko": "ğŸ‘‰ ìµœì €ê°€ ì˜ˆì•½í•˜ê¸° (Klook)", "en": "ğŸ‘‰ Book at Best Price (Klook)"},
    "tour_all_list": {"ko": "ğŸ“‹ íˆ¬ì–´ ì „ì²´ ëª©ë¡ ë³´ê¸°", "en": "ğŸ“‹ View All Tours"},
    "tour_fallback": {"ko": "ğŸŒ í´ë£©(Klook)ì—ì„œ íƒœêµ­ íˆ¬ì–´ ì „ì²´ë³´ê¸° (2,000ê°œ+)", "en": "ğŸŒ Browse All Thailand Tours on Klook (2,000+)"},
    "tour_no_match": {"ko": "ğŸ¤” ë§ˆìŒì— ë“œëŠ” íˆ¬ì–´ê°€ ì—†ìœ¼ì‹ ê°€ìš”?", "en": "ğŸ¤” Didn't find what you're looking for?"},
    "tour_reason": {"ko": "ğŸ’¡ ì¶”ì²œ ì´ìœ ", "en": "ğŸ’¡ Why We Recommend This"},
    "tour_tip": {"ko": "ğŸ¯ ê¿€íŒ", "en": "ğŸ¯ Pro Tip"},
    "tour_pros": {"ko": "ğŸ‘ í•µì‹¬ í¬ì¸íŠ¸", "en": "ğŸ‘ Key Highlights"},
}

def t(key):
    """
    Returns translated text based on st.session_state['language'].
    Defaults to 'ko' if not found or if session state is missing.
    """
    lang = st.session_state.get('language', 'Korean')
    lang_code = "en" if lang == "English" else "ko"
    
    if key in UI_TEXT:
        return UI_TEXT[key].get(lang_code, UI_TEXT[key].get("ko", key))
    return key

def detect_browser_language():
    """
    Detects the user's browser language from the Accept-Language header.
    Returns 'Korean' if Korean is detected, 'English' otherwise (default for non-Korean users).
    
    Uses st.context.headers which is available in Streamlit >= 1.37.0.
    Falls back to 'English' if headers cannot be read (for Travelpayouts reviewers).
    """
    try:
        # Streamlit >= 1.37.0: use st.context.headers
        headers = st.context.headers
        accept_lang = headers.get("Accept-Language", "")
        
        # Check if Korean is in the Accept-Language header
        if "ko" in accept_lang.lower():
            return "Korean"
        else:
            return "English"  # Default to English for non-Korean users
    except Exception:
        # Fallback: Default to English for international users / reviewers
        return "English"

import streamlit as st
import streamlit.components.v1 as components

# --- Scroll to Top Helper (Anchor ë°©ì‹) ---
def scroll_to_top(key_suffix=None):
    """
    ì•µì»¤ ìš”ì†Œë¡œ í™”ë©´ì„ ìŠ¤í¬ë¡¤í•©ë‹ˆë‹¤.
    scrollIntoView ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì¢Œí‘œ ê³„ì‚° ì—†ì´ í™•ì‹¤í•˜ê²Œ ì´ë™.
    
    ì‚¬ìš© ì „ í˜ì´ì§€ì— ì•„ë˜ ì•µì»¤ë¥¼ ì‹¬ì–´ì•¼ í•¨:
    st.markdown('<div id="news-top-anchor"></div>', unsafe_allow_html=True)
    
    Args:
        key_suffix: HTMLì— í¬í•¨ë  ê³ ìœ ê°’ (ë§¤ë²ˆ ë‹¤ë¥¸ ê°’ í•„ìš”)
    """
    import streamlit.components.v1 as components
    import time
    
    # key_suffixê°€ ì—†ìœ¼ë©´ timestamp ì‚¬ìš©
    if key_suffix is None:
        key_suffix = int(time.time() * 1000)
    
    # ì•½ê°„ì˜ ë”œë ˆì´(150ms)ë¥¼ ì¤˜ì„œ í™”ë©´ì´ ë‹¤ ê·¸ë ¤ì§„ ë’¤ ì í”„í•˜ë„ë¡ í•¨
    js = f"""
    <!-- scroll_anchor_trigger_{key_suffix} -->
    <script>
        setTimeout(function() {{
            const anchor = window.parent.document.getElementById("news-top-anchor");
            if (anchor) {{
                anchor.scrollIntoView({{ behavior: "auto", block: "start" }});
            }}
        }}, 150);
    </script>
    """
    components.html(js, height=0, width=0)

# --- Head íƒœê·¸ ì½”ë“œ ì£¼ì… Helper ---
def inject_head_code(code_string):
    """
    HTML ì½”ë“œë¥¼ ë¶€ëª¨ ìœˆë„ìš°ì˜ <head> íƒœê·¸ì— ì‚½ì…í•©ë‹ˆë‹¤.
    Travelpayouts ë“± ì œ3ì ì„œë¹„ìŠ¤ ì¸ì¦ ì½”ë“œ ì‚½ì…ì— ì‚¬ìš©.
    
    Args:
        code_string: ì‚½ì…í•  HTML ì½”ë“œ (meta íƒœê·¸, script íƒœê·¸ ë“±)
    
    Example:
        inject_head_code('<meta name="tp-verification" content="abc123" />')
    """
    import streamlit.components.v1 as components
    import time
    import html
    
    if not code_string or not code_string.strip():
        return
    
    # JavaScriptì—ì„œ ì•ˆì „í•˜ê²Œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ escape ì²˜ë¦¬
    # ë‹¨, HTML íƒœê·¸ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•´ì•¼ í•˜ë¯€ë¡œ ì¤„ë°”ê¿ˆ/ë”°ì˜´í‘œë§Œ ì²˜ë¦¬
    safe_code = code_string.replace('\\', '\\\\').replace('`', '\\`').replace('${', '\\${')
    
    # ê³ ìœ  keyë¥¼ ìœ„í•œ timestamp
    unique_id = int(time.time() * 1000)
    
    js = f"""
    <!-- head_inject_{unique_id} -->
    <script>
        (function() {{
            // ì´ë¯¸ ì‚½ì…ë˜ì—ˆëŠ”ì§€ ì²´í¬ (ì¤‘ë³µ ë°©ì§€)
            var existingMeta = window.parent.document.head.querySelector('[data-tp-injected]');
            if (existingMeta) return;
            
            // ì½”ë“œë¥¼ headì— ì‚½ì…
            var codeToInject = `{safe_code}`;
            var tempDiv = document.createElement('div');
            tempDiv.innerHTML = codeToInject;
            
            // ê° ìš”ì†Œë¥¼ headì— ì¶”ê°€
            while (tempDiv.firstChild) {{
                var node = tempDiv.firstChild;
                if (node.nodeType === 1) {{ // Element node
                    node.setAttribute('data-tp-injected', 'true');
                }}
                window.parent.document.head.appendChild(node);
            }}
        }})();
    </script>
    """
    components.html(js, height=0, width=0)

# --- SEO: Dynamic Page Title ---
def set_page_title(title):
    """
    Dynamically updates the browser tab title using JavaScript.
    Call this at the start of each tab/page to update the title for SEO.
    
    Args:
        title: The new page title to display in the browser tab
    """
    import streamlit.components.v1 as components
    import time
    
    # Escape special characters for JavaScript
    safe_title = title.replace('\\', '\\\\').replace("'", "\\'").replace('"', '\\"')
    unique_id = int(time.time() * 1000)
    
    js = f"""
    <!-- page_title_{unique_id} -->
    <script>
        window.parent.document.title = "{safe_title}";
    </script>
    """
    components.html(js, height=0, width=0)

# --- SEO: Meta Description Injection ---
def inject_meta_description(description):
    """
    Injects or updates the <meta name="description"> tag for SEO.
    Call this early in app initialization for Google search result previews.
    
    Args:
        description: The meta description content (max ~155 chars recommended)
    """
    import streamlit.components.v1 as components
    import time
    
    safe_desc = description.replace('\\', '\\\\').replace('"', '\\"').replace("'", "\\'")
    unique_id = int(time.time() * 1000)
    
    js = f"""
    <!-- meta_desc_{unique_id} -->
    <script>
        (function() {{
            var existingMeta = window.parent.document.querySelector('meta[name="description"]');
            if (existingMeta) {{
                existingMeta.setAttribute('content', "{safe_desc}");
            }} else {{
                var meta = document.createElement('meta');
                meta.name = 'description';
                meta.content = "{safe_desc}";
                window.parent.document.head.appendChild(meta);
            }}
        }})();
    </script>
    """
    components.html(js, height=0, width=0)

# --- SEO: Tab-specific Titles Dictionary ---
SEO_TITLES = {
    "nav_news": {
        "ko": "ğŸ“° íƒœêµ­ ë‰´ìŠ¤ ë¸Œë¦¬í•‘ | ì˜¤ëŠ˜ì˜ íƒœêµ­",
        "en": "ğŸ“° Thailand News Briefing | Thai Today"
    },
    "nav_hotel": {
        "ko": "ğŸ¨ ë°©ì½• í˜¸í…” íŒ©íŠ¸ì²´í¬ & ë¦¬ë·° | ì˜¤ëŠ˜ì˜ íƒœêµ­",
        "en": "ğŸ¨ Bangkok Hotel Real Reviews | Thai Today"
    },
    "nav_food": {
        "ko": "ğŸœ íƒœêµ­ ë§›ì§‘ íŒ©íŠ¸ì²´í¬ & ë¦¬ë·° | ì˜¤ëŠ˜ì˜ íƒœêµ­",
        "en": "ğŸœ Thailand Food Fact Check & Reviews | Thai Today"
    },
    "nav_guide": {
        "ko": "ğŸ“˜ íƒœêµ­ ì—¬í–‰ ê°€ì´ë“œ 2026 | ì˜¤ëŠ˜ì˜ íƒœêµ­",
        "en": "ğŸ“˜ Thailand Travel Guide 2026 | Thai Today"
    },
    "nav_tour": {
        "ko": "ğŸ’ AI íˆ¬ì–´ ì¶”ì²œ | ì˜¤ëŠ˜ì˜ íƒœêµ­",
        "en": "ğŸ“˜ Thailand Travel Guide 2026 | Thai Today"
    },
    "nav_taxi": {
        "ko": "ğŸš• ë°©ì½• íƒì‹œ ìš”ê¸ˆ ê³„ì‚°ê¸° | ì˜¤ëŠ˜ì˜ íƒœêµ­",
        "en": "ğŸš• Bangkok Taxi Fare Calculator | Thai Today"
    },
    "nav_event": {
        "ko": "ğŸª íƒœêµ­ ì´ë²¤íŠ¸ & ì¶•ì œ | ì˜¤ëŠ˜ì˜ íƒœêµ­",
        "en": "ğŸª Thailand Events & Festivals | Thai Today"
    },
    "nav_board": {
        "ko": "ğŸ—£ï¸ íƒœêµ­ ì—¬í–‰ ì»¤ë®¤ë‹ˆí‹° | ì˜¤ëŠ˜ì˜ íƒœêµ­",
        "en": "ğŸ—£ï¸ Thailand Travel Community | Thai Today"
    }
}

def get_seo_title(nav_key):
    """
    Returns the SEO-optimized page title for a given navigation key.
    
    Args:
        nav_key: The navigation key (e.g., 'nav_news', 'nav_hotel')
    
    Returns:
        str: SEO-optimized page title based on current language
    """
    lang = st.session_state.get('language', 'Korean')
    lang_code = "en" if lang == "English" else "ko"
    
    if nav_key in SEO_TITLES:
        return SEO_TITLES[nav_key].get(lang_code, SEO_TITLES[nav_key].get("ko", "Thai Today"))
    
    # Fallback
    if lang_code == "en":
        return "Thailand Travel Fact Check - Thai Today"
    else:
        return "íƒœêµ­ ì—¬í–‰ íŒ©íŠ¸ì²´í¬ - ì˜¤ëŠ˜ì˜ íƒœêµ­"

# --- URL ì •ë¦¬ Helper (íŒŒë¼ë¯¸í„° ì œê±°) ---
def clean_url_bar():
    """
    URLì—ì„œ init_marker ë“± ì¶”ì  íŒŒë¼ë¯¸í„°ë¥¼ ì‹œê°ì ìœ¼ë¡œ ì œê±°í•©ë‹ˆë‹¤.
    history.replaceStateë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ìƒˆë¡œê³ ì¹¨ ì—†ì´ ì£¼ì†Œì°½ë§Œ ê¹”ë”í•´ì§‘ë‹ˆë‹¤.
    ìˆ˜ìµ ì¶”ì  ê¸°ëŠ¥ì€ ì´ë¯¸ ì‹¤í–‰ëœ í›„ì´ë¯€ë¡œ ì˜í–¥ ì—†ìŒ.
    """
    import streamlit.components.v1 as components
    import time
    
    unique_id = int(time.time() * 1000)
    
    js = f"""
    <!-- clean_url_{unique_id} -->
    <script>
        // URLì— 'init_marker'ê°€ ë³´ì´ë©´ ì‹¤í–‰
        if (window.parent.location.search.indexOf('init_marker') > -1) {{
            // íŒŒë¼ë¯¸í„°ë¥¼ ë—€ ê¹¨ë—í•œ ì£¼ì†Œ ìƒì„±
            var clean_uri = window.parent.location.protocol + "//" + window.parent.location.host + window.parent.location.pathname;
            // ì£¼ì†Œì°½ ë°”ê¿”ì¹˜ê¸° (ìƒˆë¡œê³ ì¹¨ ì•ˆ ë¨)
            window.parent.history.replaceState({{}}, document.title, clean_uri);
        }}
    </script>
    """
    components.html(js, height=0, width=0)

# --- ì•„ê³ ë‹¤ ì œíœ´ ë§í¬ ìƒì„± ---
def generate_agoda_link(hotel_name: str) -> str:
    """
    ì•„ê³ ë‹¤ íŒŒíŠ¸ë„ˆ ê²€ìƒ‰ URLì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        hotel_name: í˜¸í…” ì´ë¦„
    
    Returns:
        ì•„ê³ ë‹¤ ê²€ìƒ‰ URL (ì œíœ´ ë§ˆì»¤ í¬í•¨)
    """
    import urllib.parse
    
    AGODA_MARKER_ID = "700591"  # Travelpayouts ë§ˆì»¤ ID
    encoded_name = urllib.parse.quote(hotel_name)
    
    return f"https://www.agoda.com/search?cid={AGODA_MARKER_ID}&checkIn=&checkOut=&rooms=1&adults=2&children=0&childages=&searchrequestid=&priceCur=KRW&textToSearch={encoded_name}&travellerType=1&pageTypeId=1"

# ============================================
# ğŸ“° Thai English News RSS Sources
# ============================================
THAI_ENGLISH_RSS = [
    "https://www.bangkokpost.com/rss/data/topstories.xml",  # Bangkok Post
    "https://thethaiger.com/feed",  # The Thaiger (popular with travelers)
    "https://www.khaosodenglish.com/feed/",  # Khaosod English
    "https://www.nationthailand.com/rss/306",  # Nation Thailand
]

# Fallback images for news without thumbnails (Thailand themed)
FALLBACK_NEWS_IMAGES = [
    "https://images.unsplash.com/photo-1508009603885-50cf7c579365?w=400",  # Bangkok Temple
    "https://images.unsplash.com/photo-1552465011-b4e21bf6e79a?w=400",  # Thai Street
    "https://images.unsplash.com/photo-1528181304800-259b08848526?w=400",  # Bangkok Skyline
    "https://images.unsplash.com/photo-1506665531195-3566af2b4dfa?w=400",  # Thai Beach
    "https://images.unsplash.com/photo-1534766555764-ce878a5e3a2b?w=400",  # Thai Food
]

import streamlit as st

@st.cache_data(ttl=1800)  # Cache for 30 minutes
def fetch_combined_english_news(max_articles=15):
    """
    Fetches and combines English news from Thai RSS feeds.
    Returns a list of article dictionaries sorted by date (newest first).
    
    Returns:
        list: List of dicts with keys: title, summary, link, image_url, source, published_date
    """
    import random
    from datetime import datetime
    import time as time_module
    
    all_articles = []
    
    for rss_url in THAI_ENGLISH_RSS:
        try:
            feed = feedparser.parse(rss_url)
            source_name = feed.feed.get('title', 'Thai News')[:30]
            
            for entry in feed.entries[:10]:  # Max 10 per source
                # Extract title
                title = entry.get('title', 'Untitled')
                
                # Extract summary/description
                summary = entry.get('summary', entry.get('description', ''))
                # Clean HTML from summary
                if summary:
                    summary = BeautifulSoup(summary, 'html.parser').get_text()[:300]
                
                # Extract link
                link = entry.get('link', '')
                
                # Extract image (check multiple possible locations)
                image_url = None
                
                # 1. Check media_content
                if hasattr(entry, 'media_content') and entry.media_content:
                    for media in entry.media_content:
                        if media.get('url'):
                            image_url = media['url']
                            break
                
                # 2. Check media_thumbnail
                if not image_url and hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
                    for thumb in entry.media_thumbnail:
                        if thumb.get('url'):
                            image_url = thumb['url']
                            break
                
                # 3. Check enclosures
                if not image_url and hasattr(entry, 'enclosures') and entry.enclosures:
                    for enc in entry.enclosures:
                        if enc.get('type', '').startswith('image'):
                            image_url = enc.get('url')
                            break
                
                # 4. Check content for img tags
                if not image_url:
                    content = entry.get('content', [{}])
                    if content:
                        content_value = content[0].get('value', '') if isinstance(content, list) else str(content)
                        soup = BeautifulSoup(content_value, 'html.parser')
                        img = soup.find('img')
                        if img and img.get('src'):
                            image_url = img['src']
                
                # 5. Fallback to random Thailand image
                if not image_url:
                    image_url = random.choice(FALLBACK_NEWS_IMAGES)
                
                # Extract publish date
                published_date = None
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    try:
                        published_date = datetime(*entry.published_parsed[:6])
                    except:
                        pass
                if not published_date and hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                    try:
                        published_date = datetime(*entry.updated_parsed[:6])
                    except:
                        pass
                if not published_date:
                    published_date = datetime.now()
                
                all_articles.append({
                    'title': title,
                    'summary': summary,
                    'link': link,
                    'image_url': image_url,
                    'source': source_name,
                    'published_date': published_date,
                    'category': 'TRAVEL'  # Default category for travel app
                })
                
        except Exception as e:
            print(f"Error fetching RSS from {rss_url}: {e}")
            continue
    
    # Sort by date (newest first)
    all_articles.sort(key=lambda x: x['published_date'], reverse=True)
    
    # Return top N articles
    return all_articles[:max_articles]


# ============================================
# ğŸ“‹ Standard Category System
# ============================================
CATEGORY_MAPPING = {
    "POLITICS": ["Politics", "Society", "Crime", "Government", "ì •ì¹˜", "ì‚¬íšŒ", "ì •ì¹˜/ì‚¬íšŒ", "ì‚¬ê±´/ì‚¬ê³ ", "ë²•ë¥ ", "General", "ê¸°íƒ€"],
    "BUSINESS": ["Economy", "Business", "Finance", "Stock", "ê²½ì œ", "ê¸ˆìœµ", "ë¶€ë™ì‚°", "ê¸ˆìœµ/ê²½ì œ"],
    "TRAVEL": ["Travel", "Tourism", "Food", "Weather", "ì—¬í–‰", "ê´€ê´‘", "ì—¬í–‰/ê´€ê´‘", "ì¶•ì œ", "êµí†µ", "ë‚ ì”¨", "ë§›ì§‘", "ì¶•ì œ/ì´ë²¤íŠ¸"],
    "LIFESTYLE": ["Entertainment", "Culture", "K-Pop", "Life", "ë¬¸í™”", "ì—”í„°í…Œì¸ë¨¼íŠ¸", "ì—°ì˜ˆ"]
}

DISPLAY_CATEGORIES = ["ì „ì²´", "POLITICS", "BUSINESS", "TRAVEL", "LIFESTYLE"]
DISPLAY_LABELS = {
    "POLITICS": "ğŸ›ï¸ ì •ì¹˜/ì‚¬íšŒ",
    "BUSINESS": "ğŸ’¼ ê²½ì œ",
    "TRAVEL": "âœˆï¸ ì—¬í–‰/ê´€ê´‘",
    "LIFESTYLE": "ğŸ­ ë¬¸í™”/ì—”í„°"
}

def normalize_category(raw_category: str) -> str:
    """
    Normalizes any category string to one of the 4 standard categories.
    Weather/Traffic news â†’ TRAVEL (priority for traveler safety)
    Unknown â†’ POLITICS (fallback)
    """
    if not raw_category:
        return "POLITICS"
    
    raw_lower = raw_category.lower()
    
    # Priority: Weather/Traffic/Flood â†’ TRAVEL (traveler safety)
    travel_keywords = ["ë‚ ì”¨", "weather", "êµí†µ", "traffic", "í™ìˆ˜", "flood", "ê³µí•­", "airport", "ë¹„ì", "visa"]
    if any(kw in raw_lower for kw in travel_keywords):
        return "TRAVEL"
    
    # Match against known aliases
    for standard_cat, aliases in CATEGORY_MAPPING.items():
        if raw_category in aliases or raw_lower in [a.lower() for a in aliases]:
            return standard_cat
    
    return "POLITICS"  # Fallback for unknown categories

# --- Hotel Share Summary Generator (No API Call) ---
def extract_hotel_share_summary(hotel_name: str, analysis: dict) -> str:
    """
    ì´ë¯¸ ë¶„ì„ëœ ê²°ê³¼(analysis dict)ì—ì„œ ê³µìœ ìš© ìš”ì•½ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    Gemini API í˜¸ì¶œ ì—†ì´ ìˆœìˆ˜ Python íŒŒì‹±ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    
    Args:
        hotel_name: í˜¸í…” ì´ë¦„
        analysis: íŒ©íŠ¸ì²´í¬ ë¶„ì„ ê²°ê³¼ dict (summary_score, pros, cons ë“± í¬í•¨)
    
    Returns:
        ê³µìœ ìš© ìš”ì•½ í…ìŠ¤íŠ¸ (ì¹´ì¹´ì˜¤í†¡/SNS ì „ì†¡ì— ì í•©í•œ í˜•ì‹)
    """
    # 1. ì ìˆ˜ ì¶”ì¶œ
    scores = analysis.get('summary_score', {})
    cleanliness = scores.get('cleanliness', 0)
    location = scores.get('location', 0)
    comfort = scores.get('comfort', 0)
    value = scores.get('value', 0)
    score_text = f"{cleanliness}/{location}/{comfort}/{value}"
    
    # 2. ì¥ì  ì¶”ì¶œ (ì²« ë²ˆì§¸ í•­ëª©)
    pros_list = analysis.get('pros', [])
    pros_text = pros_list[0] if pros_list else "ë‚´ìš© í™•ì¸ í•„ìš”"
    # ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
    if len(pros_text) > 50:
        pros_text = pros_text[:47] + "..."
    
    # 3. ë‹¨ì /ì£¼ì˜ì‚¬í•­ ì¶”ì¶œ (ì²« ë²ˆì§¸ í•­ëª©)
    cons_list = analysis.get('cons', [])
    cons_text = cons_list[0] if cons_list else "ë‚´ìš© í™•ì¸ í•„ìš”"
    # ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
    if len(cons_text) > 50:
        cons_text = cons_text[:47] + "..."
    
    # 4. í•œì¤„í‰ ì¶”ì¶œ
    one_line = analysis.get('one_line_verdict', '')
    if one_line and len(one_line) > 60:
        one_line = one_line[:57] + "..."
    
    # 5. ê³µìœ  í…ìŠ¤íŠ¸ ì¡°ë¦½
    share_text = f"""ğŸ¨ [í˜¸í…” íŒ©íŠ¸ì²´í¬] {hotel_name}
ğŸ›¡ï¸ íŒ©íŠ¸ì ìˆ˜: {score_text} (ì²­ê²°/ìœ„ì¹˜/í¸ì•ˆ/ê°€ì„±ë¹„)
âœ… ì¥ì : {pros_text}
âš ï¸ ì£¼ì˜: {cons_text}
ğŸ’¡ í•œì¤„í‰: "{one_line}"
ğŸ”— í™•ì¸í•˜ê¸°: thai-today.com"""
    
    return share_text

# --- Hotel Caching (Google Sheets) ---
def get_hotel_gsheets_client():
    """Authenticates gspread using secrets (GOOGLE_SHEETS_KEY or connections.gsheets_news)."""
    try:
        # 1. Try direct JSON string/dict from Railway/st.secrets
        creds_info = st.secrets.get("GOOGLE_SHEETS_KEY")
        
        # 2. Try nested connection config if direct key is missing
        if not creds_info:
            if "connections" in st.secrets and "gsheets_news" in st.secrets["connections"]:
                creds_info = st.secrets["connections"]["gsheets_news"]
            elif "gsheets_news" in st.secrets:
                creds_info = st.secrets["gsheets_news"]
            
        if not creds_info:
            print("GSheets Secret Missing: Please check GOOGLE_SHEETS_KEY or [connections.gsheets_news]")
            return None
             
        if isinstance(creds_info, str):
            # Parse if it's a stringified JSON
            try:
                creds_dict = json.loads(creds_info)
            except:
                # If it's just a file path (unlikely in Streamlit Cloud but possible)
                if os.path.exists(creds_info):
                    with open(creds_info, 'r') as f:
                        creds_dict = json.load(f)
                else: raise
        else:
            # If it's a dict or AttrDict from st.secrets
            creds_dict = dict(creds_info)
            
        # 3. Clean up dict for gspread (remove extra keys like 'spreadsheet' or 'worksheet')
        valid_keys = [
            "type", "project_id", "private_key_id", "private_key",
            "client_email", "client_id", "auth_uri", "token_uri",
            "auth_provider_x509_cert_url", "client_x509_cert_url", "universe_domain"
        ]
        gspread_creds = {k: v for k, v in creds_dict.items() if k in valid_keys}
            
        scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
        creds = ServiceAccountCredentials.from_json_keyfile_dict(gspread_creds, scope)
        client = gspread.authorize(creds)
        return client
    except Exception as e:
        print(f"GSheets Auth Error: {e}")
        return None

def get_hotel_cache(hotel_name, language="Korean"):
    """Checks if analysis for the given hotel already exists in GSheets (Language aware)."""
    client = get_hotel_gsheets_client()
    if not client: return None
    try:
        sh = client.open("hotel_cache_db")
        sheet = sh.get_worksheet(0)
        
        from gspread.utils import escape_for_json
        # Search for hotel_name
        cells = sheet.find(hotel_name, in_column=1)
        if cells:
             # There might be multiple entries for different languages
             all_records = sheet.get_all_values()
             for row in all_records:
                 if row[0] == hotel_name:
                     # Row: [name, date, summary, json, agoda, lang]
                     cached_lang = row[5] if len(row) >= 6 else "Korean"
                     if cached_lang == language:
                        return {
                            "hotel_name": row[0],
                            "cached_date": row[1],
                            "ai_summary": row[2],
                            "raw_json": json.loads(row[3]),
                            "agoda_url": row[4] if len(row) > 4 else None,
                            "language": cached_lang
                        }
    except Exception as e:
        print(f"Cache Lookup Error: {e}")
    return None

def save_hotel_cache(hotel_name, ai_summary, raw_json_dict, agoda_url=None, language="Korean"):
    """Appends new analysis results to the hotel_cache_db GSheet."""
    client = get_hotel_gsheets_client()
    if not client: return
    try:
        sh = client.open("hotel_cache_db")
        sheet = sh.get_worksheet(0)
        
        # Header: [hotel_name, cached_date, ai_summary, raw_json, agoda_url, language]
        from datetime import datetime
        now_str = datetime.now().strftime("%Y-%m-%d %H:%M")
        
        new_row = [
            hotel_name,
            now_str,
            ai_summary,
            json.dumps(raw_json_dict, ensure_ascii=False),
            agoda_url or "",
            language
        ]
        sheet.append_row(new_row)
        print(f"âœ… Cached ({language}) analysis for: {hotel_name}")
    except Exception as e:
        print(f"Cache Save Error: {e}")


def update_hotel_agoda_url(hotel_name, agoda_url):
    """
    íŠ¹ì • í˜¸í…”ì˜ ì•„ê³ ë‹¤ ì§í†µ URLì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    ê´€ë¦¬ìê°€ ì§í†µ ë§í¬ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì…ë ¥í•  ë•Œ ì‚¬ìš©.
    """
    client = get_hotel_gsheets_client()
    if not client: return False
    try:
        sh = client.open("hotel_cache_db")
        sheet = sh.get_worksheet(0)
        
        cell = sheet.find(hotel_name)
        if cell:
            # 5ë²ˆì§¸ ì»¬ëŸ¼(Eì—´)ì— URL ì—…ë°ì´íŠ¸
            sheet.update_cell(cell.row, 5, agoda_url)
            print(f"âœ… Updated Agoda URL for: {hotel_name}")
            return True
        else:
            print(f"âŒ Hotel not found: {hotel_name}")
            return False
    except Exception as e:
        print(f"Update Error: {e}")
        return False


def get_hotel_link(hotel_name, cached_agoda_url=None):
    """
    í•˜ì´ë¸Œë¦¬ë“œ í˜¸í…” ë§í¬ ìƒì„±.
    1. cached_agoda_urlì´ ìˆê³  ìœ íš¨í•˜ë©´ â†’ ì§í†µ ë§í¬ì— CID ì¶”ê°€/êµì²´ í›„ ë¦¬í„´
    2. ì—†ìœ¼ë©´ â†’ ê²€ìƒ‰ ë§í¬ ìƒì„±
    
    Args:
        hotel_name: í˜¸í…” ì´ë¦„
        cached_agoda_url: ìºì‹œëœ ì§í†µ ì•„ê³ ë‹¤ URL (ì„ íƒ)
    
    Returns:
        tuple: (url, is_direct) - URLê³¼ ì§í†µ ì—¬ë¶€
    """
    import urllib.parse
    import re
    
    AGODA_MARKER_ID = "700591"
    
    # 1. ì§í†µ ë§í¬ê°€ ìˆìœ¼ë©´ ì‚¬ìš© (URL ì •í™” + CIDë§Œ ì¶”ê°€)
    if cached_agoda_url and cached_agoda_url.strip() and cached_agoda_url.startswith('http'):
        url = cached_agoda_url.strip()
        
        # URL íŒŒì‹±
        parsed = urllib.parse.urlparse(url)
        
        # ëª¨ë“  ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì œê±°í•˜ê³  Base URLë§Œ ì¶”ì¶œ
        # ë‚´ CIDë§Œ ê¹”ë”í•˜ê²Œ ì¶”ê°€
        clean_url = urllib.parse.urlunparse((
            parsed.scheme,
            parsed.netloc,
            parsed.path,
            '',  # params ì œê±°
            f'cid={AGODA_MARKER_ID}',  # ë‚´ CIDë§Œ ì¶”ê°€
            ''   # fragment ì œê±°
        ))
        
        return (clean_url, True)
    
    # 2. ì—†ìœ¼ë©´ ê²€ìƒ‰ ë§í¬ ìƒì„±
    encoded_name = urllib.parse.quote(hotel_name)
    search_url = f"https://www.agoda.com/search?cid={AGODA_MARKER_ID}&checkIn=&checkOut=&rooms=1&adults=2&children=0&priceCur=KRW&textToSearch={encoded_name}&travellerType=1&pageTypeId=1"
    
    return (search_url, False)


# --- ì‹¤ì‹œê°„ ê²€ìƒ‰ ë­í‚¹ (Real-time Search Ranking) ---

SEARCH_LOG_FILE = "data/search_log.csv"

def log_search(name, rating, category):
    """
    ì‚¬ìš©ìì˜ ê²€ìƒ‰ ë‚´ì—­ì„ Google Sheets 'search_log' ì‹œíŠ¸ì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    try:
        client = get_hotel_gsheets_client()
        if not client:
            return

        sh = client.open("hotel_cache_db")
        
        # 'search_log' ì›Œí¬ì‹œíŠ¸ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±
        try:
            sheet = sh.worksheet("search_log")
        except:
            # ì‹œíŠ¸ê°€ ì—†ìœ¼ë©´ ìƒì„± (í—¤ë” í¬í•¨)
            sheet = sh.add_worksheet(title="search_log", rows="100", cols="4")
            sheet.append_row(['name', 'rating', 'category', 'timestamp'])
        
        # ë°ì´í„° ì¶”ê°€
        now_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        sheet.append_row([name, rating, category, now_str])
        
        print(f"âœ… Logged search to GSheets: {name} ({category})")
    except Exception as e:
        print(f"âŒ GSheets Logging Error: {e}")

@st.cache_data(ttl=600)  # 10ë¶„ê°„ ë­í‚¹ ìºì‹œ
def get_top_places(category, limit=10):
    """
    Google Sheetsì—ì„œ ê²€ìƒ‰ ë‚´ì—­ì„ ì½ì–´ì™€ ìŠ¤ë§ˆíŠ¸ ë­í‚¹ TOP 10ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        client = get_hotel_gsheets_client()
        if not client:
            return []

        sh = client.open("hotel_cache_db")
        try:
            sheet = sh.worksheet("search_log")
        except:
            return []

        # ëª¨ë“  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        records = sheet.get_all_records()
        if not records:
            return []
            
        df = pd.DataFrame(records)
        
        # ì¹´í…Œê³ ë¦¬ í•„í„°ë§
        df = df[df['category'] == category]
        if df.empty:
            return []
            
        # 1. ì¥ì†Œë³„ ì§‘ê³„ (í‰ê·  í‰ì , ê²€ìƒ‰ íšŸìˆ˜)
        stats = df.groupby('name').agg({
            'rating': 'mean',
            'name': 'count'
        }).rename(columns={'name': 'search_count'}).reset_index()
        
        # 2. í•„í„°ë§: í‰ì  3.5 ë¯¸ë§Œ ì œì™¸
        stats = stats[stats['rating'] >= 3.5]
        
        if stats.empty:
            return []
            
        # 3. ìŠ¤ì½”ì–´ ê³„ì‚° (ê³µì‹: í‰ì  * 10 + log(ê²€ìƒ‰íšŸìˆ˜ + 1))
        stats['score'] = stats['rating'] * 10 + np.log1p(stats['search_count'])
        
        # 4. ì •ë ¬ ë° ìƒìœ„ Nê°œ ì¶”ì¶œ
        top_df = stats.sort_values(by='score', ascending=False).head(limit)
        
        results = []
        for i, (_, row) in enumerate(top_df.iterrows()):
            name = row['name']
            badge = ""
            if i == 0:
                badge = "ğŸ”¥ ë¯¿ê³  ê°€ëŠ” ë­í‚¹ 1ìœ„"
            elif row['rating'] >= 4.8:
                badge = "ğŸ’ ìˆ¨ì€ ë³´ì„ (í‰ì  4.8+)"
            elif row['search_count'] >= 5:
                badge = "ğŸ‘€ ì§€ê¸ˆ ê°€ì¥ í•«í•¨"
            
            results.append({
                'rank': i + 1,
                'name': name,
                'rating': round(row['rating'], 1),
                'count': int(row['search_count']),
                'badge': badge
            })
            
        return results
    except Exception as e:
        print(f"âŒ GSheets Ranking Analysis Error: {e}")
        return []

# ============================================
# ğŸ“˜ Blog / Travel Guide Functions
# ============================================

def fetch_blog_posts():
    """
    ë¸”ë¡œê·¸ ê²Œì‹œê¸€ ëª©ë¡ì„ Google Sheetsì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.
    ìµœì‹  ê¸€ì´ ìœ„ë¡œ ì˜¤ë„ë¡ ì •ë ¬í•©ë‹ˆë‹¤.
    
    Returns:
        list: ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
    """
    client = get_hotel_gsheets_client()
    if not client:
        return []
    
    try:
        sh = client.open("blog_posts")
        sheet = sh.get_worksheet(0)
        
        # ëª¨ë“  ë ˆì½”ë“œ ê°€ì ¸ì˜¤ê¸°
        records = sheet.get_all_records()
        
        # ë‚ ì§œ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ (ìµœì‹  ê¸€ì´ ìœ„ë¡œ)
        records.sort(key=lambda x: x.get('date', ''), reverse=True)
        
        return records
    except Exception as e:
        print(f"Blog Fetch Error: {e}")
        return []


def get_blog_post(post_id):
    """
    íŠ¹ì • IDì˜ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    
    Args:
        post_id: ê²Œì‹œê¸€ ID
    
    Returns:
        dict or None: ê²Œì‹œê¸€ ë°ì´í„°
    """
    client = get_hotel_gsheets_client()
    if not client:
        return None
    
    try:
        sh = client.open("blog_posts")
        sheet = sh.get_worksheet(0)
        
        # IDë¡œ ê²€ìƒ‰
        cell = sheet.find(str(post_id))
        if cell:
            row_data = sheet.row_values(cell.row)
            headers = sheet.row_values(1)
            
            # ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            post = {}
            for i, header in enumerate(headers):
                post[header] = row_data[i] if i < len(row_data) else ""
            return post
    except Exception as e:
        print(f"Blog Get Error: {e}")
    return None


def save_blog_post(post_data):
    """
    ë¸”ë¡œê·¸ ê¸€ì„ ì €ì¥í•©ë‹ˆë‹¤ (Upsert: ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸, ì—†ìœ¼ë©´ ìƒì„±).
    
    Args:
        post_data: dict with keys: id, date, title, summary, content, image_url, author
    
    Returns:
        bool: ì„±ê³µ ì—¬ë¶€
    """
    client = get_hotel_gsheets_client()
    if not client:
        print("Blog Save Error: No GSheets client")
        return False
    
    try:
        # ì‹œíŠ¸ ì—´ê¸° ë˜ëŠ” ìƒì„±
        try:
            sh = client.open("blog_posts")
        except:
            # ì‹œíŠ¸ê°€ ì—†ìœ¼ë©´ ìƒì„±
            print("Creating new blog_posts spreadsheet...")
            sh = client.create("blog_posts")
            # ì„œë¹„ìŠ¤ ê³„ì •ê³¼ ê³µìœ  (ë³¸ì¸ ì´ë©”ì¼ ì¶”ê°€ í•„ìš”ì‹œ ì—¬ê¸°ì—)
            sh.share('', perm_type='anyone', role='reader')  # ì½ê¸° ê¶Œí•œ ê³µê°œ
        
        sheet = sh.get_worksheet(0)
        
        # í—¤ë”ê°€ ì—†ìœ¼ë©´ ì¶”ê°€
        first_row = sheet.row_values(1)
        if not first_row or first_row[0] != 'id':
            headers = ['id', 'date', 'title', 'summary', 'content', 'image_url', 'author']
            sheet.insert_row(headers, 1)
            print("Added header row to blog_posts")
        
        post_id = str(post_data.get('id', ''))
        
        # IDë¡œ ê¸°ì¡´ í–‰ ê²€ìƒ‰
        existing_cell = None
        try:
            existing_cell = sheet.find(post_id)
        except:
            pass
        
        # í–‰ ë°ì´í„° ì¤€ë¹„ (ì»¬ëŸ¼ ìˆœì„œ: id, date, title, summary, content, image_url, author)
        row = [
            post_data.get('id', ''),
            post_data.get('date', ''),
            post_data.get('title', ''),
            post_data.get('summary', ''),
            post_data.get('content', ''),
            post_data.get('image_url', ''),
            post_data.get('author', 'ê´€ë¦¬ì')
        ]
        
        if existing_cell:
            # ì—…ë°ì´íŠ¸
            for i, value in enumerate(row):
                sheet.update_cell(existing_cell.row, i + 1, value)
            print(f"âœ… Blog post updated: {post_id}")
        else:
            # ìƒˆë¡œ ì¶”ê°€
            sheet.append_row(row)
            print(f"âœ… Blog post created: {post_id}")
        
        return True
    except Exception as e:
        print(f"Blog Save Error: {e}")
        return False


def delete_blog_post(post_id):
    """
    ë¸”ë¡œê·¸ ê¸€ì„ ì‚­ì œí•©ë‹ˆë‹¤.
    
    Args:
        post_id: ì‚­ì œí•  ê²Œì‹œê¸€ ID
    
    Returns:
        bool: ì„±ê³µ ì—¬ë¶€
    """
    client = get_hotel_gsheets_client()
    if not client:
        return False
    
    try:
        sh = client.open("blog_posts")
        sheet = sh.get_worksheet(0)
        
        cell = sheet.find(str(post_id))
        if cell:
            sheet.delete_rows(cell.row)
            print(f"âœ… Blog post deleted: {post_id}")
            return True
        else:
            print(f"âŒ Blog post not found: {post_id}")
            return False
    except Exception as e:
        print(f"Blog Delete Error: {e}")
        return False

# ============================================
# ğŸœ Restaurant Caching System (Google Sheets)
# ============================================

def get_cached_restaurants_sheet():
    """
    cached_restaurants ì‹œíŠ¸ë¥¼ ê°€ì ¸ì˜¤ê±°ë‚˜ ìƒì„±í•©ë‹ˆë‹¤.
    """
    client = get_hotel_gsheets_client()
    if not client:
        return None
    
    try:
        try:
            sh = client.open("cached_restaurants")
        except:
            # ì‹œíŠ¸ ìƒì„±
            print("Creating cached_restaurants spreadsheet...")
            sh = client.create("cached_restaurants")
            sh.share('', perm_type='anyone', role='reader')
        
        sheet = sh.get_worksheet(0)
        
        expected_headers = ['location_id', 'name', 'rating', 'num_reviews', 'food_rating', 
                           'atmosphere_rating', 'location_rating', 'price_level', 'price',
                           'cuisines', 'hours', 'address', 'phone', 'web_url', 'photos', 'ranking', 'maps_url',
                           'editorial_summary', 'recommended_menu', 'analysis', 'weekday_text', 'language']
        
        first_row = sheet.row_values(1)
        if not first_row:
            sheet.insert_row(expected_headers, 1)
        elif first_row != expected_headers:
            # ê¸°ì¡´ í—¤ë”ì™€ ë‹¤ë¥´ë©´ (ìƒˆ ì»¬ëŸ¼ ì¶”ê°€ ë“±) ë¶€ì¡±í•œ ë¶€ë¶„ ì—…ë°ì´íŠ¸
            for i, header in enumerate(expected_headers):
                if i >= len(first_row) or first_row[i] != header:
                    sheet.update_cell(1, i + 1, header)
            print(f"âœ… Google Sheets headers synchronized: {len(expected_headers)} columns")
        
        return sheet
    except Exception as e:
        print(f"Cache Sheet Error: {e}")
        return None


def search_cached_restaurants(keyword):
    """
    ìºì‹œëœ ì‹ë‹¹ ì¤‘ì—ì„œ ê²€ìƒ‰ì–´ì™€ ì¼ì¹˜í•˜ëŠ” ì‹ë‹¹ì„ ì°¾ìŠµë‹ˆë‹¤.
    
    Args:
        keyword: ê²€ìƒ‰ì–´
    
    Returns:
        list: ìºì‹œëœ ì‹ë‹¹ ë¦¬ìŠ¤íŠ¸
    """
    sheet = get_cached_restaurants_sheet()
    if not sheet:
        return []
    
    try:
        all_data = sheet.get_all_records()
        keyword_lower = keyword.lower()
        
        cached_results = []
        for row in all_data:
            name = str(row.get('name', '')).lower()
            if keyword_lower in name or name in keyword_lower:
                cached_results.append({
                    'location_id': str(row.get('location_id', '')),
                    'name': row.get('name', ''),
                    'address': row.get('address', 'ì£¼ì†Œ ì •ë³´ ì—†ìŒ'),
                    'is_cached': True  # ìºì‹œ í‘œì‹œ
                })
        
        return cached_results
    except Exception as e:
        print(f"Search Cache Error: {e}")
        return []


def get_cached_restaurant_details(location_id, language="Korean"):
    """
    ìºì‹œì—ì„œ ì‹ë‹¹ ìƒì„¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. (ì–¸ì–´ ì¸ì‹)
    """
    sheet = get_cached_restaurants_sheet()
    if not sheet:
        return None
    
    try:
        # location_idë¡œ ê²€ìƒ‰ (ë™ì¼ IDê°€ ì—¬ëŸ¬ ì–¸ì–´ë¡œ ìˆì„ ìˆ˜ ìˆìŒ)
        all_records = sheet.get_all_records()
        for data in all_records:
            if str(data.get('location_id')) == str(location_id):
                # ì–¸ì–´ê°€ ëª…ì‹œë˜ì–´ ìˆê³  í˜„ì¬ ìš”ì²­ ì–¸ì–´ì™€ ê°™ìœ¼ë©´ ë°˜í™˜
                # (êµ¬ë²„ì „ ìºì‹œëŠ” languageê°€ ë¹„ì–´ìˆìœ¼ë¯€ë¡œ Koreanìœ¼ë¡œ ê°„ì£¼)
                cached_lang = data.get('language') or "Korean"
                if cached_lang == language:
                    # Parse logic...
                    import json
                    photos = []
                    if data.get('photos'):
                        try:
                            photos = json.loads(data['photos'])
                        except:
                            photos = data['photos'].split(',') if data['photos'] else []
                    
                    cuisines = []
                    if data.get('cuisines'):
                        try:
                            cuisines = json.loads(data['cuisines'])
                        except:
                            cuisines = data['cuisines'].split(',') if data['cuisines'] else []
                    
                    recommended_menu = []
                    if data.get('recommended_menu'):
                        try:
                            recommended_menu = json.loads(data['recommended_menu'])
                        except:
                            recommended_menu = []
                    
                    analysis = {}
                    if data.get('analysis'):
                        try:
                            analysis = json.loads(data['analysis'])
                        except:
                            analysis = {}

                    weekday_text = []
                    if data.get('weekday_text'):
                        try:
                            weekday_text = json.loads(data['weekday_text'])
                        except:
                            weekday_text = []

                    return {
                        'name': data.get('name', ''),
                        'rating': float(data.get('rating', 0) or 0),
                        'num_reviews': int(data.get('num_reviews', 0) or 0),
                        'food_rating': float(data.get('food_rating', 0) or 0),
                        'atmosphere_rating': float(data.get('atmosphere_rating', 0) or 0),
                        'location_rating': float(data.get('location_rating', 0) or 0),
                        'price_level': data.get('price_level', ''),
                        'price': data.get('price', ''),
                        'cuisines': cuisines,
                        'hours': data.get('hours', ''),
                        'weekday_text': weekday_text,
                        'address': data.get('address', ''),
                        'phone': data.get('phone', ''),
                        'web_url': data.get('web_url', ''),
                        'maps_url': data.get('maps_url', data.get('web_url', '')),
                        'photos': photos,
                        'ranking': data.get('ranking', ''),
                        'editorial_summary': data.get('editorial_summary', ''),
                        'recommended_menu': recommended_menu,
                        'analysis': analysis,
                        'language': cached_lang,
                        'is_cached': True
                    }
        return None
    except Exception as e:
        print(f"Get Cached Details Error: {e}")
        return None


def save_restaurant_to_cache(location_id, details):
    """
    ì‹ë‹¹ ì •ë³´ë¥¼ ìºì‹œì— ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        location_id: Google Places ìœ„ì¹˜ ID
        details: ì‹ë‹¹ ìƒì„¸ ì •ë³´
    """
    sheet = get_cached_restaurants_sheet()
    if not sheet:
        return False
    
    try:
        import json
        
        # ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        existing = None
        try:
            existing = sheet.find(str(location_id))
        except:
            pass
        
        # í–‰ ë°ì´í„° ì¤€ë¹„
        row = [
            str(location_id),
            details.get('name', ''),
            str(details.get('rating', 0)),
            str(details.get('num_reviews', 0)),
            str(details.get('food_rating', 0)),
            str(details.get('atmosphere_rating', 0)),
            str(details.get('location_rating', 0)),
            details.get('price_level', ''),
            details.get('price', ''),
            json.dumps(details.get('cuisines', []), ensure_ascii=False),
            details.get('hours', ''),
            details.get('address', ''),
            details.get('phone', ''),
            details.get('web_url', ''),
            json.dumps(details.get('photos', []), ensure_ascii=False),
            details.get('ranking', ''),
            details.get('maps_url', details.get('web_url', '')),
            details.get('editorial_summary', ''),
            json.dumps(details.get('recommended_menu', []), ensure_ascii=False),
            json.dumps(details.get('analysis', {}), ensure_ascii=False),
            json.dumps(details.get('weekday_text', []), ensure_ascii=False),
            details.get('language', 'Korean')
        ]
        
        if existing:
            # ì—…ë°ì´íŠ¸
            for i, value in enumerate(row):
                sheet.update_cell(existing.row, i + 1, value)
            print(f"âœ… Restaurant cache updated: {location_id}")
        else:
            # ìƒˆë¡œ ì¶”ê°€
            sheet.append_row(row)
            print(f"âœ… Restaurant cached: {location_id}")
        
        return True
    except Exception as e:
        print(f"Save Cache Error: {e}")
        return False


# ============================================
# ğŸœ Restaurant Fact Check (Google Places API)
# ============================================

# í•œê¸€-ì˜ë¬¸ ë§›ì§‘ ë§¤í•‘ (ë³´ì¡°ìš© - Googleì€ í•œêµ­ì–´ ê²€ìƒ‰ ì˜ë¨)
THAI_FOOD_MAPPING = {
    "ë¹¤íƒ€ë¦¬": "ë°˜íƒ€ë¦¬ ë°©ì½•",
    "ë°˜íƒ€ë¦¬": "ë°˜íƒ€ë¦¬ ë°©ì½•",
    "íŒì‚¬ë§ˆì´": "íŒì‚¬ë§ˆì´ ë°©ì½•",
    "ì©¨íŒŒì´": "ì œì´íŒŒì´ ë°©ì½•",
    "ì œíŒŒì´": "ì œì´íŒŒì´ ë°©ì½•",
    "ì¡ì›": "Zabb One ë°©ì½•",
}

# ìš”ë¦¬ ì¢…ë¥˜ í•„í„°ë§ì„ ìœ„í•œ ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë° ë§¤í•‘ ì‚¬ì „
IGNORED_TYPES = ['establishment', 'point_of_interest', 'food', 'store', 'restaurant', 'meal_takeaway', 'meal_delivery']

CUISINE_MAPPING = {
    "thai_restaurant": "íƒœêµ­ ìŒì‹ì  ğŸ‡¹ğŸ‡­",
    "seafood_restaurant": "í•´ì‚°ë¬¼ ì „ë¬¸ ğŸ¦€",
    "cafe": "ì¹´í˜ â˜•",
    "bar": "ë°”/ìˆ ì§‘ ğŸº",
    "bakery": "ë² ì´ì»¤ë¦¬ ğŸ¥",
    "noodle_shop": "êµ­ìˆ˜ ì „ë¬¸ì  ğŸœ",
    "korean_restaurant": "í•œì‹ë‹¹ ğŸ‡°ğŸ‡·",
    "chinese_restaurant": "ì¤‘ì‹ë‹¹ ğŸ‡¨ğŸ‡³",
    "japanese_restaurant": "ì¼ì‹ë‹¹ ğŸ‡¯ğŸ‡µ",
    "fast_food_restaurant": "íŒ¨ìŠ¤íŠ¸í‘¸ë“œ ğŸ”",
    "vegan_restaurant": "ë¹„ê±´ ì‹ë‹¹ ğŸ¥—",
    "health_food_restaurant": "ê±´ê°•ì‹",
    "breakfast_restaurant": "ì¡°ì‹ ë§›ì§‘",
    "coffee_shop": "ì»¤í”¼ìˆ"
}


def get_menu_search_url(restaurant_name, address):
    """
    ì‹ë‹¹ ì´ë¦„ê³¼ ì£¼ì†Œë¥¼ ì¡°í•©í•˜ì—¬ êµ¬ê¸€ ì´ë¯¸ì§€ ê²€ìƒ‰(ë©”ë‰´íŒ) URLì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    import urllib.parse
    
    # ì£¼ì†Œì—ì„œ ê²€ìƒ‰ì— ë„ì›€ì´ ë ë§Œí•œ ì •ë³´ ì¶”ì¶œ (ì˜ˆ: ë°©ì½•, ì¹˜ì•™ë§ˆì´ ë“± ì§€ì—­ëª…)
    area = ""
    if "Bangkok" in address or "ë°©ì½•" in address:
        area = "Bangkok"
    elif "Chiang Mai" in address or "ì¹˜ì•™ë§ˆì´" in address:
        area = "Chiang Mai"
        
    query = f"{restaurant_name} {area} menu".strip()
    encoded_query = urllib.parse.quote(query)
    
    # tbm=isch íŒŒë¼ë¯¸í„°ë¡œ êµ¬ê¸€ ì´ë¯¸ì§€ ê²€ìƒ‰ íƒ­ìœ¼ë¡œ ë°”ë¡œ ì´ë™
    return f"https://www.google.com/search?q={encoded_query}&tbm=isch"


def analyze_reviews_for_menu(reviews, editorial_summary=""):
    """
    ë¦¬ë·°ì™€ ì—ë””í† ë¦¬ì–¼ ìš”ì•½ë¬¸ì—ì„œ ì¶”ì²œ ë©”ë‰´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ì—ë””í† ë¦¬ì–¼ ìš”ì•½ë¬¸ì— ì–¸ê¸‰ëœ ë©”ë‰´ì—ëŠ” ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.
    """
    MENU_KEYWORDS = {
        "íŒŸíƒ€ì´": ["pad thai", "padthai", "íŒŸíƒ€ì´"],
        "ë˜ ì–‘ê¿": ["tom yum", "tomyam", "tomyum", "ë˜ ì–‘", "ë˜ ì–Œ"],
        "í‘¸íŒŸíì»¤ë¦¬": ["poo pad pong", "crab curry", "í‘¸íŒŸí", "í‘¸íŒŸíì»¤ë¦¬"],
        "ì†œë•€": ["som tum", "somtam", "som tam", "ì†œë•€"],
        "ìŠ¤í…Œì´í¬": ["steak", "ìŠ¤í…Œì´í¬"],
        "ë²„ê±°": ["burger", "ë²„ê±°"],
        "í”¼ì": ["pizza", "í”¼ì"],
        "íŒŒìŠ¤íƒ€": ["pasta", "íŒŒìŠ¤íƒ€"],
        "ë§ê³ ë°¥": ["mango sticky rice", "mango rice", "ë§ê³ ë°¥", "ë§ê³  ìŠ¤í‹°í‚¤"],
        "ë˜ ìŒ¥": ["tom saep", "tom zab", "ë˜ ìŒ¥", "ë˜ ì½"],
        "ê¹Œì´ì–‘": ["kai yang", "grilled chicken", "ê¹Œì´ì–‘"],
        "ë¬´ì‚¥": ["moo ping", "pork skewer", "ë¬´ì‚¥"],
        "ì¹´ì˜¤íŒŸ": ["kao phad", "fried rice", "ì¹´ì˜¤íŒŸ", "ë³¶ìŒë°¥"],
        "ë­ìŒ¥": ["leng saeb", "pork bone soup", "ë­ìŒ¥", "ë­ìƒ™"],
        "í•´ì‚°ë¬¼": ["seafood", "í•´ì‚°ë¬¼", "ì”¨í‘¸ë“œ"],
        "ë˜ ì–Œêµ­ìˆ˜": ["tom yum noodle", "ë˜ ì–Œêµ­ìˆ˜", "ë˜ ì–Œëˆ„ë“¤"]
    }
    
    scores = {}
    all_reviews_text = " ".join([r.get('text', '').lower() for r in reviews])
    summary_text = editorial_summary.lower() if editorial_summary else ""
    
    for menu, keywords in MENU_KEYWORDS.items():
        score = 0
        # ë¦¬ë·° ì–¸ê¸‰ íšŸìˆ˜ (ì¡´ì¬ ì—¬ë¶€ë¡œ ìš°ì„  íŒë‹¨)
        for kw in keywords:
            if kw in all_reviews_text:
                score += 1
            if kw in summary_text:
                score += 3  # ì—ë””í† ë¦¬ì–¼ ìš”ì•½ ê°€ì¤‘ì¹˜ 3ë°°
        
        if score > 0:
            scores[menu] = score
            
    # ì ìˆ˜ ë†’ì€ ìˆœìœ¼ë¡œ ì¶”ì²œ ë©”ë‰´ ì„ ì •
    sorted_menu = sorted(scores.items(), key=lambda x: x[1], reverse=True)
    return [m[0] for m in sorted_menu[:5]]


def calculate_review_score(review):
    """
    ë¦¬ë·°ì˜ í’ˆì§ˆ ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    (ê¸¸ì´, ìµœì‹ ì„±, í‰ì , í‚¤ì›Œë“œ í’ë¶€í•¨ ë“±ì„ ì¢…í•© ê³ ë ¤)
    """
    import time as py_time
    score = 0
    text = review.get('text', '')
    rating = review.get('rating', 0)
    review_time = review.get('time', 0)
    
    if not text:
        return -100
        
    # (1) ê¸¸ì´ (Length) - 50ì ë¯¸ë§Œì€ ê°ì , ìµœëŒ€ 50ì 
    text_len = len(text)
    if text_len < 50:
        score += 0
    else:
        score += min(text_len * 0.1, 50)
        
    # (2) ìµœì‹ ì„± (Recency) - 3ê°œì›”(90ì¼) ê¸°ì¤€
    now = py_time.time()
    three_months_sec = 90 * 24 * 60 * 60
    one_year_sec = 365 * 24 * 60 * 60
    
    if review_time > 0:
        diff = now - review_time
        if diff < three_months_sec:
            score += 30
        elif diff > one_year_sec:
            score -= 10
            
    # (3) í‰ì  (Rating) - 4ì  ì´ìƒ ìš°ëŒ€
    if rating >= 4:
        score += 20
        
    # (4) í‚¤ì›Œë“œ í¬í•¨ (Rich Content)
    RICH_KEYWORDS = ["ê°€ê²©", "ë©”ë‰´", "ì›¨ì´íŒ…", "ì„œë¹„ìŠ¤", "ì¹œì ˆ", "ì²­ê²°", "ìœ„ìƒ", 
                     "price", "taste", "queue", "service", "menu", "clean"]
    all_text_lower = text.lower()
    for kw in RICH_KEYWORDS:
        if kw in all_text_lower:
            score += 5
            
    # (5) ì¢‹ì•„ìš” (Likes/Helpful) - êµ¬ê¸€ APIëŠ” ê³µì‹ì ìœ¼ë¡œ likesë¥¼ ì•ˆì£¼ì§€ë§Œ ëŒ€ì‘ ë¡œì§
    likes = review.get('likes', 0) or review.get('helpful_votes', 0)
    if likes:
        score += int(likes) * 10
        
    return score


def analyze_restaurant_reviews(reviews, rating, price_level=0, name="", num_reviews=0, api_key=None, language="Korean"):
    """
    ë¦¬ë·° í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ì¥ì , ë‹¨ì , í•œì¤„í‰ì„ ë„ì¶œí•©ë‹ˆë‹¤. (ë‹¤êµ­ì–´ ì§€ì›)
    """
    is_english = (language == "English")
    
    if not reviews:
        return {
            'pros': ["No enough info" if is_english else "ì •ë³´ ë¶€ì¡±ìœ¼ë¡œ ì¥ì  ë„ì¶œ ë¶ˆê°€"],
            'cons': ["No enough info" if is_english else "ì •ë³´ ë¶€ì¡±ìœ¼ë¡œ ë‹¨ì  ë„ì¶œ ë¶ˆê°€"],
            'verdict': "No enough data to analyze." if is_english else "ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            'one_line_verdict': "No enough data to analyze." if is_english else "ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            'warnings': [],
            'best_review': None
        }

    # 1. Gemini AI Analysis (If API Key provided)
    ai_result = None
    if api_key:
        try:
            # [ìˆ˜ë‹¤ìŸì´ ìš°ì„  ë²•ì¹™] ë¦¬ë·°ë¥¼ ê¸¸ì´(ì •ë³´ëŸ‰) ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ 10ê°œ ì„ íƒ
            sorted_reviews = sorted(reviews, key=lambda x: len(x.get('text', '')), reverse=True)
            
            reviews_text = ""
            for r in sorted_reviews[:10]:  # ìƒìœ„ 10ê°œ ì°¸ì¡°
                text = r.get('text', '')
                r_rating = r.get('rating', 0)
                if text and len(text) > 10: # ìµœì†Œ 10ì ì´ìƒ
                    reviews_text += f"- [{r_rating}/5] {text}\n"

            genai.configure(api_key=api_key)
            model = genai.GenerativeModel('gemini-2.0-flash', generation_config={"response_mime_type": "application/json"})

            # í‰ì  ë° ì–¸ì–´ ê¸°ë°˜ ì§€ì¹¨
            if is_english:
                tone_instruction = f"Target Restaurant: {name}. Rating: {rating}. Be sharp and honest."
                if rating >= 4.5:
                    tone_instruction += " Highly recommended but finding subtle cons is mandatory."
                else:
                    tone_instruction += f" Lower rating ({rating}). Mention controversy or price issues."
                
                lang_instruction = "IMPORTANT: ALL JSON OUTPUT VALUES MUST BE IN ENGLISH."
                persona = "You are a sharp-tongued food critic expert on Bangkok."
            else:
                if rating >= 4.5:
                    tone_instruction = "ì´ ì‹ë‹¹ì€ í‰ì  4.5 ì´ìƒì˜ 'ê°•ë ¥ ì¶”ì²œ' ê¸‰ì…ë‹ˆë‹¤. ë‹¨, ë‹¨ì ì´ ìˆë‹¤ë©´ ê·¸ê²ƒë„ ë°˜ë“œì‹œ ì–¸ê¸‰í•˜ì„¸ìš”."
                elif rating >= 4.0:
                    tone_instruction = "ì´ ì‹ë‹¹ì€ í‰ì  4.0~4.4ì˜ 'ì•ˆì •ì ì¸ ì„ íƒ'ì…ë‹ˆë‹¤. ì¥ì ê³¼ ë‹¨ì ì„ ê· í˜• ìˆê²Œ ì„œìˆ í•˜ì„¸ìš”."
                else:
                    tone_instruction = f"âš ï¸ ì£¼ì˜: ì´ ì‹ë‹¹ì€ í‰ì  {rating}ì ìœ¼ë¡œ 4.0 ë¯¸ë§Œì…ë‹ˆë‹¤. ì•„ë¬´ë¦¬ ìœ ëª…í•´ë„ 'ê°•ë ¥ ì¶”ì²œ'ì´ë¼ê³  ì ˆëŒ€ ë§í•˜ì§€ ë§ˆì„¸ìš”."
                
                lang_instruction = "ì¤‘ìš”: ëª¨ë“  JSON ì¶œë ¥ê°’ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”."
                persona = "ë‹¹ì‹ ì€ ë°©ì½• í˜„ì§€ ì‚¬ì •ì— ì •í†µí•œ 'ë…ì„¤ê°€ ìŒì‹ ë¹„í‰ê°€'ì…ë‹ˆë‹¤."

            prompt = f"""
            {persona}
            Analyze the [Restaurant Info] and [Visitor Reviews] provided and write a sharp, factual report.
            {lang_instruction}

            -----
            ### ğŸš¨ [Knowledge Augmentation]
            If the reviews are too short (e.g. "Good", "Delicious"), use your internal knowledge about {name} in Bangkok to provide detailed facts.
            (e.g., for Wattana Panich: mention the 50-year-old soup, no AC, mixed reviews on hygiene).

            -----
            ### [Guidelines]
            1. **One Line Verdict:** Sharp, high-impact sentence summarizing pros and cons.
            2. **Pros:** Specific food names, taste profiles, atmosphere. No generic terms.
            3. **Cons:** Mandatory even for high-rated places. Hygiene, wait, heat, price, service, location.
            4. **Warnings:** Practical tips (Cash only, No AC, Queue tips).

            -----
            [Restaurant Info]
            - Name: {name}
            - Rating: {rating}
            - Reviews Count: {num_reviews}

            [Review Data]
            {reviews_text}
            
            [Tone]: {tone_instruction}

            **[Output Format (JSON)]**
            {{
                "one_line_verdict": "string",
                "pros": ["string", "string"],
                "cons": ["string", "string"],
                "warnings": ["string", "string"]
            }}
            """
            
            response = model.generate_content(prompt)
            print(f"DEBUG: Gemini Restaurant Raw Response: {response.text}")
            
            # Clean JSON if wrapped in markdown
            text = response.text.strip()
            if text.startswith("```json"):
                text = text.split("```json")[1].split("```")[0].strip()
            elif text.startswith("```"):
                text = text.split("```")[1].split("```")[0].strip()
                
            ai_result = json.loads(text)
            print(f"DEBUG: Extracted AI Result: {ai_result}")
        except Exception as e:
            import traceback
            print(f"Gemini Restaurant Analysis Error: {e}")
            print(traceback.format_exc())
            ai_result = None
    else:
        print("[DEBUG] No API Key provided for Restaurant Analysis")

    # 2. Keyword-based Analysis (Fallback or Complement)
    PRO_KEYWORDS = {
        "ë§›ìˆë‹¤": "í™•ì‹¤í•œ ë§› ë³´ì¥ ğŸ˜‹", "ìµœê³ ": "ë°©ë¬¸ê° ë§Œì¡±ë„ ë†’ìŒ ğŸ‘", "ì¹œì ˆ": "ì¹œì ˆí•œ ì„œë¹„ìŠ¤ âœ¨",
        "ê°€ì„±ë¹„": "í›Œë¥­í•œ ê°€ì„±ë¹„ ğŸ’°", "ì €ë ´": "ë¶€ë‹´ ì—†ëŠ” ê°€ê²©", "ê¹¨ë—": "ì²­ê²°í•œ ìœ„ìƒ ìƒíƒœ ğŸ§¼",
        "ë¶„ìœ„ê¸°": "ë¶„ìœ„ê¸° ë§›ì§‘ ğŸ•¯ï¸", "ê¹”ë”": "ê¹”ë”í•œ ìƒì°¨ë¦¼", "ì‹ ì„ ": "ì‹ ì„ í•œ ì¬ë£Œ ì‚¬ìš© ğŸ¥—",
        "ì¢‹ì•„ìš”": "ì „ë°˜ì ìœ¼ë¡œ í˜¸í‰", "delicious": "í™•ì‹¤í•œ ë§› ë³´ì¥ ğŸ˜‹", "fresh": "ì‹ ì„ í•œ ì¬ë£Œ ğŸ¥—",
        "cheap": "ì €ë ´í•œ ê°€ê²©", "kind": "ì¹œì ˆí•œ ì„œë¹„ìŠ¤ âœ¨", "nice": "ê¸°ë¶„ ì¢‹ì€ ë°©ë¬¸"
    }
    
    CON_KEYWORDS = {
        "ì§œë‹¤": "ê°„ì´ ì„¼ í¸ (Salty)", "ì§œìš”": "ê°„ì´ ì„¼ í¸ (Salty)", "salty": "ê°„ì´ ì„¼ í¸ (Salty)",
        "ë‹¬ë‹¤": "ë‹¨ë§›ì´ ê°•í•¨ (Sweet)", "sweet": "ë‹¨ë§›ì´ ê°•í•¨ (Sweet)", "ë§µë‹¤": "ë§¤ìš´ í¸ (Spicy)",
        "spicy": "ë§¤ìš´ í¸ (Spicy)", "ì›¨ì´íŒ…": "ê¸´ ëŒ€ê¸° ì‹œê°„ ì£¼ì˜ â³", "ëŒ€ê¸°": "ê¸´ ëŒ€ê¸° ì‹œê°„ ì£¼ì˜ â³",
        "queue": "ê¸´ ëŒ€ê¸° ì‹œê°„ ì£¼ì˜ â³", "ë¹„ì‹¸": "ê°€ê²©ëŒ€ê°€ ë†’ìŒ ğŸ’¸", "expensive": "ê°€ê²©ëŒ€ê°€ ë†’ìŒ ğŸ’¸",
        "ë¥ë‹¤": "ë‚´ë¶€ê°€ ë”ìš´ í¸ ğŸŒ¡ï¸", "ë”ì›Œ": "ë‚´ë¶€ê°€ ë”ìš´ í¸ ğŸŒ¡ï¸", "hot": "ë‚´ë¶€ê°€ ë”ìš´ í¸ ğŸŒ¡ï¸",
        "no ac": "ì—ì–´ì»¨ ì—†ìŒ/ì•½í•¨", "ë¶ˆì¹œì ˆ": "ì„œë¹„ìŠ¤ ì•„ì‰¬ì›€ ğŸ˜•", "ì–‘ ì ìŒ": "ì–‘ì´ ì ì„ ìˆ˜ ìˆìŒ",
        "ì¢ìŒ": "ê³µê°„ì´ í˜‘ì†Œí•¨", "waiting": "ëŒ€ê¸° ë°œìƒ ê°€ëŠ¥"
    }

    pros = []
    cons = []
    ai_warnings = []  # AIê°€ ì¶”ì¶œí•œ ê²½ê³  íƒœê·¸
    all_text = ""
    scored_reviews = []

    for r in reviews:
        text = r.get('text', '')
        if text:
            all_text += text.lower() + " "
            score = calculate_review_score(r)
            scored_reviews.append({
                'score': score,
                'review_data': {
                    'text': text,
                    'rating': r.get('rating', 0),
                    'relative_time': r.get('relative_time_description', 'ìµœê·¼')
                }
            })

    # ë² ìŠ¤íŠ¸ ë¦¬ë·° ì„ ì • (Top 3)
    best_reviews = []
    if scored_reviews:
        sorted_scored = sorted(scored_reviews, key=lambda x: x['score'], reverse=True)
        # Top 3 ì¶”ì¶œ
        best_reviews = [item['review_data'] for item in sorted_scored[:3]]
    elif reviews:
        # ì ìˆ˜ ê³„ì‚°ì´ ì•ˆ ëœ ê²½ìš° ìµœì‹ ìˆœ 3ê°œ
        best_reviews = [{'text': r.get('text', ''), 'rating': r.get('rating', 0), 'relative_time': r.get('relative_time_description', 'ìµœê·¼')} for r in reviews[:3]]

    # AI ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ í‚¤ì›Œë“œ ê¸°ë°˜
    if ai_result:
        ai_pros = ai_result.get('pros', [])
        ai_cons = ai_result.get('cons', [])
        ai_verdict = ai_result.get('one_line_verdict', '')
        ai_warnings = ai_result.get('warnings', [])
        
        pros = ai_pros if ai_pros else pros
        cons = ai_cons if ai_cons else cons
        verdict = ai_verdict if ai_verdict else ""
    else:
        # í‚¤ì›Œë“œ ê¸°ë°˜ ì¥ë‹¨ì  ì¶”ì¶œ
        for kw, label in PRO_KEYWORDS.items():
            if kw in all_text and label not in pros:
                pros.append(label)
        
        for kw, label in CON_KEYWORDS.items():
            if kw in all_text and label not in cons:
                cons.append(label)
        
        verdict = ""

    # ê¸°ë³¸ í•œì¤„í‰ ì‚°ì¶œ (AI í‰ì´ ì—†ì„ ê²½ìš° ì‚¬ìš©) - í‰ì  ê¸°ë°˜ ë¶„ê¸°
    if not verdict:
        if rating >= 4.5:
            verdict = "ì‹¤íŒ¨ ì—†ëŠ” í˜„ì§€ì¸ ì¶”ì²œ ë§›ì§‘ ğŸ†"
        elif rating >= 4.0:
            if "ì›¨ì´íŒ…" in all_text or "ëŒ€ê¸°" in all_text or "queue" in all_text:
                verdict = "ì•ˆì •ì ì¸ ë§›ì´ì§€ë§Œ ì›¨ì´íŒ…ì€ ê°ì˜¤í•´ì•¼ í•˜ëŠ” ê³³ â³"
            elif price_level >= 3:
                verdict = "ë§›ì€ ë³´ì¥ë˜ì§€ë§Œ ê°€ê²©ëŒ€ê°€ ìˆëŠ” ê³³ ğŸ’°"
            else:
                verdict = f"ë¬´ë‚œí•˜ê²Œ ì¦ê¸¸ ìˆ˜ ìˆëŠ” {name or 'ë§›ì§‘'}"
        else:
            # 4.0 ë¯¸ë§Œ: ë°˜ë“œì‹œ ë¶€ì •ì  ë‰˜ì•™ìŠ¤ í¬í•¨
            if "ì›¨ì´íŒ…" in all_text or "ëŒ€ê¸°" in all_text:
                verdict = f"ìœ ëª…ì„¸ì— ë¹„í•´ í‰ì ì´ ë‚®ê³ ({rating}ì ), ì›¨ì´íŒ… ì§€ì˜¥ê¹Œì§€ ê°ì˜¤í•´ì•¼ í•˜ëŠ” ê³³ âš ï¸"
            elif price_level >= 3:
                verdict = f"ëª…ì„±ì€ ìˆì§€ë§Œ ì‚¬ì•…í•œ ê°€ê²©ê³¼ {rating}ì ëŒ€ í‰ì ì´ ì•„ì‰¬ìš´ ê³³ ğŸ’¸"
            else:
                verdict = f"í˜¸ë¶ˆí˜¸ê°€ ê°ˆë¦¬ëŠ” ê³³ - í‰ì  {rating}ì ìœ¼ë¡œ ê¸°ëŒ€ì¹˜ ì¡°ì ˆ í•„ìš” âš ï¸"

    warnings = []
    
    # 1. AI ê¸°ë°˜ ê²½ê³  íƒœê·¸ ì¶”ê°€ (ìµœìš°ì„ )
    seen_warnings = set()
    if ai_warnings:
        for w in ai_warnings:
             # ë„ˆë¬´ ê¸´ ê²ƒì€ ìë¥´ê¸° (10ì ì´ë‚´ ê¶Œì¥í–ˆìœ¼ë‚˜ ì˜ˆì™¸ ì²˜ë¦¬)
             w_clean = w[:15]
             warnings.append({'type': 'ai_alert', 'message': f'âš ï¸ {w_clean}', 'level': 'warning'})
             seen_warnings.add(w_clean)

    # 2. í‚¤ì›Œë“œ ê¸°ë°˜ ê²½ê³  ì¶”ê°€ (ì¤‘ë³µ ë°©ì§€)
    if ("ì§œë‹¤" in all_text or "salty" in all_text) and "ê°„ì´ ì…ˆ" not in seen_warnings:
        warnings.append({'type': 'taste', 'message': 'ğŸ§‚ ê°„ì´ ì„¼ í¸', 'level': 'warning'})
    if ("ì›¨ì´íŒ…" in all_text or "queue" in all_text) and "ì›¨ì´íŒ…" not in str(seen_warnings):
        warnings.append({'type': 'waiting', 'message': 'â³ ì›¨ì´íŒ… ì£¼ì˜', 'level': 'info'})
    if ("ë”ì›Œ" in all_text or "hot" in all_text) and "ë”ì›€" not in seen_warnings:
        warnings.append({'type': 'hygiene', 'message': 'ğŸŒ¡ï¸ ë‚´ë¶€ ë”ì›€', 'level': 'warning'})
    
    # í‰ì  4.0 ë¯¸ë§Œ ê²½ê³  ì¶”ê°€
    if rating < 4.0:
        warnings.append({'type': 'rating', 'message': f'ğŸ“‰ í‰ì  {rating}ì  (í˜¸ë¶ˆí˜¸)', 'level': 'error'})

    return {
        'pros': pros[:3] if pros else ["ì „ë°˜ì ìœ¼ë¡œ ë¬´ë‚œí•¨"],
        'cons': cons[:3] if cons else ["íŠ¹ë³„í•œ ë‹¨ì  ë°œê²¬ë˜ì§€ ì•ŠìŒ âœ¨"],
        'verdict': verdict,
        'one_line_verdict': verdict,
        'warnings': warnings,
        'best_review': best_reviews[0] if best_reviews else None, # Legacy support
        'best_reviews': best_reviews # New list support
    }



def extract_restaurant_share_summary(name, details):
    """
    ë§›ì§‘ ë¶„ì„ ê²°ê³¼ ê³µìœ ìš© í…ìŠ¤íŠ¸ ìƒì„±
    """
    analysis = details.get('analysis', {})
    cuisines = ", ".join(details.get('cuisines', []))
    pros = "\n- ".join(analysis.get('pros', ["ì „ë°˜ì ìœ¼ë¡œ ë¬´ë‚œí•¨"]))
    cons = "\n- ".join(analysis.get('cons', ["íŠ¹ë³„í•œ ë‹¨ì  ë°œê²¬ë˜ì§€ ì•ŠìŒ"]))
    
    summary = f"""[ğŸ‡¹ğŸ‡­ íƒœêµ­ ë§›ì§‘ íŒ©íŠ¸ì²´í¬]

ğŸ½ï¸ ì‹ë‹¹ëª…: {name} ({cuisines})
â­ í‰ì : {details.get('rating', 0)} / 5.0 (ë¦¬ë·° {details.get('num_reviews', 0):,}ê°œ)
ğŸ’° ê°€ê²©ëŒ€: {details.get('price_text', 'ì •ë³´ ì—†ìŒ')}

ğŸ† í•œì¤„ í‰: "{analysis.get('verdict', '')}"

ğŸ‘ ì¥ì :
- {pros}

ğŸ‘ ë‹¨ì :
- {cons}

ğŸ“ êµ¬ê¸€ë§µ ë³´ê¸°: {details.get('web_url', '')}
ğŸ”— í™•ì¸í•˜ê¸°: thai-today.com"""
    return summary.strip()


def analyze_review_sentiment(reviews):
    """
    êµ¬í˜• í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ í•¨ìˆ˜ (ë¯¸ë˜ì—ëŠ” analyze_restaurant_reviewsë¡œ í†µí•© ê°€ëŠ¥)
    """
    return analyze_restaurant_reviews(reviews, 4.0)


def search_restaurants(keyword):
    """
    Google Places APIë¡œ ì‹ë‹¹ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    ìºì‹œ ìš°ì„ : ë¨¼ì € ìºì‹œì—ì„œ ê²€ìƒ‰ í›„ API í˜¸ì¶œ
    
    Args:
        keyword: ê²€ìƒ‰ì–´ (ì‹ë‹¹ ì´ë¦„)
    
    Returns:
        list: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ [{place_id, name, address, is_cached}, ...]
    """
    import requests
    
    # 1ë‹¨ê³„: ìºì‹œì—ì„œ ë¨¼ì € ê²€ìƒ‰
    cached_results = search_cached_restaurants(keyword)
    
    # 2ë‹¨ê³„: Google Places Text Search API í˜¸ì¶œ
    api_results = []
    try:
        google_places_key = st.secrets.get("google_maps_api_key")
        if not google_places_key:
            # googlemaps_api í‚¤ë¡œ í´ë°±
            google_places_key = st.secrets.get("googlemaps_api")
        
        if google_places_key:
            url = "https://maps.googleapis.com/maps/api/place/textsearch/json"
            
            # [FIX] Relax constraints to include cafes and places outside Bangkok
            # Original: query="... restaurant Bangkok Thailand", type="restaurant"
            params = {
                "query": f"{keyword} Thailand", 
                "language": "ko",
                "key": google_places_key
            }
            
            response = requests.get(url, params=params, timeout=10)
            
            # Fallback: If no results with "Thailand", try just the keyword
            if response.status_code == 200:
                data = response.json()
                if data.get('status') == 'ZERO_RESULTS':
                    params["query"] = keyword
                    response = requests.get(url, params=params, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                
                if data.get('status') == 'OK':
                    results = data.get('results', [])
                    
                    # ìºì‹œëœ place_id ì¶”ì¶œ (ì¤‘ë³µ ë°©ì§€)
                    cached_ids = {r.get('location_id', r.get('place_id', '')) for r in cached_results}
                    
                    for place in results[:10]:
                        place_id = place.get('place_id')
                        name = place.get('name', '')
                        
                        if place_id and name and place_id not in cached_ids:
                            api_results.append({
                                'place_id': place_id,
                                'location_id': place_id,  # í˜¸í™˜ì„±
                                'name': name,
                                'address': place.get('formatted_address', 'ì£¼ì†Œ ì •ë³´ ì—†ìŒ'),
                                'rating': place.get('rating', 0),
                                'is_cached': False
                            })
                else:
                    print(f"Google Places Search: {data.get('status')}")
            else:
                print(f"Google Places Error: {response.status_code}")
    except Exception as e:
        print(f"Google Places Search Error: {e}")
    
    # ìºì‹œ ê²°ê³¼ë¥¼ ë¨¼ì € ë³´ì—¬ì£¼ê³ , API ê²°ê³¼ë¥¼ ë’¤ì— ì¶”ê°€
    combined = cached_results + api_results
    return combined[:10]  # ìµœëŒ€ 10ê°œ


def get_restaurant_details(place_id, gemini_api_key=None, language="Korean"):
    # 1ë‹¨ê³„: ìºì‹œì—ì„œ ë¨¼ì € í™•ì¸ (API ë¹„ìš© 0)
    cached = get_cached_restaurant_details(place_id, language=language)
    if cached:
        print(f"âœ… Cache hit ({language}) for restaurant place_id: {place_id}")
        # ìºì‹œ íˆíŠ¸ ì‹œì—ë„ ì¸ê¸° ë­í‚¹ìš© ë¡œê·¸ ê¸°ë¡
        log_search(cached['name'], cached['rating'], 'food')
        return cached
    
    # 2ë‹¨ê³„: Google Places Details API í˜¸ì¶œ (ë¹„ìš© ë°œìƒ)
    try:
        google_places_key = st.secrets.get("google_maps_api_key")
        if not google_places_key:
            google_places_key = st.secrets.get("googlemaps_api")
        
        if not google_places_key:
            return None
        
        url = "https://maps.googleapis.com/maps/api/place/details/json"
        
        # í•„ìš”í•œ í•„ë“œë§Œ ìš”ì²­ (ë¹„ìš© ìµœì í™” + ë¦¬ë·° í¬í•¨)
        params = {
            "place_id": place_id,
            "fields": "name,rating,user_ratings_total,price_level,formatted_address,formatted_phone_number,opening_hours,photos,url,types,reviews,editorial_summary",
            "language": "ko",
            "key": google_places_key
        }
        
        response = requests.get(url, params=params, timeout=15)
        
        if response.status_code != 200:
            print(f"Google Places Details Error: {response.status_code}")
            return None
        
        data = response.json()
        
        if data.get('status') != 'OK':
            print(f"Google Places Details: {data.get('status')}")
            return None
        
        result_data = data.get('result', {})
        
        # ìƒì„¸ ì •ë³´ íŒŒì‹±
        rating = float(result_data.get('rating', 0) or 0)
        num_reviews = int(result_data.get('user_ratings_total', 0) or 0)
        price_level = result_data.get('price_level', 0)  # 0-4
        
        # ê°€ê²©ëŒ€ í…ìŠ¤íŠ¸ ë³€í™˜
        price_text = ""
        if price_level == 1:
            price_text = "ğŸ’° ì €ë ´"
        elif price_level == 2:
            price_text = "ğŸ’°ğŸ’° ë³´í†µ"
        elif price_level == 3:
            price_text = "ğŸ’°ğŸ’°ğŸ’° ë¹„ì‹¼í¸"
        elif price_level == 4:
            price_text = "ğŸ’°ğŸ’°ğŸ’°ğŸ’° ê³ ê¸‰"
        
        # ì‚¬ì§„ URL ìƒì„± (photo_reference ì‚¬ìš©)
        photos = []
        for photo in result_data.get('photos', [])[:5]:
            photo_ref = photo.get('photo_reference')
            if photo_ref:
                photo_url = f"https://maps.googleapis.com/maps/api/place/photo?maxwidth=400&photo_reference={photo_ref}&key={google_places_key}"
                photos.append(photo_url)
        
        # ì˜ì—…ì‹œê°„
        opening_hours = result_data.get('opening_hours', {})
        is_open = opening_hours.get('open_now', None)
        weekday_text = opening_hours.get('weekday_text', [])
        
        hours_text = ""
        if is_open is True:
            hours_text = "ğŸŸ¢ ì˜ì—…ì¤‘"
        elif is_open is False:
            hours_text = "ğŸ”´ ì˜ì—…ì¢…ë£Œ"
        
        # ìš”ë¦¬ ì¢…ë¥˜ ì¶”ì¶œ (í•„í„°ë§ ë° í•œê¸€í™” ì ìš©)
        types = result_data.get('types', [])
        cuisines = []
        for t in types:
            if t not in IGNORED_TYPES:
                # ë§¤í•‘ëœ í•œê¸€ ëª…ì¹­ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ Pretty Print
                ko_name = CUISINE_MAPPING.get(t)
                if ko_name:
                    cuisines.append(ko_name)
                else:
                    cuisines.append(t.replace('_', ' ').title())
        
        # ë§Œì•½ í•„í„°ë§ í›„ ë‚¨ì€ ê²Œ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì„¤ì •
        if not cuisines:
            cuisines = ["ì¼ë°˜ ìŒì‹ì "]
        
        # ë¦¬ìŠ¤íŠ¸ ì¤‘ ê°€ì¥ êµ¬ì²´ì ì¸ 1~2ê°œë§Œ ì‚¬ìš©
        cuisines = cuisines[:2]
        
        # [FIX] Define missing variables extracted from result_data
        reviews = result_data.get('reviews', [])
        name = result_data.get('name', '')
        # editorial_summary is a dict with 'text' and 'languageCode'
        editorial_summary = result_data.get('editorial_summary', {}).get('text', '')
        
        analysis = analyze_restaurant_reviews(reviews, rating, price_level, name, num_reviews=num_reviews, api_key=gemini_api_key, language=language)
        recommended_menu = analyze_reviews_for_menu(reviews, editorial_summary)
        
        result = {
            'language': language,
            'name': result_data.get('name', ''),
            'rating': rating,
            'num_reviews': num_reviews,
            'price_level': price_level,
            'price_text': price_text,
            'address': result_data.get('formatted_address', ''),
            'phone': result_data.get('formatted_phone_number', ''),
            'photos': photos,
            'hours': hours_text,
            'weekday_text': weekday_text,
            'is_open': is_open,
            'cuisines': cuisines[:3],
            'web_url': result_data.get('url', ''),
            'maps_url': result_data.get('url', ''),
            'menu_url': get_menu_search_url(result_data.get('name', ''), result_data.get('formatted_address', '')),
            'editorial_summary': editorial_summary,
            'recommended_menu': recommended_menu,
            # íŒ©íŠ¸ì²´í¬ ë¦¬í¬íŠ¸ ë°ì´í„°
            'analysis': analysis,
            # í˜¸í™˜ì„±ìš©
            'food_rating': rating,
            'atmosphere_rating': rating,
            'location_rating': rating,
        }
        
        # 3ë‹¨ê³„: ìºì‹œì— ì €ì¥ (ë‹¤ìŒì—” API ì•ˆ ë¶ˆëŸ¬ë„ ë¨)
        save_restaurant_to_cache(place_id, result)
        
        return result
        
    except Exception as e:
        import traceback
        error_msg = f"Detailed Error: {str(e)}\n{traceback.format_exc()}"
        print(f"Google Places Details Error: {error_msg}")
        return None # Keep returning None, but print detailed traceback

# Helper: Load Custom CSS from file
def load_custom_css():
    """
    Loads custom CSS from style.css and injects it into the Streamlit app.
    This applies Thai-Today.com design spec: Playfair Display fonts, Kanit,
    Glassmorphism cards, Royal Gold theme, and Deep Silk Purple accents.
    """
    css_file = "style.css"
    if os.path.exists(css_file):
        with open(css_file, "r", encoding="utf-8") as f:
            css_content = f.read()
        st.markdown(f"<style>{css_content}</style>", unsafe_allow_html=True)
    else:
        # Fallback inline CSS if file missing
        st.markdown("""
        <style>
        @import url('https://fonts.googleapis.com/css2?family=Playfair+Display:wght@400;600;700&family=Kanit:wght@300;400;500&display=swap');
        h1, h2, h3 { font-family: 'Playfair Display', Georgia, serif !important; }
        body, p, div { font-family: 'Kanit', sans-serif !important; }
        </style>
        """, unsafe_allow_html=True)

# Helper: Render Hero Section with Glassmorphism
def render_hero_section(title="ì˜¤ëŠ˜ì˜ íƒœêµ­", subtitle="ì‹¤ì‹œê°„ íƒœêµ­ ì—¬í–‰ ì •ë³´ íë ˆì´ì…˜", image_url=None):
    """
    Renders a premium hero banner at the top of the page.
    Uses the Thai-Today.com design spec: dark gradient overlay, Playfair Display title.
    """
    bg_style = ""
    if image_url:
        bg_style = f"background-image: url('{image_url}'); background-size: cover; background-position: center;"
    else:
        # Default gradient background
        bg_style = "background: linear-gradient(135deg, #2D2D2D 0%, #4B0082 50%, #D4AF37 100%);"
    
    hero_html = f"""
    <div class="hero-section" style="{bg_style}">
        <div class="hero-content">
            <h1>{title}</h1>
            <p>{subtitle}</p>
        </div>
    </div>
    """
    st.markdown(hero_html, unsafe_allow_html=True)

# Helper: Render Glass Card wrapper
def render_glass_card(content_html, custom_class=""):
    """
    Wraps content in a glassmorphism card container.
    """
    card_html = f"""
    <div class="glass-card {custom_class}">
        {content_html}
    </div>
    """
    st.markdown(card_html, unsafe_allow_html=True)

# Helper: Render Category Tag
def render_category_tag(category, variant="travel"):
    """
    Renders a styled category tag badge.
    Variants: travel (gold), food (red), safety (purple), economy (green)
    """
    tag_html = f'<span class="category-tag {variant}">{category}</span>'
    return tag_html

# Helper: Render Custom Mobile-Optimized Header
def render_custom_header(text, level=1):
    """
    Renders a custom HTML header for SEO and Mobile UI optimization.
    - H1: 22px (Mobile Friendly)
    - H2: 18px
    - Adjusts margins to save space.
    """
    font_size = "22px" if level == 1 else "18px"
    margin = "10px 0 5px 0"
    color = "#333333" # Default dark grey, can be adjusted for dark mode via CSS variables if needed
    
    # Use CSS variable for text color to support Dark Mode automatically if desired,
    # or stick to fixed color. Let's use var(--text-color) for better adaptation.
    # But user requested #333333 specifically. Let's stick to user request but add dark mode support via Streamlit's theming if possible.
    # User said: "Color: #333333 (ë‹¤í¬ëª¨ë“œ ëŒ€ì‘ í•„ìš”ì‹œ var(--text-color) ì‚¬ìš©)"
    # Let's use var(--text-color) to be safe for dark mode which is active.
    
    st.markdown(
        f"""
        <{f'h{level}'} style='text-align: left; font-size: {font_size}; font-weight: 700; margin: {margin}; color: var(--text-color); line-height: 1.2;'>
            {text}
        </{f'h{level}'}>
        """,
        unsafe_allow_html=True
    )

# Helper: Check if text contains Thai characters
def is_thai(text):
    import re
    if not text: return False
    return bool(re.search(r'[\u0E00-\u0E7F]', text))

# Helper: Convert Thai Buddhist year to Gregorian year
def convert_thai_year(text: str) -> str:
    import re
    def repl(match):
        year = int(match.group())
        if year > 2500:  # typical Buddhist year
            return str(year - 543)
        return match.group()
    return re.sub(r'\b\d{4}\b', repl, text)

# Helper: Translate text to Korean using Gemini
def translate_text(text: str, dest: str = "ko") -> str:
    """
    Translate Thai text to Korean using Gemini 2.0 Flash.
    Handles API key loading and ensures robust response.
    """
    # 1. Quick Check: Is it already Korean or just numbers?
    if not text or len(text.strip()) == 0:
        return ""
    
    # 2. Convert Thai Buddhist year first
    text = convert_thai_year(text)
    
    # 3. Use Gemini
    try:
        # Lazy load API key if needed (or assume configured globally in app)
        # But utils might be imported separately, so re-check/configure.
        import google.generativeai as genai
        import toml
        
        # Try to get key efficiently
        api_key = os.environ.get("GEMINI_API_KEY")
        if not api_key:
            try:
                secrets = toml.load(".streamlit/secrets.toml")
                api_key = secrets.get("GEMINI_API_KEY")
            except: pass
            
        if api_key:
            genai.configure(api_key=api_key)
            
        model = genai.GenerativeModel('gemini-2.0-flash')
        
        # Aggressive Prompt to ensure zero Thai script remains
        prompt = f"""
        Translate the following text to Korean.
        - IMPORTANT: Every single Thai character (script) MUST be converted/translated.
        - Use phonetic Hangul for names or terms if no direct translation exists (e.g., 'à¹à¸”à¸‡' -> 'ëŒ•').
        - The output must contain ZERO Thai script.
        - If the text is a mix of Thai and Korean, translate only the Thai parts while keeping the Korean.
        - Output ONLY the result. No explanations.
        
        Text:
        {text}
        """
        
        response = model.generate_content(prompt)
        translated = response.text.strip()
        
        # Double check: if it still has Thai, try one more time or just return it
        # But for now, the prompt should be enough.
        return translated
        
    except Exception as e:
        print(f"Translation Error for '{text[:20]}...': {e}")
        return text


# Helper: Check if article is within last N days
def is_recent(entry, days=3):
    if not hasattr(entry, 'published_parsed'):
        return True # Default to include if no date
    
    # published_parsed is a struct_time, convert to datetime
    pub_date = datetime.fromtimestamp(time.mktime(entry.published_parsed))
    limit_date = datetime.now() - timedelta(days=days)
    return pub_date >= limit_date

# Helper: Check relevance to Thailand
def is_relevant_to_thailand(entry):
    """
    Determines if an article is relevant to Thailand based on keywords and script.
    Checks: Title, Summary (if available)
    """
    import re
    
    # 1. content to check
    text = (entry.title + " " + entry.get('summary', '')).lower()
    
    # 2. Check for Thai Characters (Script)
    if re.search(r'[\u0E00-\u0E7F]', text):
        return True
        
    # 3. Check for English Keywords
    keywords = [
        "thailand", "thai", "bangkok", "phuket", "pattaya", "chiang", 
        "samui", "krabi", "isan", "baht", "pheu thai", 
        "prime minister", "paetongtarn", "thaksin", "king", "royal",
        "cabinet", "govt", "police", "otp", "airport"
    ]
    
    for kw in keywords:
        if kw in text:
            return True
            
    return False

# 1. RSS Parsing (Balanced)
def fetch_balanced_rss(feeds_config, processed_urls=None):
    """
    Fetches RSS feeds and returns a balanced mix of items across categories.
    feeds_config: List of dicts [{'category': '...', 'url': '...'}, ...]
    processed_urls: Set of strings (optional) to skip already seen news.
    """
    import requests
    
    if processed_urls is None:
        processed_urls = set()
    
    # Using a typical browser User-Agent
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
        'Accept': 'application/rss+xml, application/xml, text/xml, */*'
    }
    
    category_buckets = {}
    MAX_PER_CATEGORY = 80  # Increased from 20 to 80 to allow checking more feeds (e.g. Pattaya News)
    
    for feed in feeds_config:
        category = feed.get('category', 'General')
        url = feed.get('url')
        
        if category not in category_buckets:
            category_buckets[category] = []
            
        # Check quota early
        if len(category_buckets[category]) >= MAX_PER_CATEGORY:
            print(f"Skipping feed {url} (Quota full for {category})")
            continue
            
        try:
            print(f"Fetching [{category}] {url}...")
            response = requests.get(url, headers=headers, timeout=10)
            
            if response.status_code != 200:
                print(f"Failed to fetch {url}: Status {response.status_code}")
                continue
                
            feed_data = feedparser.parse(response.content)
            
            if feed_data.bozo:
                print(f"XML Parse Warning for {url}: {feed_data.bozo_exception}")
            
            print(f"Successfully parsed {url}: Found {len(feed_data.entries)} entries.")
            
            for entry in feed_data.entries:
                # Re-check quota inside loop
                if len(category_buckets[category]) >= MAX_PER_CATEGORY:
                    break
                
                # Filter: Relevance Check (Skip non-Thai news)
                if not is_relevant_to_thailand(entry):
                    # print(f"Skipping irrelevant: {entry.title}") 
                    continue

                # Filter: Skip already processed
                if entry.link in processed_urls:
                    # print(f"Skipping already processed: {entry.title}")
                    continue

                if is_recent(entry):
                    # Robust Source Extraction
                    raw_src = feed_data.feed.get("title", url)
                    if not raw_src or str(raw_src).lower() == 'none' or str(raw_src).strip() == '':
                        raw_src = "[MISSING_SOURCE]"
                    
                    item = {
                        "title": entry.title,
                        "link": entry.link,
                        "published": entry.get("published", str(datetime.now())),
                        "summary": entry.get("summary", ""),
                        "source": raw_src,
                        "suggested_category": category, # Hint for AI or logic
                        "_raw_entry": entry
                    }
                    category_buckets[category].append(item)
                    
        except Exception as e:
            print(f"Error fetching {url}: {e}")

    # Interleave (Round-Robin) to create balanced list
    balanced_items = []
    max_items_per_cat = max(len(items) for items in category_buckets.values()) if category_buckets else 0
    
    categories = list(category_buckets.keys())
    
    for i in range(max_items_per_cat):
        for cat in categories:
            if i < len(category_buckets[cat]):
                balanced_items.append(category_buckets[cat][i])
                
    return balanced_items

# 2. Gemini Analysis
import re

def clean_html(raw_html):
    cleanr = re.compile('<.*?>')
    cleantext = re.sub(cleanr, '', raw_html)
    return cleantext


# --------------------------------------------------------------------------------
# Google News RSS Fetcher (Backup Source)
# --------------------------------------------------------------------------------
def fetch_google_news_rss(query="Thailand Tourism", period="24h"):
    """
    Fetches Google News RSS for a specific query.
    Returns: List of dicts matching news item structure.
    """
    import feedparser
    import urllib.parse
    import time
    import requests
    
    encoded_query = urllib.parse.quote(query)
    # hl=en-TH, gl=TH ensures Thailand focus
    # when:24h = Last 24 hours
    # scoring=n = Sort by Date (Newest first)
    rss_url = f"https://news.google.com/rss/search?q={encoded_query}+when:{period}&hl=en-TH&gl=TH&ceid=TH:en&scoring=n"
    
    print(f"Fetching Google News: {query}...")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'
    }
    
    try:
        # [FIX] Use requests with User-Agent to avoid 403/Blocking
        response = requests.get(rss_url, headers=headers, timeout=10)
        
        if response.status_code == 200:
            feed = feedparser.parse(response.content)
            items = []
            for entry in feed.entries:
                # [FIX] Robust Source Extraction for Google News
                raw_src = entry.get('source', {}).get('title')
                if not raw_src or str(raw_src).lower() == 'none' or str(raw_src).strip() == '':
                    raw_src = "[MISSING_SOURCE]"
                
                # Standardize to our News Item format
                item = {
                    'title': entry.title,
                    'link': entry.link,
                    'published': entry.get('published', ''),
                    'summary': entry.get('description', ''),
                    'source': raw_src,
                    '_raw_entry': entry # Keep for image extraction
                }
                items.append(item)
            print(f" -> Found {len(items)} items from Google News.")
            return items
        else:
            print(f"Google News Fetch Failed: Status {response.status_code}")
            return []
            
    except Exception as e:
        print(f"Google News Fetch Error: {e}")
        return []

# Helper: Fetch Full Content from URL
def fetch_full_content(url):
    """
    Scrapes the main text content from a news URL.
    Returns: String (text) or None
    """
    import requests
    from bs4 import BeautifulSoup
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'
        }
        # Timeout slightly longer for scraping
        response = requests.get(url, headers=headers, timeout=5)
# Reverted Google Cache Fallback
        
        if response.status_code != 200:
            return None
            
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Remove unwanted elements
        for script in soup(["script", "style", "nav", "footer", "header", "aside", "noscript"]):
            script.decompose()
            
        # Extract text from p tags (most reliable for news)
        paragraphs = soup.find_all('p')
        text = ' '.join([p.get_text() for p in paragraphs])
        
        # Clean up whitespace
        text = ' '.join(text.split())
        
        # Remove Google Cache Header Artifacts (if any)
        if "Google's cache of" in text:
             text = text.replace("This is Google's cache of", "")
        
        if len(text) < 100: # Too short, likely failed
            return None
            
        return text[:3000] # Limit to 3000 chars
        
    except Exception as e:
        # print(f"Error scraping {url}: {e}")
        return None

def analyze_news_with_gemini(news_items, api_key, existing_titles=None, current_time=None):
    if not news_items:
        return {}, "No news items to analyze."
        
    genai.configure(api_key=api_key)
    
    # Analyze ALL provided items
    limited_news_items = news_items[:10] 
    
    aggregated_topics = []
    total_items = len(limited_news_items)
    print(f"Starting sequential analysis for {total_items} items...")

    # Format existing titles for context
    existing_context = "\n".join([f"- {t}" for t in (existing_titles or [])[:15]])

    for idx, item in enumerate(limited_news_items):
        print(f"[{idx+1}/{total_items}] Processing: {item['title']}...")
        
        full_content = fetch_full_content(item['link'])
        if not full_content:
            full_content = clean_html(item['summary'])[:800]

        # New Context-Aware Prompt
        prompt = f"""
# Role
ë‹¹ì‹ ì€ íƒœêµ­ ë°©ì½•ì„ ì—¬í–‰í•˜ëŠ” í•œêµ­ì¸ ì—¬í–‰ìë¥¼ ìœ„í•œ 'ì‹¤ì‹œê°„ ë‰´ìŠ¤ íë ˆì´í„°'ì…ë‹ˆë‹¤.
í˜„ì¬ ì‹œê°ì€ {current_time or 'ì•Œ ìˆ˜ ì—†ìŒ'} ì´ë©°, ì•„ì¹¨/ì €ë… ë¸Œë¦¬í•‘ì„ ìœ„í•´ ë‰´ìŠ¤ë¥¼ ì„ ë³„ ì¤‘ì…ë‹ˆë‹¤.

# Task
ì…ë ¥ëœ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë¶„ì„í•˜ì—¬ ì—¬í–‰ìì—ê²Œ í•„ìš”í•œ ì •ë³´ë¥¼ ì„ ë³„í•˜ê³  ìš”ì•½í•˜ì„¸ìš”.
**[CRITICAL] ëª¨ë“  ì¶œë ¥ í…ìŠ¤íŠ¸(ì œëª©, ìš”ì•½, ê¸°ì‚¬ ì „ë¬¸ ë“±)ëŠ” ë°˜ë“œì‹œ í•œêµ­ì–´(Korean)ì—¬ì•¼ í•©ë‹ˆë‹¤.** íƒœêµ­ì–´ë‚˜ ì˜ì–´ë¡œ ë‚¨ê²¨ë‘ì§€ ë§ˆì„¸ìš”.
ì´ë•Œ, **'ê¸°ê³„ì ì¸ ì¤‘ë³µ'ê³¼ 'ì˜ë¯¸ ìˆëŠ” ì—…ë°ì´íŠ¸'ë¥¼ êµ¬ë¶„**í•˜ëŠ” ê²ƒì´ ê°€ì¥ ì¤‘ìš”í•©ë‹ˆë‹¤.

# Input Data
1. **Candidate News:** 
   - Title: {item['title']}
   - Source: {item['source']}
   - Content Snippet: {full_content[:1500]}
2. **Existing News (ìµœê·¼ 24ì‹œê°„ ë‚´ ì´ë¯¸ ê²Œì‹œëœ ê¸°ì‚¬ë“¤):**
{existing_context}

# ğŸ” Filtering & Scoring Logic (3-Step)

## Step 1: 'ì—…ë°ì´íŠ¸' ì—¬ë¶€ íŒë‹¨ (Context Check)
ê¸°ì¡´ ë‰´ìŠ¤(Existing News)ì™€ ì£¼ì œê°€ ë¹„ìŠ·í•˜ë”ë¼ë„, ì•„ë˜ ê²½ìš°ì—ëŠ” **'ìƒˆë¡œìš´ ë‰´ìŠ¤'**ë¡œ ì·¨ê¸‰í•˜ì„¸ìš”.
- **ì‹œê°„ ê²½ê³¼:** ì‚¬ê±´ì˜ ì§„í–‰ ìƒí™©ì´ ë³€í•œ ê²½ìš° (ì˜ˆ: ì‹œìœ„ ë°œìƒ -> ì‹œìœ„ í•´ì‚°, ì‚¬ê³  ë°œìƒ -> ì‚¬ìƒì ì§‘ê³„ ì™„ë£Œ)
- **ì¼ì¼ ë¸Œë¦¬í•‘:** ë‚ ì”¨, ë¯¸ì„¸ë¨¼ì§€(PM2.5), í™˜ìœ¨ ë“± ë§¤ì¼ ë³€í•˜ëŠ” ìˆ˜ì¹˜ëŠ” ì–´ì œì™€ ì œëª©ì´ ë¹„ìŠ·í•´ë„ **ì˜¤ëŠ˜ ë‚ ì§œ ë°ì´í„°ë¼ë©´ í•„ìˆ˜ ê²Œì‹œ(Score +3)**.
- **ì•„ì¹¨/ì €ë…:** 'Morning Briefing' ë˜ëŠ” 'Daily Update' ì„±ê²©ì˜ ê¸°ì‚¬ëŠ” ìš°ì„ ìˆœìœ„ë¥¼ ë†’ì„.

## Step 2: Scoring (1~10ì )
- **7~10ì  (í•„ìˆ˜):** ì—¬í–‰ê° ì•ˆì „ ìœ„í˜‘(ì‹œìœ„, í™ìˆ˜, ë²”ì£„), ë¹„ì/ì…êµ­ ê·œì • ë³€ê²½, ëŒ€í˜• ì¶•ì œ, ê³µí•­ í˜¼ì¡.
- **4~6ì  (ë³´í†µ):** ìƒˆë¡œìš´ í•«í”Œ, ì¼ë°˜ì ì¸ ë‚ ì”¨, ì†Œì†Œí•œ ê·œì œ, í¥ë¯¸ë¡œìš´ ë¡œì»¬ ë‰´ìŠ¤.
- **1~3ì  (ë¬´ì‹œ):** ë‹¨ìˆœ ì •ì¹˜ ì‹¸ì›€, ì—°ì˜ˆì¸ ê°€ì‹­, ì—¬í–‰ê³¼ ë¬´ê´€í•œ ë‰´ìŠ¤.

# Constraints
- ì´ë¯¸ ê²Œì‹œëœ ë‰´ìŠ¤ì™€ **ë‚´ìš©ì´ 100% ë™ì¼í•˜ë©´ ì œì™¸**í•˜ì„¸ìš”.
- í•˜ì§€ë§Œ **'ìƒí™©ì´ ì—…ë°ì´íŠ¸' ë˜ì—ˆë‹¤ë©´ ë°˜ë“œì‹œ í¬í•¨**í•˜ì„¸ìš”.
- ì•„ì¹¨ì—ëŠ” 'ì˜¤ëŠ˜ì˜ ì˜ˆë³´/ì˜ˆì •' ìœ„ì£¼, ì €ë…ì—ëŠ” 'ì˜¤ëŠ˜ ë°œìƒí•œ ì‚¬ê±´/ê²°ê³¼' ìœ„ì£¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ë‘ì„¸ìš”.
- **[CRITICAL] ì¶œì²˜ê°€ '[MISSING_SOURCE]'ì¸ ê¸°ì‚¬ëŠ” 'tourist_impact_score'ê°€ 8ì  ì´ìƒì¸ ê²½ìš°ì—ë§Œ ê²°ê³¼ì— í¬í•¨í•˜ì„¸ìš”.** 7ì  ì´í•˜ì¸ ì¼ë°˜ ê¸°ì‚¬ëŠ” ê³¼ê°íˆ ì œì™¸í•˜ì„¸ìš”.
- ë§Œì•½ ì¶œì²˜ê°€ '[MISSING_SOURCE]'ì¸ë° ì •ë³´ë¥¼ í¬í•¨í•˜ê¸°ë¡œ ê²°ì •í–ˆë‹¤ë©´, ì¶œë ¥ JSONì˜ `source` í•„ë“œì—ëŠ” "Google News" ë˜ëŠ” ê¸°ì‚¬ ë‚´ìš©ì—ì„œ ì¶”ë¡ ëœ ì‹¤ì œ ì–¸ë¡ ì‚¬ ì´ë¦„ì„ ì ìœ¼ì„¸ìš”. ì ˆëŒ€ "None"ì´ë‚˜ "[MISSING_SOURCE]"ë¼ê³  ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”.
- **[CRITICAL - CATEGORY] ì¹´í…Œê³ ë¦¬ëŠ” ë°˜ë“œì‹œ ë‹¤ìŒ 4ê°œ ì¤‘ í•˜ë‚˜ë§Œ ì‚¬ìš©í•˜ì„¸ìš”: 'POLITICS', 'BUSINESS', 'TRAVEL', 'LIFESTYLE'. ë‹¤ë¥¸ ë‹¨ì–´(ì˜ˆ: 'ì •ì¹˜/ì‚¬íšŒ', 'General', 'ê¸°íƒ€')ë¥¼ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.**
  - ë‚ ì”¨, êµí†µ, í™ìˆ˜, ê³µí•­, ë¹„ì â†’ TRAVEL
  - ì •ì¹˜, ì‚¬íšŒ, ì‚¬ê±´/ì‚¬ê³ , ë²”ì£„ â†’ POLITICS
  - ê²½ì œ, ê¸ˆìœµ, ë¹„ì¦ˆë‹ˆìŠ¤ â†’ BUSINESS
  - ë¬¸í™”, ì—”í„°í…Œì¸ë¨¼íŠ¸, K-Pop â†’ LIFESTYLE

# Output Format (JSON Only)
{{
  "topics": [
    {{
      "title": "ê¸°ì‚¬ ì œëª©",
      "summary": "í•µì‹¬ 3ì¤„ ìš”ì•½ (- ë¡œ ì‹œì‘)",
      "full_translated": "ê¸°ì‚¬ ì „ë¬¸ (Markdown)",
      "category": "POLITICS | BUSINESS | TRAVEL | LIFESTYLE ì¤‘ í•˜ë‚˜",
      "tourist_impact_score": 0,
      "impact_reason": "ì ìˆ˜ ë¶€ì—¬ ë° ì—…ë°ì´íŠ¸ íŒë‹¨ ê·¼ê±°",
      "event_info": {{
          "date": "YYYY-MM-DD",
          "location": "...", 
          "price": "...",
          "location_google_map_query": "..."
      }},
      "references": [
        {{"title": "{item['title']}", "url": "{item['link']}", "source": "{item['source']}"}}
      ]
    }}
  ]
}}
"""
        
        # Retry Logic with Safety Limits
        max_retries = 3
        retry_count = 0
        success = False
        
        while retry_count < max_retries and not success:
            try:
                model = genai.GenerativeModel('gemini-2.0-flash', generation_config={"response_mime_type": "application/json"})
                response = model.generate_content(prompt)
                # Force HTTPS for all URLs in the generated content (Markdown links, Image URLs, References)
                safe_text = response.text.replace("http://", "https://")
                result = json.loads(safe_text)
                
                if 'topics' in result and result['topics']:
                    # --- Python Post-Processing & Verification ---
                    filtered_topics = []
                    for topic in result['topics']:
                        # 0. Sanitize Source (Emergency fix if AI failed constraints)
                        for ref in topic.get('references', []):
                            src = str(ref.get('source', '')).strip()
                            if not src or src.lower() == 'none' or src == '[MISSING_SOURCE]':
                                ref['source'] = 'Google News'
                        
                        # 1. Strict Source Filtering Verification
                        is_missing_source = (item['source'] == '[MISSING_SOURCE]')
                        impact_score = topic.get('tourist_impact_score', 0)
                        
                        if is_missing_source and impact_score < 8:
                            print(f"   -> [Filtered] Skipping '{topic['title']}' (Missing source & Low score: {impact_score})")
                            continue
                            
                        # --- [NEW] Ingestion-Time Translation Safety ---
                        # If AI returned Thai, force manual translation before saving
                        for field in ['title', 'summary', 'full_translated']:
                            if field in topic and is_thai(topic[field]):
                                print(f"   -> [Safety] Missed translation in {field}, forcing manual translation...")
                                topic[field] = translate_text(topic[field])

                        # 2. Festival/Event Strict Mode
                        if topic.get('category') == 'ì¶•ì œ/ì´ë²¤íŠ¸':
                            evt = topic.get('event_info')
                            # Check strict conditions
                            if not evt or not evt.get('location') or not evt.get('date') or not evt.get('price'):
                                print(f"   -> [Strict Mode] Downgrading '{topic['title']}' from Event to Travel News (Missing Info)")
                                topic['category'] = 'TRAVEL'
                                topic['event_info'] = None # Clear it
                            elif evt.get('location') == 'Unknown' or evt.get('location') == 'null':
                                 print(f"   -> [Strict Mode] Downgrading '{topic['title']}' (Location Unknown)")
                                 topic['category'] = 'TRAVEL'
                                 topic['event_info'] = None
                        
                        # 3. Normalize Category (Fallback safety)
                        raw_cat = topic.get('category', '')
                        topic['category'] = normalize_category(raw_cat)
                        
                        filtered_topics.append(topic)

                    aggregated_topics.extend(filtered_topics)
                    print(f"   -> Success. Topics so far: {len(aggregated_topics)}")
                    success = True
                else:
                    raise ValueError("Empty topics in response")
                
            except Exception as e:
                retry_count += 1
                wait_time = 2 ** retry_count # Exponential backoff: 2s, 4s, 8s
                print(f"   -> API Error for item {idx+1} (Attempt {retry_count}/{max_retries}): {e}")
                
                if "429" in str(e):
                    print("   -> Rate Limit Hit. Waiting longer...")
                    time.sleep(60) # Special wait for Rate Limit
                elif retry_count < max_retries:
                    print(f"   -> Retrying in {wait_time}s...")
                    time.sleep(wait_time)
                else:
                    print("   -> Max retries reached. Skipping this item.")
            
        # Delay logic (except for the last one)
        if idx < total_items - 1:
            print("   -> Waiting 20 seconds to respect API rate limits...")
            time.sleep(20)

    return {"topics": aggregated_topics}, None


def load_local_json(file_path):
    if os.path.exists(file_path):
        with open(file_path, 'r', encoding='utf-8') as f:
            try:
                return json.load(f)
            except:
                return {}
    return {}


# 3. Exchange Rate (THB -> KRW)
@st.cache_data(ttl=1800, show_spinner=False)  # Cache for 30 mins
def get_thb_krw_rate():
    """
    Fetches the current THB to KRW exchange rate.
    Uses 'data/exchange_rate.json' for persistence.
    """
    RATE_FILE = 'data/exchange_rate.json'
    url = "https://api.frankfurter.app/latest?from=THB&to=KRW"
    
    # helper to save
    def save_rate(rate):
        try:
            with open(RATE_FILE, 'w', encoding='utf-8') as f:
                json.dump({"rate": rate, "updated_at": str(datetime.now())}, f)
        except: pass

    # helper to load
    def load_cached_rate():
        if os.path.exists(RATE_FILE):
            try:
                with open(RATE_FILE, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return data.get("rate")
            except: pass
        return None

    try:
        # Increased timeout to 15s to prevent frequent timeouts
        import requests
        response = requests.get(url, timeout=15)
        if response.status_code == 200:
            data = response.json()
            rate = data.get('rates', {}).get('KRW')
            if rate:
                save_rate(rate)
                return rate
    except Exception as e:
        print(f"Exchange Rate Error: {e}")
    
    # Fallback to cached rate if live fetch fails
    cached = load_cached_rate()
    if cached:
        return cached
        
    # If absolutely no data (first run ever & fail), return None or handled by UI
    return 0.0

@st.cache_data(ttl=1800, show_spinner=False)  # Cache for 30 mins
def get_usd_thb_rate():
    """
    Fetches the current USD to THB exchange rate.
    Uses 'data/exchange_rate_usd.json' for persistence.
    """
    RATE_FILE = 'data/exchange_rate_usd.json'
    url = "https://api.frankfurter.app/latest?from=USD&to=THB"
    
    # helper to save
    def save_rate(rate):
        try:
            with open(RATE_FILE, 'w', encoding='utf-8') as f:
                json.dump({"rate": rate, "updated_at": str(datetime.now())}, f)
        except: pass

    # helper to load
    def load_cached_rate():
        if os.path.exists(RATE_FILE):
            try:
                with open(RATE_FILE, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return data.get("rate")
            except: pass
        return None

    try:
        import requests
        response = requests.get(url, timeout=15)
        if response.status_code == 200:
            data = response.json()
            rate = data.get('rates', {}).get('THB')
            if rate:
                save_rate(rate)
                return rate
    except Exception as e:
        print(f"USD Exchange Rate Error: {e}")
    
    # Fallback to cached rate if live fetch fails
    cached = load_cached_rate()
    if cached:
        return cached
        
    # Default fallback rate (approx USD/THB)
    return 34.5

# 4. Air Quality (WAQI)
@st.cache_data(ttl=1800, show_spinner=False)  # Cache for 30 mins
def get_air_quality(token):
    """
    Fetches real-time Air Quality (PM 2.5) for Bangkok.
    Returns:
        dict: {'aqi': int, 'status': str} or None if failed.
    """
    url = f"https://api.waqi.info/feed/bangkok/?token={token}"
    try:
        import requests
        response = requests.get(url, timeout=5)
        if response.status_code == 200:
            data = response.json()
            if data.get('status') == 'ok':
                aqi = data['data']['aqi']
                return {'aqi': aqi}
    except Exception as e:
        print(f"Air Quality Error: {e}")
    
    return None



def fetch_thai_events():
    """
    Fetches and parses event information from ThaiTicketMajor, BK Magazine, and TAT News using Gemini.
    Returns:
        list: A list of event dictionaries (title, date, location, region, image_url, link, type).
    """
    print("Fetching Thai Events (National)...")
    
    targets = [
        {
            "name": "ThaiTicketMajor",
            "url": "https://www.thaiticketmajor.com/concert/",
            "selector": "body"
        },
        {
            "name": "BK Magazine",
            "url": "https://bk.asia-city.com/things-to-do-bangkok",
            "selector": "div.view-content"
        },
        {
            "name": "TAT News",
            "url": "https://www.tatnews.org/category/events-festivals/",
            "selector": "body"
        }
    ]

    combined_html_context = ""

    for target in targets:
        try:
            print(f" - Requesting {target['name']}...")
            headers = {'User-Agent': 'Mozilla/5.0'}
            response = requests.get(target['url'], headers=headers, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Extract relevant part
                content = soup.select_one(target['selector'])
                if not content:
                     content = soup.body
                
                # Kill scripts
                for s in content(["script", "style", "nav", "footer", "header"]):
                    s.extract()
                
                html_snippet = str(content)[:20000] # Increased limit for TAT
                
                combined_html_context += f"\n\n--- Source: {target['name']} ({target['url']}) ---\n{html_snippet}"
                
        except Exception as e:
            print(f"Error fetching {target['name']}: {e}")

    if not combined_html_context:
        return []

    # Gemini Processing
    try:
        # Load API Key (Handle Env vs Secrets)
        api_key = os.environ.get("GEMINI_API_KEY")
        if not api_key:
             try:
                import toml
                secrets = toml.load(".streamlit/secrets.toml")
                api_key = secrets.get("GEMINI_API_KEY")
             except:
                pass
        
        if not api_key:
            return []

        import google.generativeai as genai
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.0-flash', generation_config={"response_mime_type": "application/json"})
        
        today_str = datetime.now().strftime('%Y-%m-%d')
        
        prompt = f"""
        You are a helpful event curator for Korean tourists visiting Thailand.
        Analyze the following HTML snippets from event websites (ThaiTicketMajor, BK Magazine, TAT News).
        Extract a list of distinct events/festivals across Thailand.
        
        Current Date: {today_str}
        
        CRITICAL: Identify the **REGION** (City/Province) based on the location info.
        - If "Chiang Mai" -> "ì¹˜ì•™ë§ˆì´"
        - If "Phuket" -> "í‘¸ì¼“"
        - If "Pattaya" -> "íŒŒíƒ€ì•¼"
        - If "Bangkok" -> "ë°©ì½•"
        - If "Koh Samui" -> "ì½”ì‚¬ë¬´ì´"
        - If unknown or miscellaneous, default to "ê¸°íƒ€" (Others) or "ë°©ì½•" if mostly likely Bangkok.
        
        Return the result ONLY as a JSON list of objects.
        
        JSON Format:
        [
            {{
                "title": "Event Name (Summarize in Korean, e.g. 'ì†¡í¬ë€ ì¶•ì œ')",
                "date": "YYYY-MM-DD or Date Range String (e.g. '2024-04-13 ~')",
                "location": "Venue Name (in Korean or English)",
                "region": "ë°©ì½•/ì¹˜ì•™ë§ˆì´/í‘¸ì¼“/íŒŒíƒ€ì•¼/ê¸°íƒ€",
                "image_url": "Full URL of the event poster/image",
                "link": "Full URL to booking page or article",
                "booking_date": "YYYY-MM-DD HH:MM (Ticket Open Time) or 'Now Open' or 'TBD'",
                "price": "Exact Price (e.g. '3,000 THB') or range",
                "type": "ì¶•ì œ" or "ì½˜ì„œíŠ¸" or "ì „ì‹œ" or "ê¸°íƒ€"
            }}
        ]

        Rules:
        1. Select 8-12 diverse items (Mix of Concerts, Festivals, Exhibitions).
        2. CRITICAL: EXCLUDE events that ended BEFORE {today_str}. Only show current or future events.
        3. CRITICAL: If you see a date from a past year (e.g. 2024 if today is 2026, or 2017, 2018...), IGNORE IT. Do not output old events.
        4. Prefer events happening soon (next 45 days).
        3. Ensure image_url is absolute.
        4. Output strictly JSON.
        
        HTML Context:
        {combined_html_context}
        """
        
        print(" - Sending to Gemini...")
        response = model.generate_content(prompt)
        text = response.text.strip()
        
        # Clean markdown
        if text.startswith("```json"):
            text = text[7:]
        if text.endswith("```"):
            text = text[:-3]
            
        data = json.loads(text)
        print(f" - Parsed {len(data)} events with Region info.")
        return data

    except Exception as e:
        print(f"Gemini processing error: {e}")
        return []

def extract_event_from_url(url, api_key):
    """
    Scrapes a URL and uses Gemini to extract event details.
    Returns a dict with processed event info.
    """
    try:
        # 1. Scrape Content
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
             'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7'
        }
        resp = requests.get(url, headers=headers, timeout=10)
        resp.raise_for_status()
        
        soup = BeautifulSoup(resp.content, 'html.parser')
        
        # Remove scripts/styles
        for script in soup(["script", "style", "nav", "footer", "header"]):
            script.decompose()
            
        text_content = soup.get_text(separator=' ', strip=True)[:15000] # Limit context
        
        # Try to find OG Image
        og_image = ""
        meta_img = soup.find("meta", property="og:image")
        if meta_img:
            og_image = meta_img.get("content", "")
            
        title_guess = soup.title.string if soup.title else ""

        # 2. Gemini Analysis
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.0-flash')
        
        prompt = f"""
        Analyze the following webpage text and extract event information.
        
        URL: {url}
        Page Title: {title_guess}
        Text Content:
        {text_content}
        
        Goal: Extract details for a "Big Match" event (Festival/Concert).
        
        Output JSON Format:
        {{
            "title": "Event Name (Korean, e.g. 'ë¡¤ë§ë¼ìš°ë“œ íƒœêµ­ 2024')",
            "date": "YYYY-MM-DD or Range (e.g. '2024-11-22 ~ 11-24')",
            "location": "Venue Name (Korean/English)",
            "region": "One of: ['ë°©ì½•', 'íŒŒíƒ€ì•¼', 'ì¹˜ì•™ë§ˆì´', 'í‘¸ì¼“', 'ê¸°íƒ€']",
            "type": "One of: ['ì¶•ì œ', 'ì½˜ì„œíŠ¸', 'ì „ì‹œ', 'í´ëŸ½/íŒŒí‹°', 'ê¸°íƒ€']",
            "booking_date": "Ticket Open Date (YYYY-MM-DD HH:MM) or 'Now Open'",
            "price": "Exact Price (e.g. '3,000 THB') or Range",
            "status": "One of: ['í‹°ì¼“ì˜¤í”ˆ', 'ê°œìµœí™•ì •', 'ë§¤ì§„', 'ì •ë³´ì—†ìŒ']",
            "image_url": "Use existing OG Image if valid, or find one in text. If none, return empty string.",
            "description": "1 line summary in Korean"
        }}
        
        If image_url is missing in text, use this one: {og_image}
        
        Translate all text to natural Korean.
        If information is missing, use "ì •ë³´ì—†ìŒ" or "" (empty string).
        """
        
        response = model.generate_content(prompt)
        text_response = response.text.strip()
        
        # Parse JSON
        if "```json" in text_response:
            text_response = text_response.replace("```json", "").replace("```", "")
        if text_response.startswith("```"): # Catch raw block
            text_response = text_response.replace("```", "")
        
        data = json.loads(text_response)
        data['link'] = url # Ensure link is set
        
        # Safety fallback for image
        if not data.get('image_url') and og_image:
            data['image_url'] = og_image
            
        return data, None
        
    except Exception as e:
        return None, str(e)

def fetch_big_events_by_keywords(keywords, api_key):
    """
    Crawls Google News RSS (Thailand Locale) for keywords and critically verifies details with Gemini.
    """
    import feedparser
    import urllib.parse
    
    found_events = []
    
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel('gemini-2.0-flash')

    for kw in keywords:
        print(f"Checking keyword: {kw}")
        encoded_kw = urllib.parse.quote(kw)
        # Use Thailand Locale (en-TH)
        rss_url = f"https://news.google.com/rss/search?q={encoded_kw}&hl=en-TH&gl=TH&ceid=TH:en"
        
        feed = feedparser.parse(rss_url)
        
        # Check top 2 entries (efficient)
        entries_to_check = feed.entries[:2]
        if not entries_to_check:
            continue
            
        # Aggregate text for analysis
        combined_text = f"Target Event: {kw}\n"
        for i, entry in enumerate(entries_to_check):
            combined_text += f"[{i+1}] Title: {entry.title}\nLink: {entry.link}\nSummary: {entry.get('summary','')}\nPubDate: {entry.get('published','')}\n\n"
            
        prompt = f"""
        Analyze these news search results for the event "{kw}" in Thailand.
        
        News Content:
        {combined_text}
        
        Goal: Determine if there is CONFIRMED information about the NEXT event date and venue.
        
        CRITICAL VALIDATION RULES:
        1. **CONFIRMED ONLY**: Do NOT extract if it's just a "rumor", "expected to be", "in talks", or from a past year.
        2. **Future Only**: Date must be in the future (2025-2027).
        3. **Specifics**: You must find BOTH a specific date (or confirmed month) AND a venue/city.
        
        If the event is NOT confirmed or is just a rumor:
        Return JSON: {{ "found": false, "reason": "Just a rumor or no data" }}

        If CONFIRMED:
        Return JSON:
        {{
            "found": true,
            "title": "Event Name (Korean)",
            "date": "YYYY-MM-DD or Range",
            "location": "Venue Name",
            "booking_date": "Ticket Open Date (YYYY-MM-DD HH:MM) or 'TBD'",
            "price": "Exact Price (e.g. '3,000 THB') or Range",
            "status": "ê°œìµœí™•ì •", 
            "link": "Best Link URL from the news",
            "description": "1 line confirmed summary in Korean"
        }}
        """
        
        try:
            response = model.generate_content(prompt)
            text = response.text.strip()
            if "```json" in text:
                text = text.replace("```json", "").replace("```", "")
            if text.startswith("```"):
                text = text.replace("```", "")
                
            data = json.loads(text)
            
            if data.get('found'):
                # Basic validation
                if '201' in data.get('date',''): 
                     pass
                else:
                    found_events.append(data)
            else:
                print(f" -> {kw}: Not confirmed ({data.get('reason')})")
                    
        except Exception as e:
            print(f"Error analyzing {kw}: {e}")
            
    return found_events

# --------------------------------------------------------------------------------
# Trend Hunter (Magazine) Logic - 4 Sources
# --------------------------------------------------------------------------------

@st.cache_data(ttl=3600, show_spinner=False)  # Cache for 60 mins
def fetch_trend_hunter_items(api_key, existing_links=None):
    """
    Aggregates trend/travel content via Google News RSS for 4 sources:
    1. Wongnai (Restaurants)
    2. TheSmartLocal TH (Hotspots)
    3. Chillpainai (Local Travel)
    4. BK Magazine (BKK Life)
    
    Returns:
        list: shuffled list of dicts {title, desc, location, image_url, link, badge}
    """
    import random
    import requests
    import feedparser
    
    print("Fetching Trend Hunter items via Google News RSS...")
    
    items = []
    if existing_links is None:
        existing_links = set()
    else:
        existing_links = set(existing_links)
        
    seen_links = set() # Local deduplication
    
    # Target Domains (Loaded from sources.json)
    SOURCES_FILE = 'data/sources.json'
    targets = []
    
    # 1. Try Loading from File
    if os.path.exists(SOURCES_FILE):
        try:
            with open(SOURCES_FILE, 'r', encoding='utf-8') as f:
                s_data = json.load(f)
                if s_data.get('magazine_targets'):
                    # Filter enabled only
                    targets = [t for t in s_data['magazine_targets'] if t.get('enabled', True)]
        except Exception as e:
            print(f"Error loading sources.json: {e}")
            
    # 2. Fallback if empty (Hardcoded defaults)
    if not targets:
        print("Using default magazine targets (Fallback).")
        targets = [
            {"name": "Wongnai", "domain": "wongnai.com", "tag": "[ë§›ì§‘ë­í‚¹]"},
            {"name": "Chillpainai", "domain": "chillpainai.com", "tag": "[ë¡œì»¬ì—¬í–‰]"},
            {"name": "BK Magazine", "domain": "bk.asia-city.com", "tag": "[ë°©ì½•ë¼ì´í”„]"},
            {"name": "The Smart Local", "domain": "thesmartlocal.co.th", "tag": "[MZí•«í”Œ]"}
        ]
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    # Helper: Gemini Analyzer
    def analyze_rss_items(raw_inputs, source_tag):
        if not raw_inputs: return []
        try:
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel('gemini-2.0-flash', generation_config={"response_mime_type": "application/json"})
            
            prompt = f"""
            You are a expert Korean Travel Editor acting as a **"Hotplace Detector"**.
            Your goal is to filter and rewrite RSS items into high-quality **Korean** magazine content.
            
            Input Data ({source_tag}):
            {json.dumps(raw_inputs, ensure_ascii=False)}
            
            **CRITICAL FILTERING RULES (Hotplace Detector)**:
            Analyze each item. If an item falls into any of these categories, **return null** for that item instead of a JSON object:
            1. **Not a specific visitable place**: General news, flight promos, "Thai Trip" general guides, or listicles without a clear focus.
            2. **Vague/Ad**: Content that sounds like a generic advertisement or lacks specific details.
            3. **No Image**: If you cannot infer a strong visual context or the input lacks an image.
            
            **REWRITE INSTRUCTIONS (For valid items)**:
            1. **LANGUAGE**: Natural, witty, trendy **Korean**.
            2. **INFERENCE**: Infer details (Vibe, Menu, Tips) from context.
            3. **FIELDS**:
               - "catchy_headline": Click-bait style 1-liner in Korean.
               - "desc": 2-3 sentences summary (Focus on why it's hot).
               - "location": Infer Area (e.g. 'Thong Lor', 'Siam').
               - "badge": Use "{source_tag}"
            
            Return JSON List of objects (excluding nulls).
            Example:
            [
                {{
                    "catchy_headline": "ë°©ì½• í†µë¡œì˜ ìˆ¨ê²¨ì§„ ë³´ì„, ì´êµ­ì ì¸ ë¶„ìœ„ê¸°ì˜ ë£¨í”„íƒ‘ ë°”!",
                    "desc": "í†µë¡œì˜ ì•¼ê²½ì„ í•œëˆˆì— ë‹´ì„ ìˆ˜ ìˆëŠ” ì´ ë£¨í”„íƒ‘ ë°”ëŠ” ë…íŠ¹í•œ ì¹µí…Œì¼ê³¼ ë¼ì´ë¸Œ ìŒì•…ìœ¼ë¡œ ì™„ë²½í•œ ë°¤ì„ ì„ ì‚¬í•©ë‹ˆë‹¤. ì¹œêµ¬ë“¤ê³¼ íŠ¹ë³„í•œ ì¶”ì–µì„ ë§Œë“¤ê³  ì‹¶ë‹¤ë©´ ì´ê³³ì„ ë°©ë¬¸í•´ë³´ì„¸ìš”.",
                    "location": "Thong Lor"
                }},
                null,
                {{
                    "catchy_headline": "ì§œëšœì§ ì‹œì¥ ê·¼ì²˜, í˜„ì§€ì¸ë§Œ ì•„ëŠ” ê°€ì„±ë¹„ ë§›ì§‘ ë°œê²¬!",
                    "desc": "ì£¼ë§ ì‹œì¥ êµ¬ê²½ í›„ í—ˆê¸°ì§„ ë°°ë¥¼ ì±„ìš°ê¸° ì¢‹ì€ ê³³. ì‹ ì„ í•œ í•´ì‚°ë¬¼ ìš”ë¦¬ì™€ íƒœêµ­ ì „í†µ ìŒì‹ì„ ì €ë ´í•œ ê°€ê²©ì— ì¦ê¸¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì›¨ì´íŒ…ì€ í•„ìˆ˜!",
                    "location": "Chatuchak"
                }}
            ]
            """
            
            response = model.generate_content(prompt)
            data = json.loads(response.text.strip().replace("```json", "").replace("```", ""))
            
            processed = []
            for res in data:
                if not res: continue # Skip null items (filtered)
                
                idx = res.get('original_index')
                if idx is not None and idx < len(raw_inputs):
                    original = raw_inputs[idx]
                    res['image_url'] = original.get('raw_img') 
                    res['link'] = original.get('raw_link')
                    res['badge'] = source_tag
                    processed.append(res)
            return processed
        except Exception as e:
            print(f"Analysis Error ({source_tag}): {e}")
            return []

    # Main Loop
    for target in targets:
        try:
            # Google News RSS URL (Reduced restriction)
            rss_url = f"https://news.google.com/rss/search?q=site:{target['domain']}&hl=en-TH&gl=TH&ceid=TH:en"
            print(f"Reading RSS: {target['name']}...")
            
            resp = requests.get(rss_url, headers=headers, timeout=10)
            feed = feedparser.parse(resp.content)
            
            raw_items = []
            # Check up to 10 entries to find 2 valid ones
            for entry in feed.entries[:10]:
                if len(raw_items) >= 2: break
                
                # 1. Deduplication (Link & Title)
                if entry.link in existing_links or entry.link in seen_links:
                    print(f"Skipping duplicate: {entry.title}")
                    continue
                
                # 2. Chillpainai Filter
                if target['name'] == "Chillpainai" and "Thai Trip" in entry.title:
                    print(f"Skipping Chillpainai 'Thai Trip': {entry.title}")
                    continue

                seen_links.add(entry.link)
                
                # Attempt to find image
                img_src = ""
                if 'media_content' in entry:
                    img_src = entry.media_content[0]['url']
                elif 'description' in entry:
                     import re
                     match = re.search(r'src="([^"]+)"', entry.description)
                     if match: img_src = match.group(1)
                
                raw_items.append({
                    "raw_title": entry.title,
                    "raw_link": entry.link,
                    "raw_img": img_src,
                    "context": f"Latest article from {target['name']}"
                })
            
            if raw_items:
                analyzed = analyze_rss_items(raw_items, target['tag'])
                items.extend(analyzed)
                
        except Exception as e:
            print(f"Error fetching {target['name']}: {e}")

    # Shuffle for Magazine feel
    random.shuffle(items)
    return items

def push_changes_to_github(files_to_commit, commit_message):
    """
    Commits and pushes specified files to GitHub.
    Requires GITHUB_TOKEN in secrets.toml or environment.
    """
    import subprocess
    import toml
    
    # 1. Get Token
    token = os.environ.get("GITHUB_TOKEN")
    if not token:
        try:
            secrets = toml.load(".streamlit/secrets.toml")
            token = secrets.get("GITHUB_TOKEN")
        except: pass
    
    if not token:
        return False, "GITHUB_TOKEN not found in secrets."

    # 2. Configure Git (If needed)
    # Check if user is set
    try:
        subprocess.run("git config user.name", shell=True, check=True, capture_output=True)
    except:
        subprocess.run('git config user.email "auto-deploy@streamlit.app"', shell=True)
        subprocess.run('git config user.name "Streamlit Admin"', shell=True)

    try:
        # 3. Add Files
        for f in files_to_commit:
            subprocess.run(f"git add {f}", shell=True, check=True)
            
        # 4. Commit
        subprocess.run(f'git commit -m "{commit_message}"', shell=True, check=True)
        
        # 5. Push
        # Use token in URL for auth
        repo_url = subprocess.check_output("git remote get-url origin", shell=True, text=True).strip()
        
        if "https://" in repo_url:
            auth_url = repo_url.replace("https://", f"https://{token}@")
        else:
            auth_url = repo_url
            
        subprocess.run(f"git push {auth_url} HEAD:main", shell=True, check=True)
        
        return True, "Successfully pushed to GitHub!"
        
    except subprocess.CalledProcessError as e:
        return False, f"Git Error: {e}"
    except Exception as e:
        return False, f"Error: {e}"


# --------------------------------------------------------------------------------
# Visitor Counter (counterapi.dev)
# --------------------------------------------------------------------------------

# --------------------------------------------------------------------------------
# Visitor Counter (counterapi.dev)
# --------------------------------------------------------------------------------

def get_visitor_stats():
    """
    Fetches both Total and Daily visitor counts.
    Returns: (total_count, daily_count)
    """
    try:
        import requests
        from datetime import datetime
        
        namespace = "today-thailand-app"
        today_str = datetime.now().strftime("%Y-%m-%d")
        
        # Keys
        key_total = f"total"
        key_daily = f"date_{today_str}"
        
        # 1. Get Total
        total_val = 0
        try:
            url_total = f"https://api.counterapi.dev/v1/{namespace}/{key_total}"
            r1 = requests.get(url_total, timeout=2)
            if r1.status_code == 200:
                total_val = r1.json().get("count", 0)
        except: pass
        
        # 2. Get Daily
        daily_val = 0
        try:
            url_daily = f"https://api.counterapi.dev/v1/{namespace}/{key_daily}"
            r2 = requests.get(url_daily, timeout=2)
            if r2.status_code == 200:
                daily_val = r2.json().get("count", 0)
        except: pass
            
        return total_val, daily_val
        
    except:
        return 0, 0

def is_bot_user():
    """
    Detects if the current user is a bot based on User-Agent.
    Returns True if a bot is detected.
    """
    try:
        import streamlit as st
        ua = st.context.headers.get("User-Agent", "").lower()
        bot_keywords = [
            "googlebot", "bingbot", "yandexbot", "baiduspider", "slurp", 
            "duckduckbot", "ia_archiver", "facebot", "facebookexternalhit",
            "twitterbot", "rogerbot", "linkedinbot", "embedly", "quora link preview",
            "showyoubot", "outbrain", "pinterest/0.", "naverbot", "telegrambot",
            "whatsapp", "viber", "skypeuri", "health check"
        ]
        return any(bot in ua for bot in bot_keywords)
    except:
        return False

def increment_visitor_stats():
    """
    Increments both Total and Daily counts (once per session).
    Returns: (new_total, new_daily)
    """
    try:
        import requests
        from datetime import datetime
        
        # [NEW] Bot Filtering: skip increment if bot
        is_bot = is_bot_user()
        
        namespace = "today-thailand-app"
        today_str = datetime.now().strftime("%Y-%m-%d")
        
        # Keys
        key_total = f"total"
        key_daily = f"date_{today_str}"
        
        # 1. Hit Total
        total_val = 0
        try:
            url_total = f"https://api.counterapi.dev/v1/{namespace}/{key_total}/"
            # Append 'up' only for humans
            url_total += "up" if not is_bot else "get"
            r1 = requests.get(url_total, timeout=2)
            if r1.status_code == 200:
                total_val = r1.json().get("count", 0)
        except: pass
        
        # 2. Hit Daily
        daily_val = 0
        try:
            url_daily = f"https://api.counterapi.dev/v1/{namespace}/{key_daily}/"
            # Append 'up' only for humans
            url_daily += "up" if not is_bot else "get"
            r2 = requests.get(url_daily, timeout=2)
            if r2.status_code == 200:
                daily_val = r2.json().get("count", 0)
        except: pass
        
        return total_val, daily_val
        
    except:
        return 0, 0
        return 0, 0

# --------------------------------------------------------------------------------
# Twitter Trend Analyzer (trends24.in + Gemini)
# --------------------------------------------------------------------------------
@st.cache_data(ttl=1800, show_spinner=False)  # Cache for 30 mins
def fetch_twitter_trends(api_key):
    """
    Scrapes trends24.in/thailand/ for top 10 hashtags and analyzes them with Gemini.
    Returns: dict { "topic": "...", "reason": "...", "severity": "info" } or None
    """
    import requests
    from bs4 import BeautifulSoup
    import google.generativeai as genai
    import json
    
    url = "https://trends24.in/thailand/"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        print("Fetching Twitter Trends from trends24.in...")
        resp = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(resp.content, 'html.parser')
        
        # trends24 structure: .trend-card__list (first one is latest) -> li -> a
        trend_list = soup.select('.trend-card__list')
        
        if not trend_list:
            print("No trend list found.")
            return None
            
        # Get top 10 from the most recent hour (first list)
        top_trends = []
        for li in trend_list[0].find_all('li')[:10]:
            text = li.get_text(strip=True)
            # Pre-filter: Explicitly skip "Q+number" + "shooting" patterns (Drama schedule)
            import re
            if re.search(r'Q\d+', text, re.IGNORECASE) and re.search(r'shooting', text, re.IGNORECASE):
                print(f"Skipping Drama Shooting Trend: {text}")
                continue
            top_trends.append(text)
            
        if not top_trends:
            print("No valid trends after filtering.")
            return None
            
        print(f"Top 10 Trends (Filtered): {top_trends}")
        
        # Analyze with Gemini
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.0-flash')
        
        prompt = f"""
        # Role
        You are a 'Security and Safety Analyst' for Bangkok, Thailand.

        # Context
        Thai Twitter trends are 80% dominated by **BL dramas (Y-Series), celebrity fandoms, and TV shows**.
        Words like 'Shooting', 'Attack', 'Fire' often appear but are **NOT real disasters** - they are drama titles or plot descriptions.
        Specifically, 'Q1 Shooting', 'Q2 Shooting' refers to "Queue" (Filming session) schedules, NOT gun violence.

        # Task
        Analyze these real-time Thailand Twitter trends:
        {json.dumps(top_trends, ensure_ascii=False)}
        
        Strictly determine if any trend represents a **'Real-world Physical Danger'** vs **'Media Content'**.

        # Critical Rules (MUST FOLLOW)
        1. **Drama/Movie Check:** If hashtag contains 'EP', 'Series', 'TheMovie', 'OnAir', 'Q1'~'Q99' with 'Shooting', actor names, or looks like a show title â†’ **ALWAYS return null**.
        2. **Cross-Verification:** For "Shooting" or "Fire" to be valid, there MUST be a **specific location name** (Siam Paragon, Central World, etc.) or **clear situation description**.
        3. **Default to Ignore:** If unsure whether it's a real event or drama â†’ **return null**. False alarms are MORE dangerous than missing info.
        4. **Still Useful:** K-Pop arrivals (airport crowds), Protests with location, Severe Weather with location â†’ valid.

        # Few-shot Examples
        - "#TheFireQ8Shooting" â†’ null (Reason: Q8 is episode/queue code for drama filming)
        - "#StarQ1Shooting" â†’ null (Reason: Drama filming schedule)
        - "#SiamParagonShooting" â†’ {{"severity": "warning", "reason": "ì‹œì•” íŒŒë¼ê³¤ ì‡¼í•‘ëª°ì—ì„œ ì´ê²© ì‹ ê³ ê°€ ì ‘ìˆ˜ë¨"}}
        - "#BrightWinEP10" â†’ null (BL drama episode)
        - "#à¸¡à¹‡à¸­à¸šà¸£à¸²à¸Šà¸›à¸£à¸°à¸ªà¸‡à¸„à¹Œ" â†’ {{"severity": "warning", "reason": "ë¼ì°¨ì˜ë¼ì†¡ ì‚¬ê±°ë¦¬ì—ì„œ ì‹œìœ„ ì¤‘, êµí†µ í˜¼ì¡ ì˜ˆìƒ"}}

        # Output Format (JSON)
        Return ONLY one of these:
        - null (if nothing relevant or uncertain)
        - {{"topic": "Keyword", "reason": "1 sentence in KOREAN for tourist", "severity": "warning" or "info"}}
        """
        
        response = model.generate_content(prompt)
        result_text = response.text.strip().replace("```json", "").replace("```", "")
        
        # specific handling for null
        if "null" in result_text.lower() and len(result_text) < 10:
             return None
             
        data = json.loads(result_text)
        
        # Add Collection Time (Bangkok Time)
        import pytz
        from datetime import datetime
        bkk = pytz.timezone('Asia/Bangkok')
        now_bkk = datetime.now(bkk)
        
        data['collected_at'] = now_bkk.strftime("%Y-%m-%d %H:%M:%S")
        
        return data


    except Exception as e:
        print(f"Twitter Trend Error: {e}")
        return None

# --------------------------------------------------------
# Hotel Fact Check Features
# --------------------------------------------------------
import streamlit as st # Added for user requested st.error/st.warning

def fetch_hotel_candidates(hotel_name, city, api_key):
    """
    Step 1: Search for potential hotels (Candidates).
    Returns: List of dicts [{'id':..., 'name':..., 'address':...}] or None
    """
    # 1. Query Expansion (Removed forced 'Hotel' suffix)
    # Why? 'Centara' + 'Hotel' -> strictly matches 'Centara Hotel' (budget branch),
    # obscuring 'Centara Grand Mirage' (Resort).
    # Google Places TextSearch handles "Brand in City" better without forced suffixes.
    
    hotel_name = hotel_name.strip()
    
    # 2. Construct Query
    # Detect Korean to optimize query structure
    import re
    is_korean = bool(re.search(r'[ê°€-í£]', hotel_name))
    
    if is_korean:
         # 2-1. Brand Mapping (Korean -> English) for higher accuracy
         # Google Maps works significantly better with English brand names.
         brand_map = {
             "ì„¼íƒ€ë¼": "Centara",
             "ì•„ë§ˆë¦¬": "Amari",
             "ííŠ¼": "Hilton",
             "í•˜ì–íŠ¸": "Hyatt",
             "ë©”ë¦¬ì–´íŠ¸": "Marriott",
             "ì‰ë¼í†¤": "Sheraton",
             "í™€ë¦¬ë°ì´ì¸": "Holiday Inn",
             "ì•„ë‚œíƒ€ë¼": "Anantara",
             "ì•„ë°”ë‹ˆ": "Avani",
             "ë‘ì§“íƒ€ë‹ˆ": "Dusit Thani",
             "ë…¸ë³´í…”": "Novotel",
             "ë¥´ë©”ë¥´ë””ì•™": "Le Meridien",
             "ì†Œí”¼í…”": "Sofitel",
             "í’€ë§Œ": "Pullman",
             "ì¸í„°ì»¨í‹°ë„¨íƒˆ": "InterContinental",
             "ë°˜ì–€íŠ¸ë¦¬": "Banyan Tree",
             "ìƒ¹ê·¸ë¦´ë¼": "Shangri-La",
             "ì¼í•€ìŠ¤í‚¤": "Kempinski",
             "ì¹´í ë¼": "Capella",
             "í¬ì‹œì¦ŒìŠ¤": "Four Seasons",
             "ì„¸ì¸íŠ¸ë ˆì§€ìŠ¤": "St. Regis",
             "ë”ìŠ¤íƒ ë‹¤ë“œ": "The Standard"
         }
         
         # Check if hotel_name starts with or contains a known brand
         english_brand = None
         for kr_brand, en_brand in brand_map.items():
            if kr_brand in hotel_name:
                # Replace Korean Brand with English Brand in the query
                # e.g. "ì„¼íƒ€ë¼" -> "Centara"
                # e.g. "ì„¼íƒ€ë¼ ê·¸ëœë“œ" -> "Centara ê·¸ëœë“œ" (Mixed is fine, but pure English is best)
                # Let's just switch to English mode if it's a pure brand query
                if hotel_name.strip() == kr_brand:
                    hotel_name = en_brand
                    is_korean = False # Switch to English Logic
                else:
                    # Mixed case: "ì„¼íƒ€ë¼ ë¦¬ì¡°íŠ¸" -> replace 'ì„¼íƒ€ë¼' with 'Centara'
                    hotel_name = hotel_name.replace(kr_brand, en_brand)
                    # Keep is_korean = True for now unless we are sure, 
                    # but actually "Centara ë¦¬ì¡°íŠ¸" is better searched as "Centara Resort" (English logic handles mixed okay?)
                    # Let's try to trust the English Logic if we have English Name now.
                    # Actually better to treat as English-ish if we injected English Brand.
                    pass 
                break
    
    if is_korean:
         # Korean Fallback: Revert to Broad Search (No 'Hotel Resort' force)
         # 'Hotel Resort' keyword excluded pure Hotels (e.g. Centara Nova).
         # Broad search 'Name City Thailand' is safest for unmapped brands.
         search_query = f"{hotel_name} {city} Thailand"
    else:
         # English (or Mapped English): Use 'in' logic
         search_query = f"{hotel_name} in {city}, Thailand"
    
    url = "https://places.googleapis.com/v1/places:searchText"
    headers = {
        "Content-Type": "application/json",
        "X-Goog-Api-Key": api_key,
        "X-Goog-FieldMask": "places.id,places.displayName,places.formattedAddress"
    }
    # Limit to 10 candidates
    payload = {
        "textQuery": search_query,
        "maxResultCount": 20
    }
    
    try:
        response = requests.post(url, json=payload, headers=headers)
        if response.status_code != 200:
            # st.error(f"ğŸš¨ API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
            return None
            
        data = response.json()
        
        if not data.get("places"):
            return [] 
            
        # Extract meaningful candidates (Increased to 10)
        candidates = []
        for p in data["places"][:10]:
            candidates.append({
                "id": p["id"],
                "name": p.get("displayName", {}).get("text", "Unknown"),
                "address": p.get("formattedAddress", "")
            })
        return candidates

    except Exception as e:
        st.error(f"ì‹œìŠ¤í…œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def fetch_hotel_details(place_id, api_key):
    """
    Step 2: Fetch full details for a specific Place ID.
    Returns: place_dict or None
    """
    url = f"https://places.googleapis.com/v1/places/{place_id}"
    headers = {
        "X-Goog-Api-Key": api_key,
        "X-Goog-FieldMask": "id,displayName,formattedAddress,rating,userRatingCount,reviews,photos"
    }
    
    try:
        resp = requests.get(url, headers=headers)
        if resp.status_code != 200:
            st.error(f"ìƒì„¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {resp.text}")
            return None
            
        place = resp.json()
        
        # Photo handling - ë‹¨ì¼ ëŒ€í‘œ ì‚¬ì§„
        photo_url = None
        # Photo gallery - ìµœëŒ€ 10ì¥
        photo_urls = []
        
        if place.get("photos"):
            photos = place["photos"][:10]  # ìµœëŒ€ 10ì¥
            for photo in photos:
                photo_ref = photo.get("name")
                if photo_ref:
                    gallery_url = f"https://places.googleapis.com/v1/{photo_ref}/media?maxHeightPx=400&maxWidthPx=600&key={api_key}"
                    photo_urls.append(gallery_url)
            
            # ì²« ë²ˆì§¸ ì‚¬ì§„ì„ ëŒ€í‘œ ì‚¬ì§„ìœ¼ë¡œ
            if photos:
                photo_ref = photos[0]["name"]
                photo_url = f"https://places.googleapis.com/v1/{photo_ref}/media?maxHeightPx=800&maxWidthPx=800&key={api_key}"

        return {
            "name": place.get("displayName", {}).get("text", "Unknown"),
            "address": place.get("formattedAddress", ""),
            "rating": place.get("rating", 0.0),
            "review_count": place.get("userRatingCount", 0),
            "reviews": place.get("reviews", []),
            "photo_url": photo_url,
            "photo_urls": photo_urls  # ê°¤ëŸ¬ë¦¬ìš© ì‚¬ì§„ ë¦¬ìŠ¤íŠ¸
        }
    except Exception as e:
        st.error(f"ìƒì„¸ ì •ë³´ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return None

def analyze_hotel_reviews(hotel_name, rating, reviews, api_key, language="Korean"):
    """
    Analyze hotel reviews using Gemini with a specific 'Cold Inspector' persona.
    (Supports English and Korean)
    """
    is_english = (language == "English")
    try:
        # 1. Prepare Review Text
        reviews_text = ""
        for r in reviews[:5]: # Use top 5 reviews
             text = r.get("text", {}).get("text", "")
             if text:
                 reviews_text += f"- {text}\n"

        # 2. Gemini Prompt
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.0-flash', generation_config={"response_mime_type": "application/json"})

        if is_english:
            lang_instruction = "IMPORTANT: ALL JSON OUTPUT VALUES MUST BE IN ENGLISH."
            persona = "You are a 'Cold Hotel Inspector'. Provide a blunt, factual analysis of the hotel based on facts and data, avoiding marketing fluff."
            cons_instruction = "List only real issues like noise, dirt, bad breakfast, far location. If none, write: 'No significant drawbacks found. (Overall excellent evaluation)'"
        else:
            lang_instruction = "ì¤‘ìš”: ëª¨ë“  JSON ì¶œë ¥ê°’ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”."
            persona = "ë„ˆëŠ” 'ëƒ‰ì² í•œ í˜¸í…” ê²€ì¦ê°€'ì•¼. ì‚¬ìš©ìê°€ ì´ í˜¸í…”ì„ **'ì‹¤ì œë¡œ ì˜ˆì•½í• ì§€ ë§ì§€'** ê²°ì •í•  ìˆ˜ ìˆë„ë¡, ê´‘ê³  ë©˜íŠ¸ëŠ” ë¹¼ê³  ì˜¤ì§ **íŒ©íŠ¸ì™€ ì‹¤ì œ í›„ê¸°**ì— ê¸°ë°˜í•´ì„œ ë¶„ì„í•´ì¤˜."
            cons_instruction = "ëª…í™•í•˜ê²Œ ì§€ì ëœ ë¶€ì •ì  í‚¤ì›Œë“œê°€ ìˆì„ ë•Œë§Œ ì ì–´. ë‹¨ì ì´ í•˜ë‚˜ë„ ì—†ë‹¤ë©´: 'íŠ¹ë³„í•œ ë‹¨ì ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (ì „ë°˜ì ìœ¼ë¡œ ìš°ìˆ˜í•œ í‰ê°€)'ë¼ê³  ì ì–´."

        prompt = f"""
        {persona}
        {lang_instruction}

        **[Information]**
        * Hotel: {hotel_name} (Rating: {rating})
        * Recent Reviews: {reviews_text}
        * **Augment:** Use your internal knowledge about {hotel_name}'s location, brand, breakfast, and pool.

        **[Rules for Cons]**
        1. Don't say 'No information about X'.
        2. {cons_instruction}

        **[Not Recommended Guide]**
        Must be specific to Price, Noise, Location, or Mood. (e.g., 'Budget travelers seeking value' or 'Guests who prefer walking to BTS')

        **[Output Format (JSON)]**
        {{
            "name_eng": "Official English name (e.g. Centara Grand at CentralWorld)",
            "trip_keyword": "Korean keyword for Trip.com search (city omitted, e.g. ì•„ë§ˆë¦¬ ì›Œí…”ê²Œì´íŠ¸)",
            "price_level": "ğŸ’° step (1~4)",
            "price_range_text": "Price range in KRW (e.g. ì•½ 120,000ì› ~ 180,000ì›)",
            "one_line_verdict": "string",
            "recommendation_target": "string",
            "location_analysis": "string",
            "room_condition": "string",
            "service_breakfast": "string",
            "pool_facilities": "string",
            "pros": ["string", "string", "string"],
            "cons": ["string", "string", "string"],
            "summary_score": {{
                "cleanliness": 0, "location": 0, "comfort": 0, "value": 0
            }}
        }}
        """
        
        response = model.generate_content(prompt)
        return json.loads(response.text)

    except Exception as e:
        return {"error": str(e)}

# --------------------------------------------------------------------------------
# Infographic Generator (PIL based)
# --------------------------------------------------------------------------------
def ensure_font_loaded():
    """
    Downloads NanumGothic font if not present.
    Returns path to font file.
    """
    FONT_URL = "https://github.com/google/fonts/raw/main/ofl/nanumgothic/NanumGothic-Bold.ttf"
    FONT_PATH = "data/NanumGothic-Bold.ttf"
    
    if not os.path.exists(FONT_PATH):
        try:
            print("Downloading font for Infographic...")
            import requests
            r = requests.get(FONT_URL, timeout=10)
            with open(FONT_PATH, 'wb') as f:
                f.write(r.content)
            print("Font downloaded.")
        except Exception as e:
            print(f"Font download failed: {e}")
            return None # Fallback to default
            
    return FONT_PATH

def prettify_infographic_text(category, items, api_key):
    """
    Uses Gemini to shorten news into 'Emoji + One-liner' format.
    """
    if not items: return []
    
    # Cost optimization: If API Key missing, just use titles
    if not api_key:
        return [f"ğŸ“° {item['title']}" for item in items[:3]]

    import google.generativeai as genai
    import json
    
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel('gemini-2.0-flash')
    
    # Simplified inputs
    inputs = "\n".join([f"- {item['title']}" for item in items[:3]])
    
    prompt = f"""
    Convert these 3 news headlines into a "Social Media Infographic" style (Korean).
    Category: {category}
    
    Input:
    {inputs}
    
    Goal: Return a JSON list of strings. Each string must start with a relevant Emoji and be very short (max 20 chars).
    Example: ["ğŸš¨ ì‹œì•” íŒŒë¼ê³¤ ì´ê²© ë°œìƒ", "â›ˆï¸ ë‚´ì¼ ë°©ì½• í™ìˆ˜ ì£¼ì˜", "ğŸ‰ ì†¡í¬ë€ ì¶•ì œ ì¼ì • ë°œí‘œ"]
    
    Output JSON: {{ "lines": ["...", "...", "..."] }}
    """
    
    try:
        resp = model.generate_content(prompt)
        text = resp.text.strip().replace("```json", "").replace("```", "")
        if text.startswith("```"): text = text.replace("```", "")
        data = json.loads(text)
        result = data.get("lines", [])
        if not result:
            raise ValueError("Empty lines from AI")
        return result
    except Exception as e:
        print(f"Infographic AI Error: {e}")
        # Fallback to simple titles
        return [f"ğŸ“° {item['title'][:18]}..." for item in items[:3]]

def generate_category_infographic(category, items, date_str, api_key):
    """
    Generates a social media image for a specific category.
    """
    try:
        from PIL import Image, ImageDraw, ImageFont
    except ImportError as e:
        import streamlit as st
        st.error(f"Pillow Library Missing: {e}")
        return None

    try:
        import os
        
        # 1. Config Map (Color & Text)
        # Categories: "ì •ì¹˜/ì‚¬íšŒ", "ê²½ì œ", "ì—¬í–‰/ê´€ê´‘", "ì‚¬ê±´/ì‚¬ê³ ", "ì¶•ì œ/ì´ë²¤íŠ¸", "ê¸°íƒ€"
        theme_map = {
            "ì •ì¹˜/ì‚¬íšŒ": {"color": (59, 130, 246), "bg_file": "assets/bg_politics.png", "title": "POLITICS & SOCIAL"}, # Blue
            "ê²½ì œ": {"color": (34, 197, 94), "bg_file": "assets/bg_economy.png", "title": "ECONOMY"}, # Green
            "ì—¬í–‰/ê´€ê´‘": {"color": (249, 115, 22), "bg_file": "assets/bg_travel.png", "title": "TRAVEL NEWS"}, # Orange
            "ì‚¬ê±´/ì‚¬ê³ ": {"color": (239, 68, 68), "bg_file": "assets/bg_safety.png", "title": "SAFETY ALERT"}, # Red
            "ì¶•ì œ/ì´ë²¤íŠ¸": {"color": (236, 72, 153), "bg_file": "assets/bg_travel.png", "title": "THAI EVENTS"}, # Pink
            "ê¸°íƒ€": {"color": (107, 114, 128), "bg_file": "assets/template.png", "title": "DAILY NEWS"} # Gray
        }
        
        theme = theme_map.get(category, theme_map["ê¸°íƒ€"])
        
        # 2. Get AI Content
        lines = prettify_infographic_text(category, items, api_key)
        if not lines: return None

        # 3. Setup Canvas (1080x1080 Square for Instagram)
        # 3. Setup Canvas (1080x1080 Square for Instagram)
        W, H = 1080, 1080
        
        # Try Dynamic Background (Use Image from first news item)
        bg_img = None
        
        # Find first item with valid image
        target_img_url = None
        for item in items:
            if item.get("image_url") and item["image_url"].startswith("http"):
                target_img_url = item["image_url"]
                break
                
        if target_img_url:
            try:
                import requests
                from io import BytesIO
                
                # Download Image
                # print(f"Downloading BG: {target_img_url}")
                resp = requests.get(target_img_url, timeout=5)
                if resp.status_code == 200:
                    raw_img = Image.open(BytesIO(resp.content)).convert("RGB")
                    
                    # Resize & Center Crop to cover 1080x1080
                    # logic: Scale shortest side to 1080, then crop center
                    img_w, img_h = raw_img.size
                    ratio = max(W/img_w, H/img_h)
                    new_size = (int(img_w * ratio), int(img_h * ratio))
                    raw_img = raw_img.resize(new_size, Image.LANCZOS)
                    
                    # Center Crop
                    left = (new_size[0] - W)/2
                    top = (new_size[1] - H)/2
                    right = (new_size[0] + W)/2
                    bottom = (new_size[1] + H)/2
                    
                    bg_img = raw_img.crop((left, top, right, bottom))
                    
                    # Apply Dimming (Black Overlay 60%)
                    overlay = Image.new('RGBA', (W, H), (0, 0, 0, 150))
                    bg_img.paste(overlay, (0, 0), mask=overlay)
                    
            except Exception as e:
                # print(f"BG Image Error: {e}")
                bg_img = None

        # Fallback to Theme Background if Dynamic failed
        if not bg_img:
            if os.path.exists(theme['bg_file']):
                bg_img = Image.open(theme['bg_file']).convert("RGB")
                bg_img = bg_img.resize((W, H))
            else:
                # Create solid color background with gradient-ish look (simple solid for now)
                bg_img = Image.new('RGB', (W, H), theme['color'])
                # Add a subtle dark overlay for text contrast
                overlay = Image.new('RGBA', (W, H), (0,0,0, 50))
                bg_img.paste(overlay, (0,0), mask=overlay)
                
        img = bg_img
        
        draw = ImageDraw.Draw(img)
        
        # Fonts
        font_path = ensure_font_loaded()
        if not font_path:
            # Emergency fallback (might fail on korean)
            font_cat = ImageFont.load_default()
            font_date = ImageFont.load_default()
            font_body = ImageFont.load_default()
            font_footer = ImageFont.load_default()
        else:
            font_cat = ImageFont.truetype(font_path, 60)
            font_date = ImageFont.truetype(font_path, 40)
            font_body = ImageFont.truetype(font_path, 55)
            font_footer = ImageFont.truetype(font_path, 30)
            
        # Draw logic
        # Header: Category Title (English) + Date
        draw.text((80, 80), theme['title'], font=font_cat, fill="white")
        draw.text((80, 160), date_str, font=font_date, fill=(255, 255, 255, 200)) # Alpha 200
        
        # Divider
        draw.line((80, 230, 1000, 230), fill="white", width=4)
        
        # Body Content (Centered vertically-ish)
        start_y = 350
        gap = 120
        
        for i, line in enumerate(lines):
            # Draw badge/bullet?
            # Just text
            draw.text((80, start_y + (i * gap)), line, font=font_body, fill="white")
            
        # Footer
        draw.text((80, 1000), "ğŸ‡¹ğŸ‡­ ì˜¤ëŠ˜ì˜ íƒœêµ­ (Thai Briefing)", font=font_footer, fill=(255, 255, 255, 150))
        
        return img

    except Exception as e:
        import streamlit as st
        st.error(f"Infographic Error ({category}): {e}")
        return None
    
# --------------------------------------------------------------------------------
# Taxi Fare Calculator (Google Maps + Rush Hour Logic)
# --------------------------------------------------------------------------------
def get_route_estimates(origin, destination, api_key):
    """
    Get Distance & Duration using Google Routes API (Compute Routes v2).
    Replaces legacy Directions API.
    Returns: dist_km, dur_min, traffic_ratio, error_message
    """
    if not origin or not destination:
        return None, None, None, "ì¶œë°œì§€ì™€ ëª©ì ì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."
        
    endpoint = "https://routes.googleapis.com/directions/v2:computeRoutes"
    
    # Prepare Origin/Dest objects
    def build_wp(val):
        if val.startswith("place_id:"):
            return {"placeId": val.split(":")[1]}
        else:
            return {"address": val}
            
    payload = {
        "origin": build_wp(origin),
        "destination": build_wp(destination),
        "travelMode": "DRIVE",
        "routingPreference": "TRAFFIC_AWARE", # Important for traffic data
        "computeAlternativeRoutes": False,
        "languageCode": "ko-KR",
        "units": "METRIC"
    }
    
    headers = {
        "Content-Type": "application/json",
        "X-Goog-Api-Key": api_key,
        "X-Goog-FieldMask": "routes.distanceMeters,routes.duration,routes.staticDuration"
    }
    
    try:
        import requests
        resp = requests.post(endpoint, json=payload, headers=headers, timeout=10)
        data = resp.json()
        
        if resp.status_code == 200:
            if not data.get("routes"):
                return None, None, None, "ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                
            route = data["routes"][0]
            
            # Distance (meters)
            dist_km = route.get("distanceMeters", 0) / 1000
            
            # Helper to parse "123s" string format
            def parse_duration(dur_str):
                if not dur_str: return 0
                return int(dur_str.replace("s", ""))
                
            # Duration (Real-time with TRAFFIC_AWARE)
            real_dur_sec = parse_duration(route.get("duration", "0s"))
            # Static Duration (No traffic)
            base_dur_sec = parse_duration(route.get("staticDuration", "0s"))
            
            dur_min = real_dur_sec / 60
            
            # Traffic Ratio
            traffic_ratio = 1.0
            if base_dur_sec > 0:
                traffic_ratio = real_dur_sec / base_dur_sec
            
            return dist_km, dur_min, traffic_ratio, None
            
        else:
            # API Error
            err_details = data.get("error", {})
            msg = err_details.get("message", "Unknown Error")
            status = err_details.get("status", resp.status_code)
            return None, None, None, f"Routes API ì˜¤ë¥˜ ({status}): {msg}"
            
    except Exception as e:
        return None, None, None, f"ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}"

def calculate_expert_fare(dist_km, dur_min, origin_txt="", dest_txt=""):
    """
    Calculates fair prices for various transport modes in Bangkok.
    Now includes Rush Hour Logic & Hell Zone Detection.
    
    Args:
        origin_txt (str): Name/Address of origin (for Hell Zone checking)
        dest_txt (str): Name/Address of dest
    """
    from datetime import datetime, time
    import pytz
    
    # 1. Check Rush Hour (Bangkok Time)
    tz_bkk = pytz.timezone('Asia/Bangkok')
    now_bkk = datetime.now(tz_bkk)
    current_time = now_bkk.time()
    
    is_rush_hour = False
    morning_start = time(7, 0)
    morning_end = time(9, 30)
    evening_start = time(16, 30)
    evening_end = time(20, 0)
    
    if (morning_start <= current_time <= morning_end) or \
       (evening_start <= current_time <= evening_end):
        is_rush_hour = True
        
    # 2. Check Hell Zone (Traffic Hell)
    hell_zones = ["Asok", "Sukhumvit", "Siam", "Sathorn", "Silom", "Thong Lo", "Phrom Phong"]
    chk_str = (str(origin_txt) + " " + str(dest_txt)).lower()
    is_hell_zone = any(z.lower() in chk_str for z in hell_zones)

    # 3. Base Meter Calculation
    # Note: 'dur_min' already includes traffic delay if Routes API works correclty.
    # Adjusted: Reduced time weight (2.5 -> 2.25) to be more realistic with modern traffic apps
    base_meter = 35 + (dist_km * 7) + (dur_min * 2.25)
    base_meter = int(base_meter)
    
    # 4. Multipliers
    # Tuned down Rush Hour Multiplier (1.5 -> 1.25) based on user feedback
    rush_mult = 1.25 if is_rush_hour else 1.0
    tuktuk_rush_mult = 1.2 if is_rush_hour else 1.0
    
    # Hell Zone Surcharge (1.1x) if applicable
    hell_mult = 1.1 if is_hell_zone else 1.0
    
    # Final App Multiplier (Combined)
    total_app_mult = rush_mult * hell_mult

    # Calculate raw prices (Adjusted down based on user feedback)
    # Target: Meter x (1.2 ~ 1.6 including surge)
    bolt_basic_raw = int(base_meter * 0.85 * total_app_mult)
    bolt_std_raw = int(base_meter * 1.0 * total_app_mult)
    grab_raw = int(base_meter * 1.1 * total_app_mult)
    
    # Grab Range (+- 10%)
    grab_min = int(grab_raw * 0.9)
    grab_max = int(grab_raw * 1.1)

    # Bike Range (+- 10%)
    bike_raw = 25 + (dist_km * 8)
    bike_min = int(bike_raw * 0.9)
    bike_max = int(bike_raw * 1.1)

    fares = {
        "bolt": {
            "label": "âš¡ Bolt (í†µí•©)",
            "price": f"{bolt_basic_raw} ~ {bolt_std_raw}",
            "tag": "ì°¨ ì¡ê¸° í˜ë“¦" if not is_rush_hour else "ë§¤ìš° ë¹„ìŒˆ",
            "color": "green" # Merged color
        },
        "grab_taxi": {
            "label": "ğŸ’š Grab (Standard)",
            "price": f"{grab_min} ~ {grab_max}",
            "tag": "ì•ˆì „/ë¹ ë¦„" if not is_rush_hour else "ë§¤ìš° ë¹„ìŒˆ",
            "color": "blue"
        },
        "bike": {
            "label": "ğŸï¸ ì˜¤í† ë°”ì´ (Win)",
            "price": f"{bike_min} ~ {bike_max}",
            "tag": "ğŸš€ ê°€ì¥ ë¹ ë¦„",
            "color": "orange",
            "warning_text": "âš ï¸ ì‚¬ê³  ìœ„í—˜ ë†’ìŒ / í—¬ë©§ í•„ìˆ˜ / ë³´í—˜ í™•ì¸"
        },
        "tuktuk": {
            "label": "ğŸ›º ëšëš (TukTuk)",
            "tag": "í˜‘ìƒ í•„ìˆ˜",
            "color": "red",
            "warning": True
        }
    }
    
    # Calc TukTuk Range
    tt_min = int(base_meter * 1.5 * tuktuk_rush_mult) 
    tt_max = int(base_meter * 2.0 * tuktuk_rush_mult)
    fares['tuktuk']['price'] = f"{tt_min} ~ {tt_max}"
    
    # ---------------------------------------------------------
    # 5. Intercity / Long Distance Logic (Flat Rate)
    # ---------------------------------------------------------
    is_intercity = False
    intercity_tip = None
    
    # Check Keywords (Priority)
    dest_lower = str(dest_txt).lower()
    
    flat_rates = {
        "pattaya": {"range": (1100, 1400), "tip": "ğŸšŒ ì—ê¹Œë§ˆì´ í„°ë¯¸ë„ì—ì„œ ë²„ìŠ¤ íƒ€ë©´ ì•½ 131ë°”íŠ¸!"},
        "hua hin": {"range": (2000, 2400), "tip": "ğŸš† ê¸°ì°¨ë‚˜ ë¯¸ë‹ˆë°´ì„ ì´ìš©í•˜ë©´ 200~400ë°”íŠ¸!"},
        "ayutthaya": {"range": (900, 1200), "tip": "ğŸš† ê¸°ì°¨(20ë°”íŠ¸~)ë‚˜ ë¯¸ë‹ˆë°´ì„ ì¶”ì²œí•©ë‹ˆë‹¤!"},
        "suvarnabhumi": {"range": (400, 500), "tip": "ğŸš† ê³µí•­ì² ë„(ARL)ë¥¼ íƒ€ë©´ ì‹œë‚´ê¹Œì§€ 45ë°”íŠ¸ ë‚´ì™¸!"} # Airport special
    }
    
    matched_zone = None
    for key, data in flat_rates.items():
        if key in dest_lower:
            matched_zone = data
            is_intercity = True
            break
            
    # Generic Long Distance (> 60km)
    if not matched_zone and dist_km >= 60:
        is_intercity = True
        # Formula: 1200 + ((dist - 100) * 10)
        est_price = 1200 + ((dist_km - 100) * 10)
        est_min = int(est_price * 0.9)
        est_max = int(est_price * 1.1)
        
        matched_zone = {"range": (est_min, est_max), "tip": "ğŸšŒ ì¥ê±°ë¦¬ ì´ë™ì€ ë²„ìŠ¤/ê¸°ì°¨/ë¯¸ë‹ˆë°´ ì´ìš©ì„ ê³ ë ¤í•´ë³´ì„¸ìš”! (í›¨ì”¬ ì €ë ´í•¨)"}

    if is_intercity and matched_zone:
        r_min, r_max = matched_zone['range']
        price_str = f"{r_min} ~ {r_max}"
        intercity_tip = matched_zone['tip']
        
        # Override Fares
        fares['bolt']['price'] = price_str
        fares['grab_taxi']['price'] = price_str # Apps often follow market flat rates for long distance
        fares['tuktuk']['price'] = "ìš´í–‰ ë¶ˆê°€" # Tuktuk highly unlikely
        fares['bike']['price'] = "ì¶”ì²œ ì•ˆí•¨"
    
    return base_meter, fares, is_rush_hour, is_hell_zone, intercity_tip

def search_places(query, api_key):
    """
    Search using Google Places Autocomplete API for better partial matching.
    Returns: {name, address, place_id}
    """
    if not query: return []
    
    # Use Autocomplete API as requested into order to support 'Top 10' predictions and 'components' filtering
    endpoint = "https://maps.googleapis.com/maps/api/place/autocomplete/json"
    params = {
        "input": query,
        "key": api_key,
        "language": "ko",
        "components": "country:TH" # Strict Thailand restriction
    }
    
    try:
        import requests
        resp = requests.get(endpoint, params=params, timeout=5)
        data = resp.json()
        
        candidates = []
        if data.get('status') == 'OK':
            for p in data.get('predictions', [])[:10]:
                main_text = p.get('structured_formatting', {}).get('main_text', '')
                sec_text = p.get('structured_formatting', {}).get('secondary_text', '')
                full_text = p.get('description', '')
                
                candidates.append({
                    "name": main_text if main_text else full_text,
                    "address": sec_text,
                    "place_id": p.get('place_id')
                })
        return candidates
    except Exception as e:
        print(f"Autocomplete Error: {e}")
        return []

# --------------------------------------------------------------------------------
# Wongnai Restaurant Analyzer
# --------------------------------------------------------------------------------
def search_wongnai_restaurant(restaurant_name, api_key=None):
    """
    Search for a restaurant on Wongnai using Google search.
    Tries legacy search first, and always falls back to Gemini if it fails or returns nothing.
    """
    found_url = None
    
    # 1. Try legacy search (might be throttled or throw exceptions)
    queries = [
        f"site:wongnai.com {restaurant_name}",
        f"wongnai {restaurant_name}"
    ]
    
    try:
        for query in queries:
            results = googlesearch.search(query, num_results=3)
            for url in results:
                if "wongnai.com/restaurants/" in url or "wongnai.com/r/" in url:
                    found_url = url
                    break
            if found_url: break
    except Exception as e:
        print(f"Legacy search failed: {e}")
        pass # Ignore legacy errors and move to Gemini fallback
    
    if found_url:
        return found_url
    
    # 2. Strong Fallback: Gemini Search
    if api_key:
        try:
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel('gemini-1.5-flash')
            prompt = f"Find the Wongnai restaurant URL for: {restaurant_name}. Return ONLY the direct URL starting with https://www.wongnai.com/restaurants/ or https://www.wongnai.com/r/"
            response = model.generate_content(prompt)
            raw_text = response.text.strip()
            
            # Extract URL more robustly
            match = re.search(r'(https?://(?:www\.)?wongnai\.com/(?:restaurants|r)/[^\s]+)', raw_text)
            if match:
                return match.group(1).rstrip('.')
        except Exception as e:
            print(f"Gemini fallback search error: {e}")
            
    return None

def scrape_wongnai_restaurant(url):
    """
    Scrape restaurant data from a Wongnai URL.
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    try:
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code != 200:
            return {"error": f"í˜„ì‹œì  ì›¡ë‚˜ì´ ì ‘ì†ì´ ì›í™œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ (Code: {response.status_code})"}

        soup = BeautifulSoup(response.text, 'html.parser')
        
        # 1. Name (Wongnai uses dynamic classes sometimes, but h1 is fairly stable)
        name_tag = soup.find('h1')
        name = name_tag.get_text(strip=True) if name_tag else "Unknown Restaurant"
        
        # 2. Score
        # Typically in a span or div with specific class patterns
        score_tag = soup.find(string=re.compile(r'^\d\.\d$')) # Looks for "4.5" etc.
        score = score_tag.strip() if score_tag else "ë°ì´í„° ì—†ìŒ"
        
        # 3. Price
        price_tag = soup.find(string=re.compile(r'^[à¸¿]+$')) # Looks for "à¸¿à¸¿", "à¸¿à¸¿à¸¿"
        price = price_tag.strip() if price_tag else "ë°ì´í„° ì—†ìŒ"
        
        # 4. Photo
        # Find first large image
        photo_url = None
        img_tags = soup.find_all('img')
        for img in img_tags:
            src = img.get('src', '')
            if 'wongnai.com' in src and '/static2/' not in src: # Avoid icons/loaders
                photo_url = src
                break
        
        # 5. Reviews
        reviews = []
        # Wongnai reviews are often in complex structures
        # We try to grab text blocks that look like reviews
        review_texts = soup.find_all(['p', 'span', 'div'], string=re.compile(r'.{20,}'))
        count = 0
        for rt in review_texts:
            text = rt.get_text(strip=True)
            if len(text) > 40 and count < 10:
                reviews.append(text)
                count += 1
            
        return {
            "name": name,
            "score": score,
            "price": price,
            "photo_url": photo_url,
            "reviews": reviews,
            "url": url
        }
    except Exception as e:
        return {"error": f"ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {str(e)}"}

def analyze_wongnai_data(restaurant_data, api_key):
    """
    Analyze Wongnai data using Gemini AI.
    """
    if "error" in restaurant_data:
        return restaurant_data

    genai.configure(api_key=api_key)
    model = genai.GenerativeModel('gemini-2.0-flash')

    reviews_text = "\n".join([f"- {r[:200]}..." for r in restaurant_data['reviews']])
    
    prompt = f"""
    íƒœêµ­ í˜„ì§€ì¸ ë§›ì§‘ ì‚¬ì´íŠ¸ 'Wongnai'ì˜ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´ ì‹ë‹¹ì„ í•œêµ­ì¸ ì—¬í–‰ê° ê´€ì ì—ì„œ ë¶„ì„í•´ì¤˜.

    [ì‹ë‹¹ ì •ë³´]
    - ì´ë¦„: {restaurant_data['name']}
    - ì›¡ë‚˜ì´ ë³„ì : {restaurant_data['score']}
    - íƒœêµ­ í˜„ì§€ ê°€ê²©ëŒ€: {restaurant_data['price']}
    
    [í˜„ì§€ ë¦¬ë·° ë°ì´í„° ìš”ì•½]
    {reviews_text}

    [ë¶„ì„ ê²°ê³¼ í•„ìˆ˜ í¬í•¨ ì‚¬í•­ (í•œêµ­ì–´ë¡œ ì‘ì„±)]:
    1. â­ í˜„ì§€ì¸ ë³„ì  ë¶„ìœ„ê¸° (ì ìˆ˜ê°€ ë†’ì€ì§€, ë¡œì»¬ ì‚¬ëŒë“¤ì—ê²Œ ì¸ê¸° ìˆëŠ” ê³³ì¸ì§€)
    2. ğŸ½ï¸ ì¶”ì²œ ë©”ë‰´ (ë¦¬ë·°ì—ì„œ ê°€ì¥ ë§ì´ ì¹­ì°¬ë°›ëŠ” ìŒì‹ ë˜ëŠ” ëŒ€í‘œ ë©”ë‰´)
    3. ğŸ‡°ğŸ‡· í•œêµ­ì¸ ì…ë§› ì í•©ë„ (ë§µê¸°, í–¥ì‹ ë£Œ ê°•ë„, í•œêµ­ì¸ì´ ì¢‹ì•„í•  ë§Œí•œ í¬ì¸íŠ¸)
    4. ğŸ’° ì²´ê° ë¬¼ê°€ (íƒœêµ­ ë¡œì»¬ ë¬¼ê°€ ëŒ€ë¹„ ì–´ëŠ ì •ë„ ìˆ˜ì¤€ì¸ì§€)
    5. ğŸš« ì£¼ì˜ì‚¬í•­ (ì›¨ì´íŒ… ì—¬ë¶€, ìœ„ì¹˜ì  íŠ¹ì§•, ì„œë¹„ìŠ¤ ê´€ë ¨ ì§€ì  ë“±)

    ì¹œì ˆí•˜ê³  ì‹ ë¢°ê° ìˆëŠ” ë§íˆ¬ë¡œ ìš”ì•½í•´ì„œ ë‹µë³€í•´ì¤˜. ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì„ ì‚¬ìš©í•˜ì—¬ ê°€ë…ì„± ìˆê²Œ ì‘ì„±í•  ê²ƒ.
    """
    
    try:
        response = model.generate_content(prompt)
        return {
            "summary": response.text,
            "info": restaurant_data
        }
    except Exception as e:
        return {"error": f"Gemini ë¶„ì„ ì‹¤íŒ¨: {e}"}


# ============================================
# ğŸ’ AI Tour Recommendation Engine
# ============================================

def recommend_tours(who, style, budget, region="ë°©ì½•", language="Korean"):
    """
    ì‚¬ìš©ì ì…ë ¥ì„ ë°”íƒ•ìœ¼ë¡œ Gemini AIê°€ íˆ¬ì–´ë¥¼ ì¶”ì²œí•˜ëŠ” í•¨ìˆ˜.
    
    Args:
        who: ë™í–‰ì¸ (ì˜ˆ: "í˜¼ì", "ì—°ì¸/ë¶€ë¶€", "ê°€ì¡±(ì•„ì´ë™ë°˜)")
        style: ì„ í˜¸ ìŠ¤íƒ€ì¼ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ["ì¸ìƒìƒ·/ì‚¬ì§„", "ì—­ì‚¬/ë¬¸í™”"])
        budget: ì˜ˆì‚° ì„ í˜¸ (ì˜ˆ: "ê°€ì„±ë¹„(ì €ë ´)", "ì ë‹¹í•¨", "ëŸ­ì…”ë¦¬/í”„ë¦¬ë¯¸ì—„")
        region: ì—¬í–‰ ì§€ì—­ (ì˜ˆ: "ë°©ì½•", "íŒŒíƒ€ì•¼", "ì¹˜ì•™ë§ˆì´")
        language: ì¶œë ¥ ì–¸ì–´ ("Korean" or "English")
    
    Returns:
        dict: {"recommendations": [{"tour_name": ..., "reason": ..., "tip": ...}, ...]}
        None: on failure
    """
    import google.generativeai as genai
    
    # Get API key
    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key:
        try:
            import toml
            secrets = toml.load(".streamlit/secrets.toml")
            api_key = secrets.get("GEMINI_API_KEY")
        except:
            pass
    if not api_key:
        try:
            api_key = st.secrets.get("GEMINI_API_KEY")
        except:
            pass
    
    if not api_key:
        print("âŒ GEMINI_API_KEY not found for tour recommendation")
        return None
    
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel(
            'gemini-2.0-flash',
            generation_config={"response_mime_type": "application/json"}
        )
        
        # Load tours
        TOURS = load_tours()
        
        # Filter tours by region
        filtered_tours = [t for t in TOURS if t.get('region', 'ë°©ì½•') == region]
        
        if not filtered_tours:
            return {"recommendations": []} # No tours for this region

        # Build product catalog for prompt
        is_english = (language == "English")
        
        products_list = []
        for t in filtered_tours:
            if is_english:
                # Prioritize English fields if available
                p_name = t.get('name_en') or t.get('name', 'Unknown')
                p_desc = t.get('desc_en') or t.get('desc', '')
                p_pros = t.get('pros_en') or t.get('pros', '')
            else:
                p_name = t.get('name', 'Unknown')
                p_desc = t.get('desc', '')
                p_pros = t.get('pros', '')
            
            products_list.append(
                f"- ID {t['id']}. {p_name} (Price: {t['price']}): "
                f"Tag={t['type']}, Desc: {p_desc}, Pros: {p_pros}"
            )
        
        products_info = "\n".join(products_list)
        
        style_str = ", ".join(style) if style else ("No specific preference" if is_english else "íŠ¹ë³„í•œ ì„ í˜¸ ì—†ìŒ")
        is_english = (language == "English")

        if is_english:
            prompt = f"""
You are a 'Thailand Travel AI Coordinator' expert on {region}.
Analyze the user's travel style and recommend the **top 6 perfect products** from the [Product Catalog] below.

[User Info]
- Region: {region}
- With: {who}
- Style: {style_str}
- Budget/Other: {budget}

[Product Catalog ({region} only)]
{products_info}

[Output Format - JSON]
Output MUST be in the following JSON format ONLY. 
Descriptions should be friendly, persuasive, and include emojis. 
Write reasons specifically tailored to the user's companions and style. 
ALL OUTPUT VALUES MUST BE IN ENGLISH.

{{
    "recommendations": [
        {{
            "tour_name": "Product Name (MUST match the name in the list exactly)",
            "tour_name_en": "Translated English Product Name",
            "tour_id": "Product ID (integer)",
            "reason": "Why we recommend this (2-3 sentences, persuasive, emoji included)",
            "tip": "One useful tip (e.g., Best at sunset, Raincoat needed, etc.)"
        }},
        ... (Total 6 recommendations)
    ]
}}
"""
        else:
            prompt = f"""
ë‹¹ì‹ ì€ íƒœêµ­ {region} ì—¬í–‰ ì „ë¬¸ 'AI íˆ¬ì–´ ì½”ë””ë„¤ì´í„°'ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì—¬í–‰ ìŠ¤íƒ€ì¼ì„ ë¶„ì„í•˜ì—¬, ì•„ë˜ [ìƒí’ˆ ëª©ë¡] ì¤‘ **ê°€ì¥ ì™„ë²½í•œ ìƒí’ˆ 6ê°œ**ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.

[ì‚¬ìš©ì ì •ë³´]
- ì—¬í–‰ ì§€ì—­: {region}
- ë™í–‰ì¸: {who}
- ì„ í˜¸ ìŠ¤íƒ€ì¼: {style_str}
- ì˜ˆì‚°/ê¸°íƒ€: {budget}

[ìƒí’ˆ ëª©ë¡ ({region} ì „ìš©)]
{products_info}

[ì¶œë ¥ í˜•ì‹ - JSON]
ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì„¤ëª…ì€ í•œêµ­ì–´ë¡œ ì¹œê·¼í•˜ê²Œ, ì´ëª¨ì§€ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.
ì‚¬ìš©ìì˜ ë™í–‰ì¸ê³¼ ìŠ¤íƒ€ì¼ì— ë§ì¶°ì„œ ê°œì¸í™”ëœ ì¶”ì²œ ì´ìœ ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
{{
    "recommendations": [
        {{
            "tour_name": "ìƒí’ˆëª… (ëª©ë¡ì— ìˆëŠ” ì´ë¦„ê³¼ ì •í™•íˆ ì¼ì¹˜)",
            "tour_id": "ìƒí’ˆ ID (ìˆ«ì)",
            "reason": "ì´ íˆ¬ì–´ë¥¼ ì¶”ì²œí•˜ëŠ” ì´ìœ  (ì‚¬ìš©ì ìƒí™©ì— ë§ì¶°ì„œ 2~3ë¬¸ì¥ìœ¼ë¡œ ì„¤ë“ë ¥ ìˆê²Œ, ì´ëª¨ì§€ í¬í•¨)",
            "tip": "ê¿€íŒ í•œì¤„ (ì˜ˆ: ì¼ëª° ì‹œê°„ëŒ€ 5ì‹œ ì¶”ì²œ, ìš°ê¸°ì—” ìš°ë¹„ í•„ìˆ˜ ë“±)"
        }},
        ... (ì´ 6ê°œì˜ ì¶”ì²œ í•­ëª©)
    ]
}}
"""
        
        response = model.generate_content(prompt)
        result = json.loads(response.text)
        return result
        
    except Exception as e:
        print(f"âŒ Tour recommendation error: {e}")
        return None

# --- 3. ë°ì´í„° ë¡œë“œ ë° ì €ì¥ (Data Handling) ---

# êµ¬ê¸€ ì‹œíŠ¸ URL (íˆ¬ì–´ ë°ì´í„°ë² ì´ìŠ¤)
TOURS_SHEET_URL = "https://docs.google.com/spreadsheets/d/186j6qGv1PYmaxUhVDihErGjlFvQfSHERt-4udzrxsHQ/edit?usp=sharing"
TOURS_SHEET_NAME = "ì‹œíŠ¸1"

def load_tours_from_sheet():
    """
    Load tours from Google Sheets.
    Returns: List of tour dictionaries or None on failure.
    """
    try:
        conn = st.connection("gsheets_tours", type=GSheetsConnection)
        df = conn.read(spreadsheet=TOURS_SHEET_URL, worksheet=TOURS_SHEET_NAME)
        
        # Convert DataFrame to list of dicts
        tours = df.to_dict('records')
        
        # Post-process: 'type' column (string -> list)
        for t in tours:
            if isinstance(t.get('type'), str):
                t['type'] = [x.strip() for x in t['type'].split(',') if x.strip()]
            elif not t.get('type'):
                t['type'] = []
                
            # Ensure ID is int
            if 'id' in t:
                try:
                    t['id'] = int(t['id'])
                except:
                    pass
            
            # Ensure price is string (sometimes read as float/int)
            if 'price' in t:
                t['price'] = str(t['price'])

        return tours
    except Exception as e:
        st.error(f"êµ¬ê¸€ ì‹œíŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        print(f"Error loading tours from sheet: {e}")
        return None

def save_tours_to_sheet(tours_data):
    """
    Save tours to Google Sheets.
    Args:
        tours_data: List of tour dictionaries
    """
    try:
        # Convert list back to DataFrame
        df = pd.DataFrame(tours_data)
        
        # Pre-process: 'type' list -> string
        if 'type' in df.columns:
            df['type'] = df['type'].apply(lambda x: ",".join(x) if isinstance(x, list) else str(x))
            
        conn = st.connection("gsheets_tours", type=GSheetsConnection)
        conn.update(spreadsheet=TOURS_SHEET_URL, worksheet=TOURS_SHEET_NAME, data=df)
        return True
    except Exception as e:
        st.error(f"êµ¬ê¸€ ì‹œíŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
        print(f"Error saving tours to sheet: {e}")
        return False

def load_tours():
    """Load tours from Google Sheets (primary) or local JSON (fallback)"""
    # 1. Try Google Sheets
    sheet_tours = load_tours_from_sheet()
    if sheet_tours:
        # Update local cache (Disabled to prevent app restart during session)
        # save_tours_local(sheet_tours)
        return sheet_tours
        
    # 2. Fallback to Local
    print("Fallback to local tours.json")
    return load_tours_local()

def load_tours_local():
    """Load tours from data/tours.json"""
    try:
        with open('data/tours.json', 'r', encoding='utf-8') as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return []

def save_tours(tours):
    """Save tours to Google Sheets AND local JSON"""
    # 1. Save to Sheet
    success = save_tours_to_sheet(tours)
    
    # 2. Save to Local (Cache - Only if Sheet fails and on Localhost)
    # Writing to source files causes Streamlit to restart, clearing sessions.
    if not success:
        save_tours_local(tours)
        print("Warning: Failed to save to Google Sheet, but saved locally.")

def save_tours_local(tours):
    """Save tours to data/tours.json"""
    try:
        os.makedirs(os.path.dirname('data/tours.json'), exist_ok=True)
        with open('data/tours.json', 'w', encoding='utf-8') as f:
            json.dump(tours, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"Error saving local tours: {e}")

# ì§€ì—­ë³„ í´ë£© ì œíœ´ ë§í¬ (ìƒìˆ˜)
CITY_LINKS = {
    "ë°©ì½•": "https://klook.tpx.li/X9VgSPk8",
    "íŒŒíƒ€ì•¼": "https://klook.tpx.li/Te6TSv6q",
    "ì¹˜ì•™ë§ˆì´": "https://klook.tpx.li/yPsMZRxS",
    "í‘¸ì¼“": "https://klook.tpx.li/FDM1ZPlZ",
    "ì½”ì‚¬ë¬´ì´": "https://klook.tpx.li/PjbJR2GU",
    "ë„ë¼ë¹„": "https://klook.tpx.li/WoWJSmgF",
}

# UIì—ì„œ ì‚¬ìš©í•˜ëŠ” ì§€ì—­ ì˜µì…˜ (ì´ëª¨ì§€ í¬í•¨)
# ì§€ì—­ë³„ ì˜µì…˜ ë° ë§¤í•‘ (Localization ì§€ì›)
def get_region_options():
    lang = st.session_state.get('language', 'Korean')
    if lang == 'English':
        return ["ğŸ™ï¸ Bangkok", "ğŸ–ï¸ Pattaya", "ğŸ˜ Chiang Mai", "ğŸï¸ Phuket", "ğŸŒ´ Koh Samui", "â›µ Krabi"]
    else:
        return ["ğŸ™ï¸ ë°©ì½•", "ğŸ–ï¸ íŒŒíƒ€ì•¼", "ğŸ˜ ì¹˜ì•™ë§ˆì´", "ğŸï¸ í‘¸ì¼“", "ğŸŒ´ ì½”ì‚¬ë¬´ì´", "â›µ ë„ë¼ë¹„"]

def get_region_label_to_key():
    lang = st.session_state.get('language', 'Korean')
    if lang == 'English':
        return {
            "ğŸ™ï¸ Bangkok": "ë°©ì½•",
            "ğŸ–ï¸ Pattaya": "íŒŒíƒ€ì•¼",
            "ğŸ˜ Chiang Mai": "ì¹˜ì•™ë§ˆì´",
            "ğŸï¸ Phuket": "í‘¸ì¼“",
            "ğŸŒ´ Koh Samui": "ì½”ì‚¬ë¬´ì´",
            "â›µ Krabi": "ë„ë¼ë¹„"
        }
    else:
        return {
            "ğŸ™ï¸ ë°©ì½•": "ë°©ì½•",
            "ğŸ–ï¸ íŒŒíƒ€ì•¼": "íŒŒíƒ€ì•¼",
            "ğŸ˜ ì¹˜ì•™ë§ˆì´": "ì¹˜ì•™ë§ˆì´",
            "ğŸï¸ í‘¸ì¼“": "í‘¸ì¼“",
            "ğŸŒ´ ì½”ì‚¬ë¬´ì´": "ì½”ì‚¬ë¬´ì´",
            "â›µ ë„ë¼ë¹„": "ë„ë¼ë¹„"
        }

# Klook ì „ì²´ë³´ê¸° ë§í¬
KLOOK_ALL_TOURS_LINK = "https://klook.tpx.li/P3FlPqvh"

def generate_tour_itinerary(tours, region="ë°©ì½•"):
    """
    Generate a 1-day itinerary using the selected tours.
    Args:
        tours: List of tour dictionaries (id, name, type, etc.)
        region: City name
    Returns:
        str: Markdown formatted itinerary
    """
    import google.generativeai as genai
    
    # Get API key
    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key:
        try:
            import toml
            secrets = toml.load(".streamlit/secrets.toml")
            api_key = secrets.get("GEMINI_API_KEY")
        except:
            pass
    if not api_key:
        try:
            api_key = st.secrets.get("GEMINI_API_KEY")
        except:
            pass
            
    if not api_key:
        return "âŒ API Key Missing"

    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.0-flash')
        
        # Determine Current Season in Thailand
        current_month = datetime.now().month
        # Typical Thailand: Rainy (Jun-Oct), Dry/Cool (Nov-May)
        is_rainy_season = 6 <= current_month <= 10
        season_str = "ìš°ê¸°(ë¹„ê°€ ìì£¼ ì˜´)" if is_rainy_season else "ê±´ê¸°(ì—¬í–‰í•˜ê¸° ì¢‹ìŒ)"

        # Format tour list for prompt
        tour_list_str = "\n".join([f"- {t['name']} (íƒœê·¸: {', '.join(t.get('type', [])) if isinstance(t.get('type'), list) else t.get('type', 'ì¼ë°˜')}, ì„¤ëª…: {t.get('desc', '')})" for t in tours])
        
        prompt = f"""
        ë‹¹ì‹ ì€ íƒœêµ­ {region} ì—¬í–‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
        í˜„ì¬ëŠ” **{current_month}ì›”({season_str})**ì…ë‹ˆë‹¤. 
        ì‚¬ìš©ìê°€ ì¥ë°”êµ¬ë‹ˆì— ë‹´ì€ ì•„ë˜ íˆ¬ì–´ ìƒí’ˆë“¤ì„ ì¡°í•©í•˜ì—¬ ê°€ì¥ í˜„ì‹¤ì ì´ê³  ì—¬ìœ ë¡œìš´ **'ìµœì ì˜ ì—¬í–‰ ì¼ì •í‘œ'**ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
        
        [ì„ íƒí•œ íˆ¬ì–´ ëª©ë¡]
        {tour_list_str}
        
        [í•„ìˆ˜ ê³ ë ¤ì‚¬í•­]
        1. **íˆ¬ì–´ ì‹œê°„ ë° ê¸°ê°„ (ë§¤ìš° ì¤‘ìš”)**:
           - ìƒí’ˆ íƒœê·¸ì— **'ì „ì¼íˆ¬ì–´'**, **'ì¢…ì¼'**ì´ ìˆê±°ë‚˜, í˜¹ì€ íƒœê·¸ê°€ ì—†ë”ë¼ë„ **ì•„ìœ íƒ€ì•¼(Ayutthaya), ì¹¸ì°¨ë‚˜ë¶€ë¦¬(Kanchanaburi), ì¹´ì˜¤ì•¼ì´(Khao Yai) ë“± ì™¸ê³½ ì§€ì—­ íˆ¬ì–´**ì™€ ê°™ì´ ì¼ë°˜ì ìœ¼ë¡œ í•˜ë£¨ê°€ ê¼¬ë°• ì†Œìš”ë˜ëŠ” 'ë„ë¦¬ ì•Œë ¤ì§„ íˆ¬ì–´ ìƒí’ˆ'ì¸ ê²½ìš°, í•˜ë£¨ ì „ì²´(8~10ì‹œê°„)ë¥¼ ì†Œìš”í•˜ëŠ” ê²ƒìœ¼ë¡œ ê°„ì£¼í•˜ì—¬ ê·¸ë‚ ì€ ë‹¤ë¥¸ í° ì¼ì •ì„ ì¡ì§€ ë§ˆì„¸ìš”.
           - **'ë°˜ì¼íˆ¬ì–´'** íƒœê·¸ê°€ ìˆê±°ë‚˜ ì‹œë‚´ ì‚¬ì› íˆ¬ì–´, ì¿ í‚¹ í´ë˜ìŠ¤ ë“± ì¼ë°˜ì ìœ¼ë¡œ 4ì‹œê°„ ë‚´ì™¸ì¸ ìƒí’ˆì€ ì˜¤ì „ ë˜ëŠ” ì˜¤í›„ ì¤‘ í•˜ë‚˜ì— ë°°ì¹˜í•˜ê³ , ë‚¨ëŠ” ì‹œê°„ì—ëŠ” ê°€ë²¼ìš´ ììœ  ì¼ì •ì´ë‚˜ ë‹¤ë¥¸ ì§§ì€ ì½”ìŠ¤ë¥¼ ê²°í•©í•˜ì„¸ìš”.
        2. **ê³„ì ˆ ë° ë‚ ì”¨ (ì¤‘ìš”)**: 
           - í˜„ì¬ê°€ **ìš°ê¸°**ì¸ ê²½ìš°, ìƒí’ˆ íƒœê·¸ì— 'ì‹¤ë‚´'ê°€ í¬í•¨ëœ ìƒí’ˆì„ ìš°ì„ ì ìœ¼ë¡œ ë°°ì¹˜í•˜ê±°ë‚˜ ë¹„ê°€ ë‚´ë¦´ ë•Œë¥¼ ëŒ€ë¹„í•œ í”ŒëœBë¥¼ ì œì•ˆí•˜ì„¸ìš”.
           - **ê±´ê¸°**ì¸ ê²½ìš°, ì•¼ì™¸ í™œë™ê³¼ í’ê²½ ê°ìƒì„ ìµœëŒ€í•œ ì¦ê¸¸ ìˆ˜ ìˆë„ë¡ ë°°ì¹˜í•˜ì„¸ìš”.
        3. **êµí†µ ì²´ì¦ ë° ì´ë™ ì‹œê°„**: {region}ì˜ êµí†µ ì²´ì¦(íŠ¸ë˜í”½ ì¼)ì„ ê³ ë ¤í•˜ì—¬ ì¼ì • ì‚¬ì´ì˜ ì´ë™ ì‹œê°„ì„ ë§¤ìš° ë„‰ë„‰í•˜ê²Œ(ìµœì†Œ 1~1.5ì‹œê°„ ì´ìƒ) ë°°ì¹˜í•˜ì„¸ìš”.
        4. **ì²´ë ¥ ë° í”¼ë¡œë„**: ì—¬í–‰ìì˜ ì²´ë ¥ì„ ê³ ë ¤í•˜ì—¬ í•˜ë£¨ì— ë„ˆë¬´ ë§ì€ íˆ¬ì–´ë¥¼ ëª°ì•„ë„£ì§€ ë§ˆì„¸ìš”. 'ëŠê¸‹í•˜ê³  ì—¬ìœ ë¡œìš´ ì—¬í–‰(Slow Travel)'ì´ ë˜ë„ë¡ ë°°ì¹˜í•˜ì„¸ìš”.
        5. **ìœ ì—°í•œ ê¸°ê°„ ì„¤ì •**: ì„ íƒëœ íˆ¬ì–´ì˜ ê°œìˆ˜ì™€ ì„±ê²©ì— ë”°ë¼ 1~5ì¼ ì´ìƒì˜ ì¥ê¸° ì¼ì •ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ í™•ì¥í•˜ì—¬ êµ¬ì„±í•˜ì„¸ìš”.
        6. **ì‹ì‚¬ ë° íœ´ì‹**: ë§¤ì¼ ì ì ˆí•œ ì ì‹¬, ì €ë… ì‹ì‚¬ ì‹œê°„ê³¼ ì¤‘ê°„ íœ´ì‹ ì‹œê°„ì„ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”.
        
        [ì¶œë ¥ í˜•ì‹]
        - ë‚ ì§œë³„ë¡œ êµ¬ë¶„í•˜ì—¬ ì¶œë ¥í•˜ì„¸ìš” (ì˜ˆ: Day 1, Day 2...).
        - ê¹”ë”í•œ ë§ˆí¬ë‹¤ìš´(Markdown) ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” í‘œ í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”.
        - ì „ë¬¸ì ì¸ íŒ(ë³µì¥, ì¤€ë¹„ë¬¼, ë§›ì§‘ ë“±)ì„ ë‚ ì”¨ì— ë§ê²Œ í•œ ì¤„ì”© ì¶”ê°€í•˜ì„¸ìš”.
        - ì„œë¡ ê³¼ ê²°ë¡ ì€ ìƒëµí•˜ê³  ë°”ë¡œ ì¼ì •í‘œ ë‚´ìš©ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì¹œê·¼í•œ ë§íˆ¬(í•´ìš”ì²´)ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
        """
        
        response = model.generate_content(prompt)
        return response.text
        
    except Exception as e:
        return f"âŒ ì¼ì • ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


