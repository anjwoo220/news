import feedparser
import google.generativeai as genai
from datetime import datetime, timedelta
import time
import json
import os
import requests
import re
from bs4 import BeautifulSoup


# Helper: Check if article is within last N days
def is_recent(entry, days=3):
    if not hasattr(entry, 'published_parsed'):
        return True # Default to include if no date
    
    # published_parsed is a struct_time, convert to datetime
    pub_date = datetime.fromtimestamp(time.mktime(entry.published_parsed))
    limit_date = datetime.now() - timedelta(days=days)
    return pub_date >= limit_date

# Helper: Check relevance to Thailand
def is_relevant_to_thailand(entry):
    """
    Determines if an article is relevant to Thailand based on keywords and script.
    Checks: Title, Summary (if available)
    """
    import re
    
    # 1. content to check
    text = (entry.title + " " + entry.get('summary', '')).lower()
    
    # 2. Check for Thai Characters (Script)
    if re.search(r'[\u0E00-\u0E7F]', text):
        return True
        
    # 3. Check for English Keywords
    keywords = [
        "thailand", "thai", "bangkok", "phuket", "pattaya", "chiang", 
        "samui", "krabi", "isan", "baht", "pheu thai", 
        "prime minister", "paetongtarn", "thaksin", "king", "royal",
        "cabinet", "govt", "police", "otp", "airport"
    ]
    
    for kw in keywords:
        if kw in text:
            return True
            
    return False

# 1. RSS Parsing (Balanced)
def fetch_balanced_rss(feeds_config, processed_urls=None):
    """
    Fetches RSS feeds and returns a balanced mix of items across categories.
    feeds_config: List of dicts [{'category': '...', 'url': '...'}, ...]
    processed_urls: Set of strings (optional) to skip already seen news.
    """
    import requests
    
    if processed_urls is None:
        processed_urls = set()
    
    # Using a typical browser User-Agent
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
        'Accept': 'application/rss+xml, application/xml, text/xml, */*'
    }
    
    category_buckets = {}
    MAX_PER_CATEGORY = 20  # Increased from 2 to allow checking more feeds
    
    for feed in feeds_config:
        category = feed.get('category', 'General')
        url = feed.get('url')
        
        if category not in category_buckets:
            category_buckets[category] = []
            
        # Check quota early
        if len(category_buckets[category]) >= MAX_PER_CATEGORY:
            print(f"Skipping feed {url} (Quota full for {category})")
            continue
            
        try:
            print(f"Fetching [{category}] {url}...")
            response = requests.get(url, headers=headers, timeout=10)
            
            if response.status_code != 200:
                print(f"Failed to fetch {url}: Status {response.status_code}")
                continue
                
            feed_data = feedparser.parse(response.content)
            
            if feed_data.bozo:
                print(f"XML Parse Warning for {url}: {feed_data.bozo_exception}")
            
            print(f"Successfully parsed {url}: Found {len(feed_data.entries)} entries.")
            
            for entry in feed_data.entries:
                # Re-check quota inside loop
                if len(category_buckets[category]) >= MAX_PER_CATEGORY:
                    break
                
                # Filter: Relevance Check (Skip non-Thai news)
                if not is_relevant_to_thailand(entry):
                    # print(f"Skipping irrelevant: {entry.title}") 
                    continue

                # Filter: Skip already processed
                if entry.link in processed_urls:
                    # print(f"Skipping already processed: {entry.title}")
                    continue

                if is_recent(entry):
                    item = {
                        "title": entry.title,
                        "link": entry.link,
                        "published": entry.get("published", str(datetime.now())),
                        "summary": entry.get("summary", ""),
                        "source": feed_data.feed.get("title", url),
                        "suggested_category": category, # Hint for AI or logic
                        "_raw_entry": entry
                    }
                    category_buckets[category].append(item)
                    
        except Exception as e:
            print(f"Error fetching {url}: {e}")

    # Interleave (Round-Robin) to create balanced list
    balanced_items = []
    max_items_per_cat = max(len(items) for items in category_buckets.values()) if category_buckets else 0
    
    categories = list(category_buckets.keys())
    
    for i in range(max_items_per_cat):
        for cat in categories:
            if i < len(category_buckets[cat]):
                balanced_items.append(category_buckets[cat][i])
                
    return balanced_items

# 2. Gemini Analysis
import re

def clean_html(raw_html):
    cleanr = re.compile('<.*?>')
    cleantext = re.sub(cleanr, '', raw_html)
    return cleantext


def fetch_full_content(url):
    """
    Attempts to scrape full article content.
    Returns truncated text (max 2000 chars) or None if failed.
    """
    try:
        import requests
        from bs4 import BeautifulSoup
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'
        }
        
        # Simple domain checks for known Paywalls if needed
        if "bangkokpost.com" in url or "thairath" in url or "khaosod" in url:
             pass 

        response = requests.get(url, headers=headers, timeout=5)
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Common Content Selectors (Bangkok Post, Thairath, etc.)
            selectors = [
                 "div.article-content", # Bangkok Post
                 "div.artcl-content",   # Bangkok Post variant
                 "div#content-body",
                 "div.news-detail",     # Thairath often uses this or similar
                 "div.entry-content",   # WordPress standard (Khaosod English)
                 "article"              # Semantic fallback
            ]
            
            content_text = ""
            for select in selectors:
                found = soup.select_one(select)
                if found:
                    # Remove scripts/styles
                    for script in found(["script", "style", "iframe", "div.ads"]):
                        script.decompose()
                    content_text = found.get_text(separator="\n", strip=True)
                    break
            
            # Fallback: Just grab all P tags if no container found
            if not content_text:
                ps = soup.find_all('p')
                # Filter out very short lines (nav items)
                content_text = "\n".join([p.get_text(strip=True) for p in ps if len(p.get_text(strip=True)) > 30])

            if content_text:
                return content_text[:3000] # Cap at 3000 chars for context window
    except Exception as e:
        print(f"Error scraping {url}: {e}")
        
    return None

def analyze_news_with_gemini(news_items, api_key):
    if not news_items:
        return None, "No news items to analyze."
        
    genai.configure(api_key=api_key)
    
    # Limit to top 5 items to avoid Rate Limit (429) on Free Tier
    max_items = 5
    limited_news_items = news_items[:max_items]
    
    aggregated_topics = []
    total_items = len(limited_news_items)
    
    print(f"Starting sequential analysis for {total_items} items (with 10s delay)...")

    for idx, item in enumerate(limited_news_items):
        print(f"[{idx+1}/{total_items}] Processing: {item['title']}...")
        
        # 1. Try to get FULL content
        full_content = fetch_full_content(item['link'])
        
        if not full_content:
            print("   -> Scraping failed/empty, falling back to summary.")
            full_content = clean_html(item['summary'])[:800]
        else:
            print(f"   -> Scraped full content ({len(full_content)} chars).")

        # Single Item Prompt
        prompt = f"""
        ë‹¹ì‹ ì€ íƒœêµ­ ë°©ì½•ì— ì£¼ì¬í•˜ëŠ” 'íƒœêµ­ì–´ì™€ ì˜ì–´ì— ëª¨ë‘ ëŠ¥í†µí•œ ë² í…Œë‘ í•œêµ­ íŠ¹íŒŒì›'ì…ë‹ˆë‹¤.
        ì…ë ¥ëœ ë‰´ìŠ¤ ê¸°ì‚¬(ì˜ì–´, íƒœêµ­ì–´ í˜¼ìš© ê°€ëŠ¥)ë¥¼ í•œêµ­ êµë¯¼, ì—¬í–‰ìë“¤ì´ ì´í•´í•˜ê¸° ì‰¬ìš´ **ì™„ë²½í•œ í•œêµ­ì–´ ë‰´ìŠ¤ ê¸°ì‚¬**ë¡œ ì¬ì‘ì„±í•˜ì„¸ìš”.

        [í•µì‹¬ ì²˜ë¦¬ ê·œì¹™]
        1. **ë‹¤êµ­ì–´ ì²˜ë¦¬ (ì¤‘ìš”):** 
           - ê¸°ì‚¬ ì›ë¬¸ì— **íƒœêµ­ì–´(Thai Script)**ê°€ í¬í•¨ëœ ê²½ìš°, ì ˆëŒ€ ìƒëµí•˜ê±°ë‚˜ ì›ë¬¸ ê·¸ëŒ€ë¡œ ë‚¨ê²¨ë‘ì§€ ë§ˆì„¸ìš”.
           - **ì¼ë°˜ ë¬¸ì¥:** í•œêµ­ì–´ë¡œ ì˜ë¯¸ë¥¼ ë²ˆì—­í•˜ì„¸ìš”.
           - **ê³ ìœ ëª…ì‚¬(ì§€ëª…, ì¸ëª…, ê°€ê²Œ ì´ë¦„):** í•œêµ­ì–´ í‘œì¤€ ì™¸ë˜ì–´ í‘œê¸°ë²•ì— ë§ì¶° **ë°œìŒëŒ€ë¡œ í‘œê¸°**í•˜ì„¸ìš”. (ì˜ˆ: à¸ à¸¹ì¼“ -> í‘¸ì¼“, à¸ªà¸¸à¸‚à¸¸à¸¡à¸§à¸´à¸— -> ìˆ˜ì¿°ë¹—)
           - ë§Œì•½ ì •í™•í•œ ë°œìŒì„ ëª¨ë¥¼ ê²½ìš°ì—ë§Œ ê´„í˜¸ ì•ˆì— ì›ì–´ë¥¼ ë³‘ê¸°í•˜ì„¸ìš”. ì˜ˆ: ì™“ ì•„ë£¬(Wat Arun)

        2. **ë‚ ì§œ/ì—°ë„ ë³€í™˜ (ë§¤ìš° ì¤‘ìš”):**
           - íƒœêµ­ ë¶ˆê¸° ì—°ë„(BE)ê°€ ë‚˜ì˜¤ë©´ ë°˜ë“œì‹œ **ì„œê¸°(AD)**ë¡œ ë³€í™˜í•˜ì„¸ìš”.
           - ê³µì‹: **(ë¶ˆê¸°) - 543 = (ì„œê¸°)**
           - ì˜ˆ: 2569ë…„ 1ì›” -> 2026ë…„ 1ì›”, 2567ë…„ -> 2024ë…„.
           - ì ˆëŒ€ 25xxë…„ ê·¸ëŒ€ë¡œ í‘œê¸°í•˜ì§€ ë§ˆì„¸ìš”.

        3. **ê¸°ìì²´ ì‚¬ìš©:** "~í–ˆìŠµë‹ˆë‹¤" ëŒ€ì‹  "~í–ˆë‹¤", "~ì „ë§ì´ë‹¤" ë“± ëª…ë£Œí•œ ë³´ë„ì²´ ë¬¸ì¥ì„ ì‚¬ìš©í•˜ì„¸ìš”.
        3. **ë¶ˆí•„ìš”í•œ ì„œìˆ  ì œê±°:** "ê¸°ì‚¬ì— ë”°ë¥´ë©´", "ë‹¤ìŒì€ ë²ˆì—­ì…ë‹ˆë‹¤" ê°™ì€ AI íˆ¬ì˜ ë¬¸ì¥ì€ ì‚­ì œí•˜ê³  ë°”ë¡œ ì‚¬ì‹¤(Fact)ë¶€í„° ì „ë‹¬í•˜ì„¸ìš”.
        4. **ë…ì ì¤‘ì‹¬:** ì£¼ ë…ìëŠ” íƒœêµ­ ê±°ì£¼ í•œêµ­ì¸ì…ë‹ˆë‹¤. ê·¸ë“¤ì—ê²Œ í•„ìš”í•œ ì •ë³´(ìœ„ì¹˜, ë‚ ì§œ, ê°€ê²©, ì£¼ì˜ì‚¬í•­)ë¥¼ ê°•ì¡°í•˜ì„¸ìš”.

        [ê¸°ì‚¬ ì •ë³´]
        - Title: {item['title']}
        - Source: {item['source']}
        - Link: {item['link']}
        
        [ê¸°ì‚¬ ë³¸ë¬¸]
        {full_content}

        [ì‘ì„± ìš”êµ¬ì‚¬í•­]
        1. **í—¤ë“œë¼ì¸:** í•œêµ­ ë…ìì˜ ëˆˆê¸¸ì„ ë„ëŠ” ë§¤ë ¥ì ì¸ í•œêµ­ì–´ ì œëª©ì„ ë½‘ìœ¼ì„¸ìš”.
        2. **í•µì‹¬ ìš”ì•½:** ë°”ìœ í˜„ëŒ€ì¸ì„ ìœ„í•´ í•µì‹¬ ë‚´ìš©ì„ 3ì¤„ ì´ë‚´ì˜ ê°œì¡°ì‹ìœ¼ë¡œ ëª…ë£Œí•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”.
        3. **ë¶„ë¥˜:** ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒ: ["ì •ì¹˜/ì‚¬íšŒ", "ê²½ì œ", "ì—¬í–‰/ê´€ê´‘", "ì‚¬ê±´/ì‚¬ê³ ", "ì—”í„°í…Œì¸ë¨¼íŠ¸", "ê¸°íƒ€"]
           - **Weather Rule:** ë‚ ì”¨, ê¸°ì˜¨, í™ìˆ˜, ë¯¸ì„¸ë¨¼ì§€ ë“± ê¸°ìƒ ê´€ë ¨ ë‚´ìš©ì€ ë¬´ì¡°ê±´ **'ì—¬í–‰/ê´€ê´‘'**ìœ¼ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
           - ë‚ ì”¨ ê¸°ì‚¬ì¼ ê²½ìš° ë³¸ë¬¸ì˜ êµ¬ì²´ì  ì˜¨ë„(ì˜ˆ: 38ë„)ë¥¼ ìš”ì•½ì— ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”.
        
        4. **[ì¤‘ìš”] ê¸°ì‚¬ ì „ë¬¸ ì‘ì„± (`full_translated`):**
           - ìœ„ [ê¸°ì‚¬ ë³¸ë¬¸]ì„ ë°”íƒ•ìœ¼ë¡œ ì™„ë²½í•œ íë¦„ì˜ í•œêµ­ì–´ ê¸°ì‚¬ë¥¼ ìƒˆë¡­ê²Œ ì‘ì„±í•˜ì„¸ìš”. (ë‹¨ìˆœ ë²ˆì—­ X, ê¸°ì‚¬ ì‘ì„± O)
           - ë¬¸ë‹¨ ë‚˜ëˆ„ê¸°ëŠ” `\\n\\n`ìœ¼ë¡œ ëª…í™•íˆ í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì´ì„¸ìš”.
           - ì ˆëŒ€ ì›ë¬¸ì˜ ë‚´ìš©ì„ ìƒëµí•˜ì§€ ë§ê³  ì¶©ì‹¤íˆ ì „ë‹¬í•˜ë˜, ë¬¸ì²´ëŠ” ì™„ë²½í•œ í•œêµ­ì–´ ê¸°ìì²´ì—¬ì•¼ í•©ë‹ˆë‹¤.

        [ì¶œë ¥ í¬ë§· (JSON Only)]
        {{
          "topics": [
            {{
              "title": "ì‘ì„±ëœ ê¸°ì‚¬ ì œëª©",
              "summary": "3ì¤„ í•µì‹¬ ìš”ì•½",
              "full_translated": "ì‘ì„±ëœ ê³ í’ˆì§ˆ ê¸°ì‚¬ ì „ë¬¸ (Markdown í˜•ì‹)",
              "category": "ì„ íƒëœ ì¹´í…Œê³ ë¦¬",
              "references": [
                {{"title": "{item['title']}", "url": "{item['link']}", "source": "{item['source']}"}}
              ]
            }}
          ]
        }}
        """
        
        # Retry Logic with Safety Limits
        max_retries = 3
        retry_count = 0
        success = False
        
        while retry_count < max_retries and not success:
            try:
                model = genai.GenerativeModel('gemini-2.0-flash', generation_config={"response_mime_type": "application/json"})
                response = model.generate_content(prompt)
                result = json.loads(response.text)
                
                if 'topics' in result and result['topics']:
                    aggregated_topics.extend(result['topics'])
                    print(f"   -> Success. Topics so far: {len(aggregated_topics)}")
                    success = True
                else:
                    raise ValueError("Empty topics in response")
                
            except Exception as e:
                retry_count += 1
                wait_time = 2 ** retry_count # Exponential backoff: 2s, 4s, 8s
                print(f"   -> API Error for item {idx+1} (Attempt {retry_count}/{max_retries}): {e}")
                
                if "429" in str(e):
                    print("   -> Rate Limit Hit. Waiting longer...")
                    time.sleep(60) # Special wait for Rate Limit
                elif retry_count < max_retries:
                    print(f"   -> Retrying in {wait_time}s...")
                    time.sleep(wait_time)
                else:
                    print("   -> Max retries reached. Skipping this item.")
            
        # Delay logic (except for the last one)
        if idx < total_items - 1:
            print("   -> Waiting 20 seconds to respect API rate limits...")
            time.sleep(20)

    return {"topics": aggregated_topics}, None


def load_local_json(file_path):
    if os.path.exists(file_path):
        with open(file_path, 'r', encoding='utf-8') as f:
            try:
                return json.load(f)
            except:
                return {}
    return {}


# 3. Exchange Rate (THB -> KRW)
def get_thb_krw_rate():
    """
    Fetches the current THB to KRW exchange rate.
    Uses 'data/exchange_rate.json' for persistence.
    """
    RATE_FILE = 'data/exchange_rate.json'
    url = "https://api.frankfurter.app/latest?from=THB&to=KRW"
    
    # helper to save
    def save_rate(rate):
        try:
            with open(RATE_FILE, 'w', encoding='utf-8') as f:
                json.dump({"rate": rate, "updated_at": str(datetime.now())}, f)
        except: pass

    # helper to load
    def load_cached_rate():
        if os.path.exists(RATE_FILE):
            try:
                with open(RATE_FILE, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return data.get("rate")
            except: pass
        return None

    try:
        import requests
        response = requests.get(url, timeout=5)
        if response.status_code == 200:
            data = response.json()
            rate = data.get('rates', {}).get('KRW')
            if rate:
                save_rate(rate)
                return rate
    except Exception as e:
        print(f"Exchange Rate Error: {e}")
    
    # Fallback to cached rate if live fetch fails
    cached = load_cached_rate()
    if cached:
        return cached
        
    # If absolutely no data (first run ever & fail), return None or handled by UI
    return 0.0

# 4. Air Quality (WAQI)
def get_air_quality(token):
    """
    Fetches real-time Air Quality (PM 2.5) for Bangkok.
    Returns:
        dict: {'aqi': int, 'status': str} or None if failed.
    """
    url = f"https://api.waqi.info/feed/bangkok/?token={token}"
    try:
        import requests
        response = requests.get(url, timeout=5)
        if response.status_code == 200:
            data = response.json()
            if data.get('status') == 'ok':
                aqi = data['data']['aqi']
                return {'aqi': aqi}
    except Exception as e:
        print(f"Air Quality Error: {e}")
    
    return None



def fetch_thai_events():
    """
    Fetches and parses event information from ThaiTicketMajor, BK Magazine, and TAT News using Gemini.
    Returns:
        list: A list of event dictionaries (title, date, location, region, image_url, link, type).
    """
    print("Fetching Thai Events (National)...")
    
    targets = [
        {
            "name": "ThaiTicketMajor",
            "url": "https://www.thaiticketmajor.com/concert/",
            "selector": "body"
        },
        {
            "name": "BK Magazine",
            "url": "https://bk.asia-city.com/things-to-do-bangkok",
            "selector": "div.view-content"
        },
        {
            "name": "TAT News",
            "url": "https://www.tatnews.org/category/events-festivals/",
            "selector": "body"
        }
    ]

    combined_html_context = ""

    for target in targets:
        try:
            print(f" - Requesting {target['name']}...")
            headers = {'User-Agent': 'Mozilla/5.0'}
            response = requests.get(target['url'], headers=headers, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Extract relevant part
                content = soup.select_one(target['selector'])
                if not content:
                     content = soup.body
                
                # Kill scripts
                for s in content(["script", "style", "nav", "footer", "header"]):
                    s.extract()
                
                html_snippet = str(content)[:20000] # Increased limit for TAT
                
                combined_html_context += f"\n\n--- Source: {target['name']} ({target['url']}) ---\n{html_snippet}"
                
        except Exception as e:
            print(f"Error fetching {target['name']}: {e}")

    if not combined_html_context:
        return []

    # Gemini Processing
    try:
        # Load API Key (Handle Env vs Secrets)
        api_key = os.environ.get("GEMINI_API_KEY")
        if not api_key:
             try:
                import toml
                secrets = toml.load(".streamlit/secrets.toml")
                api_key = secrets.get("GEMINI_API_KEY")
             except:
                pass
        
        if not api_key:
            return []

        import google.generativeai as genai
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.0-flash', generation_config={"response_mime_type": "application/json"})
        
        today_str = datetime.now().strftime('%Y-%m-%d')
        
        prompt = f"""
        You are a helpful event curator for Korean tourists visiting Thailand.
        Analyze the following HTML snippets from event websites (ThaiTicketMajor, BK Magazine, TAT News).
        Extract a list of distinct events/festivals across Thailand.
        
        Current Date: {today_str}
        
        CRITICAL: Identify the **REGION** (City/Province) based on the location info.
        - If "Chiang Mai" -> "ì¹˜ì•™ë§ˆì´"
        - If "Phuket" -> "í‘¸ì¼“"
        - If "Pattaya" -> "íŒŒíƒ€ì•¼"
        - If "Bangkok" -> "ë°©ì½•"
        - If "Koh Samui" -> "ì½”ì‚¬ë¬´ì´"
        - If unknown or miscellaneous, default to "ê¸°íƒ€" (Others) or "ë°©ì½•" if mostly likely Bangkok.
        
        Return the result ONLY as a JSON list of objects.
        
        JSON Format:
        [
            {{
                "title": "Event Name (Summarize in Korean, e.g. 'ì†¡í¬ë€ ì¶•ì œ')",
                "date": "YYYY-MM-DD or Date Range String (e.g. '2024-04-13 ~')",
                "location": "Venue Name (in Korean or English)",
                "region": "ë°©ì½•/ì¹˜ì•™ë§ˆì´/í‘¸ì¼“/íŒŒíƒ€ì•¼/ê¸°íƒ€",
                "image_url": "Full URL of the event poster/image",
                "link": "Full URL to booking page or article",
                "booking_date": "YYYY-MM-DD HH:MM (Ticket Open Time) or 'Now Open' or 'TBD'",
                "price": "Exact Price (e.g. '3,000 THB') or range",
                "type": "ì¶•ì œ" or "ì½˜ì„œíŠ¸" or "ì „ì‹œ" or "ê¸°íƒ€"
            }}
        ]

        Rules:
        1. Select 8-12 diverse items (Mix of Concerts, Festivals, Exhibitions).
        2. CRITICAL: EXCLUDE events that ended BEFORE {today_str}. Only show current or future events.
        3. CRITICAL: If you see a date from a past year (e.g. 2024 if today is 2026, or 2017, 2018...), IGNORE IT. Do not output old events.
        4. Prefer events happening soon (next 45 days).
        3. Ensure image_url is absolute.
        4. Output strictly JSON.
        
        HTML Context:
        {combined_html_context}
        """
        
        print(" - Sending to Gemini...")
        response = model.generate_content(prompt)
        text = response.text.strip()
        
        # Clean markdown
        if text.startswith("```json"):
            text = text[7:]
        if text.endswith("```"):
            text = text[:-3]
            
        data = json.loads(text)
        print(f" - Parsed {len(data)} events with Region info.")
        return data

    except Exception as e:
        print(f"Gemini processing error: {e}")
        return []

def extract_event_from_url(url, api_key):
    """
    Scrapes a URL and uses Gemini to extract event details.
    Returns a dict with processed event info.
    """
    try:
        # 1. Scrape Content
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
             'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7'
        }
        resp = requests.get(url, headers=headers, timeout=10)
        resp.raise_for_status()
        
        soup = BeautifulSoup(resp.content, 'html.parser')
        
        # Remove scripts/styles
        for script in soup(["script", "style", "nav", "footer", "header"]):
            script.decompose()
            
        text_content = soup.get_text(separator=' ', strip=True)[:15000] # Limit context
        
        # Try to find OG Image
        og_image = ""
        meta_img = soup.find("meta", property="og:image")
        if meta_img:
            og_image = meta_img.get("content", "")
            
        title_guess = soup.title.string if soup.title else ""

        # 2. Gemini Analysis
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.0-flash-exp')
        
        prompt = f"""
        Analyze the following webpage text and extract event information.
        
        URL: {url}
        Page Title: {title_guess}
        Text Content:
        {text_content}
        
        Goal: Extract details for a "Big Match" event (Festival/Concert).
        
        Output JSON Format:
        {{
            "title": "Event Name (Korean, e.g. 'ë¡¤ë§ë¼ìš°ë“œ íƒœêµ­ 2024')",
            "date": "YYYY-MM-DD or Range (e.g. '2024-11-22 ~ 11-24')",
            "location": "Venue Name (Korean/English)",
            "region": "One of: ['ë°©ì½•', 'íŒŒíƒ€ì•¼', 'ì¹˜ì•™ë§ˆì´', 'í‘¸ì¼“', 'ê¸°íƒ€']",
            "type": "One of: ['ì¶•ì œ', 'ì½˜ì„œíŠ¸', 'ì „ì‹œ', 'í´ëŸ½/íŒŒí‹°', 'ê¸°íƒ€']",
            "booking_date": "Ticket Open Date (YYYY-MM-DD HH:MM) or 'Now Open'",
            "price": "Exact Price (e.g. '3,000 THB') or Range",
            "status": "One of: ['í‹°ì¼“ì˜¤í”ˆ', 'ê°œìµœí™•ì •', 'ë§¤ì§„', 'ì •ë³´ì—†ìŒ']",
            "image_url": "Use existing OG Image if valid, or find one in text. If none, return empty string.",
            "description": "1 line summary in Korean"
        }}
        
        If image_url is missing in text, use this one: {og_image}
        
        Translate all text to natural Korean.
        If information is missing, use "ì •ë³´ì—†ìŒ" or "" (empty string).
        """
        
        response = model.generate_content(prompt)
        text_response = response.text.strip()
        
        # Parse JSON
        if "```json" in text_response:
            text_response = text_response.replace("```json", "").replace("```", "")
        if text_response.startswith("```"): # Catch raw block
            text_response = text_response.replace("```", "")
        
        data = json.loads(text_response)
        data['link'] = url # Ensure link is set
        
        # Safety fallback for image
        if not data.get('image_url') and og_image:
            data['image_url'] = og_image
            
        return data, None
        
    except Exception as e:
        return None, str(e)

def fetch_big_events_by_keywords(keywords, api_key):
    """
    Crawls Google News RSS (Thailand Locale) for keywords and critically verifies details with Gemini.
    """
    import feedparser
    import urllib.parse
    
    found_events = []
    
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel('gemini-2.0-flash-exp')

    for kw in keywords:
        print(f"Checking keyword: {kw}")
        encoded_kw = urllib.parse.quote(kw)
        # Use Thailand Locale (en-TH)
        rss_url = f"https://news.google.com/rss/search?q={encoded_kw}&hl=en-TH&gl=TH&ceid=TH:en"
        
        feed = feedparser.parse(rss_url)
        
        # Check top 2 entries (efficient)
        entries_to_check = feed.entries[:2]
        if not entries_to_check:
            continue
            
        # Aggregate text for analysis
        combined_text = f"Target Event: {kw}\n"
        for i, entry in enumerate(entries_to_check):
            combined_text += f"[{i+1}] Title: {entry.title}\nLink: {entry.link}\nSummary: {entry.get('summary','')}\nPubDate: {entry.get('published','')}\n\n"
            
        prompt = f"""
        Analyze these news search results for the event "{kw}" in Thailand.
        
        News Content:
        {combined_text}
        
        Goal: Determine if there is CONFIRMED information about the NEXT event date and venue.
        
        CRITICAL VALIDATION RULES:
        1. **CONFIRMED ONLY**: Do NOT extract if it's just a "rumor", "expected to be", "in talks", or from a past year.
        2. **Future Only**: Date must be in the future (2025-2027).
        3. **Specifics**: You must find BOTH a specific date (or confirmed month) AND a venue/city.
        
        If the event is NOT confirmed or is just a rumor:
        Return JSON: {{ "found": false, "reason": "Just a rumor or no data" }}

        If CONFIRMED:
        Return JSON:
        {{
            "found": true,
            "title": "Event Name (Korean)",
            "date": "YYYY-MM-DD or Range",
            "location": "Venue Name",
            "booking_date": "Ticket Open Date (YYYY-MM-DD HH:MM) or 'TBD'",
            "price": "Exact Price (e.g. '3,000 THB') or Range",
            "status": "ê°œìµœí™•ì •", 
            "link": "Best Link URL from the news",
            "description": "1 line confirmed summary in Korean"
        }}
        """
        
        try:
            response = model.generate_content(prompt)
            text = response.text.strip()
            if "```json" in text:
                text = text.replace("```json", "").replace("```", "")
            if text.startswith("```"):
                text = text.replace("```", "")
                
            data = json.loads(text)
            
            if data.get('found'):
                # Basic validation
                if '201' in data.get('date',''): 
                     pass
                else:
                    found_events.append(data)
            else:
                print(f" -> {kw}: Not confirmed ({data.get('reason')})")
                    
        except Exception as e:
            print(f"Error analyzing {kw}: {e}")
            
    return found_events

# --------------------------------------------------------------------------------
# Trend Hunter (Magazine) Logic - 4 Sources
# --------------------------------------------------------------------------------

def fetch_trend_hunter_items(api_key, existing_links=None):
    """
    Aggregates trend/travel content via Google News RSS for 4 sources:
    1. Wongnai (Restaurants)
    2. TheSmartLocal TH (Hotspots)
    3. Chillpainai (Local Travel)
    4. BK Magazine (BKK Life)
    
    Returns:
        list: shuffled list of dicts {title, desc, location, image_url, link, badge}
    """
    import random
    import requests
    import feedparser
    
    print("Fetching Trend Hunter items via Google News RSS...")
    
    items = []
    if existing_links is None:
        existing_links = set()
    else:
        existing_links = set(existing_links)
        
    seen_links = set() # Local deduplication
    
    # Target Domains (Loaded from sources.json)
    SOURCES_FILE = 'data/sources.json'
    targets = []
    
    # 1. Try Loading from File
    if os.path.exists(SOURCES_FILE):
        try:
            with open(SOURCES_FILE, 'r', encoding='utf-8') as f:
                s_data = json.load(f)
                if s_data.get('magazine_targets'):
                    # Filter enabled only
                    targets = [t for t in s_data['magazine_targets'] if t.get('enabled', True)]
        except Exception as e:
            print(f"Error loading sources.json: {e}")
            
    # 2. Fallback if empty (Hardcoded defaults)
    if not targets:
        print("Using default magazine targets (Fallback).")
        targets = [
            {"name": "Wongnai", "domain": "wongnai.com", "tag": "[ë§›ì§‘ë­í‚¹]"},
            {"name": "Chillpainai", "domain": "chillpainai.com", "tag": "[ë¡œì»¬ì—¬í–‰]"},
            {"name": "BK Magazine", "domain": "bk.asia-city.com", "tag": "[ë°©ì½•ë¼ì´í”„]"},
            {"name": "The Smart Local", "domain": "thesmartlocal.co.th", "tag": "[MZí•«í”Œ]"}
        ]
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    # Helper: Gemini Analyzer
    def analyze_rss_items(raw_inputs, source_tag):
        if not raw_inputs: return []
        try:
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel('gemini-2.0-flash', generation_config={"response_mime_type": "application/json"})
            
            prompt = f"""
            You are a expert Korean Travel Editor acting as a **"Hotplace Detector"**.
            Your goal is to filter and rewrite RSS items into high-quality **Korean** magazine content.
            
            Input Data ({source_tag}):
            {json.dumps(raw_inputs, ensure_ascii=False)}
            
            **CRITICAL FILTERING RULES (Hotplace Detector)**:
            Analyze each item. If an item falls into any of these categories, **return null** for that item instead of a JSON object:
            1. **Not a specific visitable place**: General news, flight promos, "Thai Trip" general guides, or listicles without a clear focus.
            2. **Vague/Ad**: Content that sounds like a generic advertisement or lacks specific details.
            3. **No Image**: If you cannot infer a strong visual context or the input lacks an image.
            
            **REWRITE INSTRUCTIONS (For valid items)**:
            1. **LANGUAGE**: Natural, witty, trendy **Korean**.
            2. **INFERENCE**: Infer details (Vibe, Menu, Tips) from context.
            3. **FIELDS**:
               - "catchy_headline": Click-bait style 1-liner in Korean.
               - "desc": 2-3 sentences summary (Focus on why it's hot).
               - "location": Infer Area (e.g. 'Thong Lor', 'Siam').
               - "badge": Use "{source_tag}"
            
            Return JSON List of objects (excluding nulls).
            Example:
            [
                }},
                null,
                ...
            ]
            """
            
            response = model.generate_content(prompt)
            data = json.loads(response.text.strip().replace("```json", "").replace("```", ""))
            
            processed = []
            for res in data:
                if not res: continue # Skip null items (filtered)
                
                idx = res.get('original_index')
                if idx is not None and idx < len(raw_inputs):
                    original = raw_inputs[idx]
                    res['image_url'] = original.get('raw_img') 
                    res['link'] = original.get('raw_link')
                    res['badge'] = source_tag
                    processed.append(res)
            return processed
        except Exception as e:
            print(f"Analysis Error ({source_tag}): {e}")
            return []

    # Main Loop
    for target in targets:
        try:
            # Google News RSS URL (Reduced restriction)
            rss_url = f"https://news.google.com/rss/search?q=site:{target['domain']}&hl=en-TH&gl=TH&ceid=TH:en"
            print(f"Reading RSS: {target['name']}...")
            
            resp = requests.get(rss_url, headers=headers, timeout=10)
            feed = feedparser.parse(resp.content)
            
            raw_items = []
            # Check up to 10 entries to find 2 valid ones
            for entry in feed.entries[:10]:
                if len(raw_items) >= 2: break
                
                # 1. Deduplication (Link & Title)
                if entry.link in existing_links or entry.link in seen_links:
                    print(f"Skipping duplicate: {entry.title}")
                    continue
                
                # 2. Chillpainai Filter
                if target['name'] == "Chillpainai" and "Thai Trip" in entry.title:
                    print(f"Skipping Chillpainai 'Thai Trip': {entry.title}")
                    continue

                seen_links.add(entry.link)
                
                # Attempt to find image
                img_src = ""
                if 'media_content' in entry:
                    img_src = entry.media_content[0]['url']
                elif 'description' in entry:
                     import re
                     match = re.search(r'src="([^"]+)"', entry.description)
                     if match: img_src = match.group(1)
                
                raw_items.append({
                    "raw_title": entry.title,
                    "raw_link": entry.link,
                    "raw_img": img_src,
                    "context": f"Latest article from {target['name']}"
                })
            
            if raw_items:
                analyzed = analyze_rss_items(raw_items, target['tag'])
                items.extend(analyzed)
                
        except Exception as e:
            print(f"Error fetching {target['name']}: {e}")

    # Shuffle for Magazine feel
    random.shuffle(items)
    return items

def push_changes_to_github(files_to_commit, commit_message):
    """
    Commits and pushes specified files to GitHub.
    Requires GITHUB_TOKEN in secrets.toml or environment.
    """
    import subprocess
    import toml
    
    # 1. Get Token
    token = os.environ.get("GITHUB_TOKEN")
    if not token:
        try:
            secrets = toml.load(".streamlit/secrets.toml")
            token = secrets.get("GITHUB_TOKEN")
        except: pass
    
    if not token:
        return False, "GITHUB_TOKEN not found in secrets."

    # 2. Configure Git (If needed)
    # Check if user is set
    try:
        subprocess.run("git config user.name", shell=True, check=True, capture_output=True)
    except:
        subprocess.run('git config user.email "auto-deploy@streamlit.app"', shell=True)
        subprocess.run('git config user.name "Streamlit Admin"', shell=True)

    try:
        # 3. Add Files
        for f in files_to_commit:
            subprocess.run(f"git add {f}", shell=True, check=True)
            
        # 4. Commit
        subprocess.run(f'git commit -m "{commit_message}"', shell=True, check=True)
        
        # 5. Push
        # Use token in URL for auth
        repo_url = subprocess.check_output("git remote get-url origin", shell=True, text=True).strip()
        
        if "https://" in repo_url:
            auth_url = repo_url.replace("https://", f"https://{token}@")
        else:
            auth_url = repo_url
            
        subprocess.run(f"git push {auth_url} HEAD:main", shell=True, check=True)
        
        return True, "Successfully pushed to GitHub!"
        
    except subprocess.CalledProcessError as e:
        return False, f"Git Error: {e}"
    except Exception as e:
        return False, f"Error: {e}"


# --------------------------------------------------------------------------------
# Visitor Counter (counterapi.dev)
# --------------------------------------------------------------------------------

# --------------------------------------------------------------------------------
# Visitor Counter (counterapi.dev)
# --------------------------------------------------------------------------------

def get_visitor_stats():
    """
    Fetches both Total and Daily visitor counts.
    Returns: (total_count, daily_count)
    """
    try:
        import requests
        from datetime import datetime
        
        namespace = "today-thailand-app"
        today_str = datetime.now().strftime("%Y-%m-%d")
        
        # Keys
        key_total = f"total"
        key_daily = f"date_{today_str}"
        
        # 1. Get Total
        total_val = 0
        try:
            url_total = f"https://api.counterapi.dev/v1/{namespace}/{key_total}"
            r1 = requests.get(url_total, timeout=2)
            if r1.status_code == 200:
                total_val = r1.json().get("count", 0)
        except: pass
        
        # 2. Get Daily
        daily_val = 0
        try:
            url_daily = f"https://api.counterapi.dev/v1/{namespace}/{key_daily}"
            r2 = requests.get(url_daily, timeout=2)
            if r2.status_code == 200:
                daily_val = r2.json().get("count", 0)
        except: pass
            
        return total_val, daily_val
        
    except:
        return 0, 0

def increment_visitor_stats():
    """
    Increments both Total and Daily counts (once per session).
    Returns: (new_total, new_daily)
    """
    try:
        import requests
        from datetime import datetime
        
        namespace = "today-thailand-app"
        today_str = datetime.now().strftime("%Y-%m-%d")
        
        # Keys
        key_total = f"total"
        key_daily = f"date_{today_str}"
        
        # 1. Hit Total (+1)
        total_val = 0
        try:
            url_total = f"https://api.counterapi.dev/v1/{namespace}/{key_total}/up"
            r1 = requests.get(url_total, timeout=2)
            if r1.status_code == 200:
                total_val = r1.json().get("count", 0)
        except: pass
        
        # 2. Hit Daily (+1)
        daily_val = 0
        try:
            url_daily = f"https://api.counterapi.dev/v1/{namespace}/{key_daily}/up"
            r2 = requests.get(url_daily, timeout=2)
            if r2.status_code == 200:
                daily_val = r2.json().get("count", 0)
        except: pass
        
        return total_val, daily_val
        
    except:
        return 0, 0

# --------------------------------------------------------------------------------
# Twitter Trend Analyzer (trends24.in + Gemini)
# --------------------------------------------------------------------------------
def fetch_twitter_trends(api_key):
    """
    Scrapes trends24.in/thailand/ for top 10 hashtags and analyzes them with Gemini.
    Returns: dict { "topic": "...", "reason": "...", "severity": "info" } or None
    """
    import requests
    from bs4 import BeautifulSoup
    import google.generativeai as genai
    import json
    
    url = "https://trends24.in/thailand/"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        print("Fetching Twitter Trends from trends24.in...")
        resp = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(resp.content, 'html.parser')
        
        # trends24 structure: .trend-card__list (first one is latest) -> li -> a
        trend_list = soup.select('.trend-card__list')
        
        if not trend_list:
            print("No trend list found.")
            return None
            
        # Get top 10 from the most recent hour (first list)
        top_trends = []
        for li in trend_list[0].find_all('li')[:10]:
            top_trends.append(li.get_text(strip=True))
            
        if not top_trends:
            return None
            
        print(f"Top 10 Trends: {top_trends}")
        
        # Analyze with Gemini
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.0-flash-exp')
        
        prompt = f"""
        Analyze these real-time Thailand Twitter trends:
        {json.dumps(top_trends, ensure_ascii=False)}
        
        Goal: Identify if there is ANY critical or useful information for a FOREIGN TRAVELER in Bangkok/Thailand right now.
        
        Criteria:
        1.  **Critical**: Protests, Riots, Severe Weather (Floods), Transport Chaos (BTS/MRT breakdowns), Major Accidents.
        2.  **Useful**: K-Pop Star arrival (Airport crowds), Major Festival occurring NOW.
        3.  **Ignore**: Political gossip, Celebrity dating rumors, Fan-wars, TV show hashtags.
        
        Output JSON (Return null if nothing important):
        {{
            "topic": "Keyword or Hashtag",
            "reason": "1 simple sentence in KOREAN explaining the situation for a tourist. (e.g. 'ì‹œìœ„ë¡œ ì¸í•´ ì‹œì•” ì§€ì—­ì´ í˜¼ì¡í•˜ë‹ˆ í”¼í•˜ì„¸ìš”')",
            "severity": "warning" (for danger/disruption) or "info" (for events/crowds)
        }}
        
        * If multiple issues, pick the MOST critical one.
        * If nothing relevant, return null.
        """
        
        response = model.generate_content(prompt)
        result_text = response.text.strip().replace("```json", "").replace("```", "")
        
        # specific handling for null
        if "null" in result_text.lower() and len(result_text) < 10:
             return None
             
        data = json.loads(result_text)
        return data


    except Exception as e:
        print(f"Twitter Trend Error: {e}")
        return None

# --------------------------------------------------------
# Hotel Fact Check Features
# --------------------------------------------------------
import streamlit as st # Added for user requested st.error/st.warning

def fetch_hotel_candidates(hotel_name, city, api_key):
    """
    Step 1: Search for potential hotels (Candidates).
    Returns: List of dicts [{'id':..., 'name':..., 'address':...}] or None
    """
    search_query = f"{hotel_name} Hotel {city} Thailand"
    url = "https://places.googleapis.com/v1/places:searchText"
    headers = {
        "Content-Type": "application/json",
        "X-Goog-Api-Key": api_key,
        "X-Goog-FieldMask": "places.id,places.displayName,places.formattedAddress"
    }
    payload = {"textQuery": search_query}
    
    try:
        response = requests.post(url, json=payload, headers=headers)
        if response.status_code != 200:
            st.error(f"ğŸš¨ API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code} - {response.text}")
            return None
            
        data = response.json()
        
        if not data.get("places"):
            return [] # Return empty list if no results
            
        # Extract meaningful candidates (up to 5)
        candidates = []
        for p in data["places"][:5]:
            candidates.append({
                "id": p["id"],
                "name": p.get("displayName", {}).get("text", "Unknown"),
                "address": p.get("formattedAddress", "")
            })
        return candidates

    except Exception as e:
        st.error(f"ì‹œìŠ¤í…œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def fetch_hotel_details(place_id, api_key):
    """
    Step 2: Fetch full details for a specific Place ID.
    Returns: place_dict or None
    """
    url = f"https://places.googleapis.com/v1/places/{place_id}"
    headers = {
        "X-Goog-Api-Key": api_key,
        "X-Goog-FieldMask": "id,displayName,formattedAddress,rating,userRatingCount,reviews,photos"
    }
    
    try:
        resp = requests.get(url, headers=headers)
        if resp.status_code != 200:
            st.error(f"ìƒì„¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {resp.text}")
            return None
            
        place = resp.json()
        
        # Photo handling
        photo_url = None
        if place.get("photos"):
            photo_ref = place["photos"][0]["name"]
            photo_url = f"https://places.googleapis.com/v1/{photo_ref}/media?maxHeightPx=800&maxWidthPx=800&key={api_key}"

        return {
            "name": place.get("displayName", {}).get("text", "Unknown"),
            "address": place.get("formattedAddress", ""),
            "rating": place.get("rating", 0.0),
            "review_count": place.get("userRatingCount", 0),
            "reviews": place.get("reviews", []),
            "photo_url": photo_url
        }
    except Exception as e:
        st.error(f"ìƒì„¸ ì •ë³´ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return None

def analyze_hotel_reviews(hotel_name, rating, reviews, api_key):
    """
    Analyze hotel reviews using Gemini with a specific 'Cold Inspector' persona.
    """
    try:
        # 1. Prepare Review Text
        reviews_text = ""
        for r in reviews[:5]: # Use top 5 reviews
             text = r.get("text", {}).get("text", "")
             if text:
                 reviews_text += f"- {text}\n"

        # 2. Gemini Prompt
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.0-flash', generation_config={"response_mime_type": "application/json"})

        prompt = f"""
        ë„ˆëŠ” 'ëƒ‰ì² í•œ í˜¸í…” ê²€ì¦ê°€'ì•¼. ì‚¬ìš©ìê°€ ì´ í˜¸í…”ì„ **"ì‹¤ì œë¡œ ì˜ˆì•½í• ì§€ ë§ì§€"** ê²°ì •í•  ìˆ˜ ìˆë„ë¡, ê´‘ê³  ë©˜íŠ¸ëŠ” ë¹¼ê³  ì˜¤ì§ **íŒ©íŠ¸ì™€ ì‹¤ì œ í›„ê¸°**ì— ê¸°ë°˜í•´ì„œ ë¶„ì„í•´ì¤˜.

        **[ë¶„ì„ ëŒ€ìƒ]**
        * í˜¸í…”ëª…: {hotel_name} (í‰ì : {rating})
        * êµ¬ê¸€ ë§µ ìµœì‹  ë¦¬ë·° ë°ì´í„°: {reviews_text}
        * **ì¶”ê°€ ì§€ì‹:** ìœ„ ë¦¬ë·° ì™¸ì—ë„, ë„¤ê°€ ì´ë¯¸ í•™ìŠµí•´ì„œ ì•Œê³  ìˆëŠ” ì´ í˜¸í…”ì˜ íŠ¹ì§•(ìœ„ì¹˜, ë¸Œëœë“œ í‰íŒ, ìˆ˜ì˜ì¥, ì¡°ì‹ ìŠ¤íƒ€ì¼ ë“±)ì„ ì´ë™ì›í•´.

        **[ì‘ì„± ê°€ì´ë“œë¼ì¸ - ì—„ê²© ì¤€ìˆ˜]**
        1. **ì¶”ì¸¡ ê¸ˆì§€:** ëª¨ë¥´ëŠ” êµ¬ì²´ì  ìˆ˜ì¹˜(ì˜ˆ: ì •í™•í•œ ë””íŒŒì§“ ê¸ˆì•¡)ëŠ” ì–µì§€ë¡œ ì“°ì§€ ë§ê³ , ì „ë°˜ì ì¸ 'ê²½í–¥ì„±(ì²´ê³„ì ì´ë‹¤/ëŠë¦¬ë‹¤)' ìœ„ì£¼ë¡œ ì„œìˆ í•´.
        2. **ë¹„íŒì  ì‹œê°:** "ì¢‹ì•˜ë‹¤" ëŒ€ì‹  "ìˆ˜ì••ì´ ë§ˆì‚¬ì§€ ìˆ˜ì¤€ì´ë‹¤" í˜¹ì€ "ë°°ìˆ˜ê°€ ëŠë ¤ ë¬¼ì´ ê³ ì¸ë‹¤"ì²˜ëŸ¼ êµ¬ì²´ì ìœ¼ë¡œ ë¬˜ì‚¬í•´.
        3. **í•œêµ­ì¸ ë§ì¶¤:** í•œêµ­ì¸ì´ ë¯¼ê°í•œ 'ë²Œë ˆ', 'ìƒ¤ì›Œê¸° í•„í„° ë³€ìƒ‰', 'ë°©ìŒ', 'ì¡°ì‹ ê¹€ì¹˜ ìœ ë¬´' ë“±ì˜ ì •ë³´ê°€ ìˆë‹¤ë©´ í•„ìˆ˜ë¡œ í¬í•¨í•´.

        **[ì¶œë ¥ í¬ë§· (JSON)]**
        ì‘ë‹µì€ ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ì„ ì§€ì¼œì¤˜.

        {{
            "one_line_verdict": "í•œ ì¤„ ê²°ë¡  (ì˜ˆ: ìœ„ì¹˜ëŠ” ê¹¡íŒ¨ì§€ë§Œ ê·€ë§ˆê°œ í•„ìˆ˜ì¸ ê°€ì„±ë¹„ í˜¸í…”)",
            "recommendation_target": "ì¶”ì²œ: [ëŒ€ìƒ], ë¹„ì¶”ì²œ: [ëŒ€ìƒ] (ì˜ˆ: ì¶”ì²œ: ì ë§Œ ì˜ í˜¼í–‰ì¡±, ë¹„ì¶”ì²œ: ì˜ˆë¯¼í•œ ì»¤í”Œ)",
            "location_analysis": "ìœ„ì¹˜ ë° ë™ì„  (ì—­ê³¼ì˜ ê±°ë¦¬, ì£¼ë³€ í¸ì˜ì /ë§ˆì‚¬ì§€ìƒµ, ì¹˜ì•ˆ, ë„ë³´ ë‚œì´ë„)",
            "room_condition": "ê°ì‹¤ ë””í…Œì¼ (ì²­ê²°ë„, ì¹¨êµ¬, ìŠµê¸°/ëƒ„ìƒˆ, ì†ŒìŒ, ë²Œë ˆ, ë·°)",
            "service_breakfast": "ì„œë¹„ìŠ¤ ë° ì¡°ì‹ (ì§ì› ì¹œì ˆë„, ì¡°ì‹ ë©”ë‰´ êµ¬ì„± ë° ë§›, í•œêµ­ì¸ ì…ë§› ì í•©ë„)",
            "pool_facilities": "ìˆ˜ì˜ì¥ ë° ë¶€ëŒ€ì‹œì„¤ (ìˆ˜ì˜ì¥ í¬ê¸°/ìˆ˜ì§ˆ/ê·¸ëŠ˜ ì—¬ë¶€, í—¬ìŠ¤ì¥ ë“±)",
            "pros": ["ì¥ì 1 (êµ¬ì²´ì  ê·¼ê±°)", "ì¥ì 2", "ì¥ì 3", "ì¥ì 4", "ì¥ì 5"],
            "cons": ["ë‹¨ì 1 (ì¹˜ëª…ì ì¸ ë¶€ë¶„)", "ë‹¨ì 2", "ë‹¨ì 3", "ë‹¨ì 4", "ë‹¨ì 5"],
            "summary_score": {{
                "cleanliness": 0,  // 5ì  ë§Œì  (ì •ìˆ˜)
                "location": 0,
                "comfort": 0,
                "value": 0
            }}
        }}
        """
        
        response = model.generate_content(prompt)
        return json.loads(response.text)

    except Exception as e:
        return {"error": str(e)}
